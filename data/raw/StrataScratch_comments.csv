Comment_id,author,Reply_for,Type,video_id,total_reply,like_count,published_at,textdisplay,updatedat
Ugy02bnPbk_z5oN2PG94AaABAg,@ARJUN-op2dh,,1,yFyyP_Uvxj0,1,0,2023-09-13T04:39:42Z,"52 P 2 Using the permutation formula: &quot;52 P 2 = 52! / (52 - 2)! = 52! / 50!.&quot; = (52 * 51) / (2 * 1) = 1326 , what am i missing or wrong here?",2023-09-13T04:39:42Z
Ugy02bnPbk_z5oN2PG94AaABAg.9ua6vECnPSd9ua7VrCoNu2,@ARJUN-op2dh,Ugy02bnPbk_z5oN2PG94AaABAg,2,yFyyP_Uvxj0,0,0,2023-09-13T04:44:50Z,"Number of ways to choose a suit = C(4, 1) (choose 1 suit out of 4) = 4<br><br>Number of ways to choose 2 cards from that suit = C(13, 2) (choose 2 cards from 13 in that suit) = 78<br><br>Favorable Cases = 4 * 78 = 312<br><br>Probability = Favorable Cases / Total Cases = 312 / 1,326 ‚âà 0.235<br><br>So, the probability of drawing two cards from the same suit (without replacement) is approximately 0.235, or 23.5%.",2023-09-13T04:44:50Z
UgzSkpoXFP1W69BP-0x4AaABAg,@alfatmiuzma,,1,yFyyP_Uvxj0,0,0,2022-05-14T20:14:33Z,Very nice video üëå. Expecting more on probability and statistics üòäüòä,2022-05-14T20:14:33Z
UgzatS3W8dI1KBZFmfh4AaABAg,@__goyal__,,1,yFyyP_Uvxj0,0,0,2022-04-12T03:03:57Z,Thanks Vivek!,2022-04-12T03:03:57Z
Ugzld_pmmrCMhNhhlZZ4AaABAg,@harshitsati,,1,yFyyP_Uvxj0,0,1,2022-03-13T01:55:23Z,Grate vid! :),2022-03-13T01:55:23Z
UgweaG-8gmDoK2AO1FN4AaABAg,@eleanorli7932,,1,yFyyP_Uvxj0,0,3,2022-03-12T12:52:20Z,Great examples! Looking forward to more stats videos :D,2022-03-12T12:52:20Z
UgxZ_jPEDE3Ox7QpY9x4AaABAg,@stratascratch,,1,ToEl_tpyoM4,2,14,2022-01-10T06:08:24Z,Hi everyone! Just wanted to announce that we‚Äôll be expanding our content and producing more videos every month. You‚Äôll be getting videos that cover all aspects of data science from our team. Everyone on our team has industry experience so I know you‚Äôll find the videos educational and interesting. Thanks for watching! And thanks for the support!,2022-01-10T06:08:24Z
UgxZ_jPEDE3Ox7QpY9x4AaABAg.9X0-NLsyyE09X3nn5a8c0U,@alfatmiuzma,UgxZ_jPEDE3Ox7QpY9x4AaABAg,2,ToEl_tpyoM4,0,0,2022-01-11T17:35:25Z,Thank you so much. Eagerly waitingüòä,2022-01-11T17:35:25Z
UgxZ_jPEDE3Ox7QpY9x4AaABAg.9X0-NLsyyE09XAgvCubaqI,@konstantinpluzhnikov4862,UgxZ_jPEDE3Ox7QpY9x4AaABAg,2,ToEl_tpyoM4,0,0,2022-01-14T09:50:03Z,Thanks for the video! Very interested in NLP questions (maybe something about string similiarity). Wish you all the best.,2022-01-14T09:50:03Z
UgzwlTeVwUwtSzTD9Vh4AaABAg,@adeyemibiola9763,,1,ToEl_tpyoM4,0,0,2023-04-17T09:59:19Z,pd.NaT represents missing values for datetime64[ns] types in pandas. <br>Great content.,2023-04-17T09:59:19Z
UgxKFt2sRB6q716Y1Oh4AaABAg,@AIFashionistaGuide,,1,ToEl_tpyoM4,0,1,2022-12-20T12:12:42Z,Plz sir will be glad to do the solutions to you. i cannot afford getting subsciption to stratascractch,2022-12-20T12:12:42Z
UgxigWiLoeIU_z_yGLN4AaABAg,@AIFashionistaGuide,,1,ToEl_tpyoM4,0,1,2022-12-20T12:11:51Z,can i share the subscription with you so that i could help you with question solving ????????????,2022-12-20T12:11:51Z
UgzHClexBgNDFkwgTEp4AaABAg,@elu1,,1,ToEl_tpyoM4,1,0,2022-07-29T17:10:02Z,very niceüëç,2022-07-29T17:10:02Z
UgzHClexBgNDFkwgTEp4AaABAg.9e4A4EwYVcH9e4GYfGG80a,@stratascratch,UgzHClexBgNDFkwgTEp4AaABAg,2,ToEl_tpyoM4,0,0,2022-07-29T18:06:37Z,Thank you.  Glad you find it helpful.,2022-07-29T18:06:37Z
UgxYBtKohWz-e5REdHd4AaABAg,@aazarjan7407,,1,ToEl_tpyoM4,0,0,2022-07-28T17:40:30Z,Has anyone solved it in SQL?,2022-07-28T17:40:30Z
Ugy9MPaYO5mJxBp1iPJ4AaABAg,@arianazin5419,,1,ToEl_tpyoM4,0,0,2022-03-06T09:07:12Z,The question clearly asks you to write a query (SQL). Solving it in a programming language makes it so much easier because of the additional constructs you can use.,2022-03-06T09:07:12Z
Ugymg_4N7ip63rMzoDR4AaABAg,@sathya018,,1,ToEl_tpyoM4,0,0,2022-01-20T17:24:27Z,I love itüëå,2022-01-20T17:24:27Z
Ugyink5bspgDEsjrEjp4AaABAg,@henrold6228,,1,ToEl_tpyoM4,2,2,2022-01-15T00:06:36Z,"I nailed my sql technical questions (thanks to this channel), but they sprung a python one on me and I bombed it. Having so, so much trouble finding a job as a data science intern. Looking extremely bleak.",2022-01-15T00:06:36Z
Ugyink5bspgDEsjrEjp4AaABAg.9XCDwn2kaKe9XCq-zetLW1,@stratascratch,Ugyink5bspgDEsjrEjp4AaABAg,2,ToEl_tpyoM4,0,0,2022-01-15T05:47:58Z,Sorry to hear! It&#39;s definitely not easy with all the different skills you need to be a data scientist. Keep on practicing! We&#39;re planning to pump out more content this year related to python and other ds topics.,2022-01-15T05:47:58Z
Ugyink5bspgDEsjrEjp4AaABAg.9XCDwn2kaKe9Y0M4rUYWdW,@johnwig285,Ugyink5bspgDEsjrEjp4AaABAg,2,ToEl_tpyoM4,0,0,2022-02-04T05:58:15Z,"Did u try asking whether it is okay for u to use SQL instead? Because ultimately languages are just tools and a means to an end, they should&#39;ve let u solved in both languages.",2022-02-04T05:58:15Z
Ugy_wqZ1EGA_Qc3fB4F4AaABAg,@ashishalex10,,1,ToEl_tpyoM4,1,1,2022-01-14T15:02:50Z,Are we supposed to know these pandas functions by heart in the interview or we can look them up ?,2022-01-14T15:02:50Z
Ugy_wqZ1EGA_Qc3fB4F4AaABAg.9XBFi8TbuWh9XBbMXmhJO2,@stratascratch,Ugy_wqZ1EGA_Qc3fB4F4AaABAg,2,ToEl_tpyoM4,0,2,2022-01-14T18:20:44Z,You&#39;re supposed to know them. But it&#39;s perfectly fine if you can&#39;t recall exactly the syntax and number of parameters needed for the functions. But it should be pretty close to the real function. What&#39;s most important is the right approach and methodology though.,2022-01-14T18:20:44Z
UgwVhMpfCWIx5vJNlfZ4AaABAg,@alfatmiuzma,,1,ToEl_tpyoM4,1,1,2022-01-11T17:34:40Z,"very clear explanantion. Thanks a lotüòä.We expect more such awesome videos on Data science, please..",2022-01-11T17:34:40Z
UgwVhMpfCWIx5vJNlfZ4AaABAg.9X3nhUgl9Lw9X4wybAeIog,@stratascratch,UgwVhMpfCWIx5vJNlfZ4AaABAg,2,ToEl_tpyoM4,0,0,2022-01-12T04:14:53Z,Thank you! Much more to come! I&#39;m hoping to create 3-4 a month with the help of my team. And will start to do a lot of different types of DS content.,2022-01-12T04:14:53Z
Ugzu0nMK9T6OCWutOOt4AaABAg,@rakibraihan1572,,1,ToEl_tpyoM4,0,0,2022-01-11T13:27:05Z,great,2022-01-11T13:27:05Z
Ugy2GiBJXoYS8rouToF4AaABAg,@narendrareddy4923,,1,ToEl_tpyoM4,2,0,2022-01-11T06:47:07Z,"Hey, nice video and informative <br>Where can i find these type of questions in various difficulty ranges to practice..? Thanks in advance.",2022-01-11T06:47:07Z
Ugy2GiBJXoYS8rouToF4AaABAg.9X2danKXz6F9X3j49YGAsq,@stratascratch,Ugy2GiBJXoYS8rouToF4AaABAg,2,ToEl_tpyoM4,0,1,2022-01-11T16:54:12Z,"You can try <a href=""http://stratascratch.com/"">stratascratch.com</a> which is the platform we&#39;re using! But there is also leetcode if you want to try python algorithm questions too.",2022-01-11T16:54:12Z
Ugy2GiBJXoYS8rouToF4AaABAg.9X2danKXz6F9YZ15EeWIU8,@carlykelly8881,Ugy2GiBJXoYS8rouToF4AaABAg,2,ToEl_tpyoM4,0,0,2022-02-17T17:08:13Z,@@stratascratch o,2022-02-17T17:08:13Z
UgyiGPw-OOYQoaEc7MN4AaABAg,@ragnickgibcy1009,,1,ToEl_tpyoM4,0,1,2022-01-10T22:51:12Z,"<a href=""about:invalid#zCSafez""></a> I respect the work you put in your videos. I find it truly baffling that all major crypto you-tubers just look at pure wave and completely ignore the bigger narrative of why BTC is pumping and why the future outlook might not be as rosy as it seems. It&#39;s sort of irresponsible to ignore the fact that each ETF launch so far has caused a major dump at the peaks of BTC. We were already on shaky footing with historically low volume and almost pure whale pumps, narrowly avoiding a long-term bear market. this is the worst possible time in history to hodl as so many don&#39;t back up their crypto assets. more emphasis should be put into day trading as it is less affected by the unpredictable nature of the market. I have made over 9 btc with 2.7 btc from day trading with Daniel&#39;s help in few weeks, this is one of the best medium to backup your assets incase it goes bearish.<a href=""about:invalid#zCSafez""></a>You can reach Daniel on Õ≤eI—î…†…æŒ±müëâDanielwrightfx.",2022-01-10T22:51:12Z
UgwTODi99GH1VJVexIh4AaABAg,@SeattleDataGuy,,1,ToEl_tpyoM4,1,1,2022-01-10T21:11:27Z,Woah! this is great! I love the new feel. Looks like someone has been busy,2022-01-10T21:11:27Z
UgwTODi99GH1VJVexIh4AaABAg.9X1biRH9vox9X2Dc7k_4Rz,@stratascratch,UgwTODi99GH1VJVexIh4AaABAg,2,ToEl_tpyoM4,0,0,2022-01-11T02:51:23Z,"Hey! Thanks a lot! Yea trying to pump out videos as much as possible so I got help from my team (they&#39;re better data scientists than I am anyways so it works =)). Trying to also do other type of content and experiment a bit with content, people, and video editing style. Thanks for watching man! Hope you&#39;ve been doing well.",2022-01-11T02:51:23Z
Ugyj8htTfooXnNXlxWF4AaABAg,@kaustavraytalukdar638,,1,ToEl_tpyoM4,0,0,2022-01-10T17:59:56Z,"is pd.NAT function is not working in Jupiter notebook, any suggestions please",2022-01-10T17:59:56Z
Ugwqj12NKBicqg0Whl54AaABAg,@ibsenvillarroel9543,,1,ToEl_tpyoM4,0,0,2022-01-10T15:31:43Z,"Great video, thanks",2022-01-10T15:31:43Z
UgzUGDcAThDo8N9bl9p4AaABAg,@learnwithgandalf,,1,ToEl_tpyoM4,1,1,2022-01-10T08:27:06Z,zoom in please...,2022-01-10T08:27:06Z
UgzUGDcAThDo8N9bl9p4AaABAg.9X0FF7S_HMe9X17yE2NfpF,@stratascratch,UgzUGDcAThDo8N9bl9p4AaABAg,2,ToEl_tpyoM4,0,0,2022-01-10T16:42:43Z,Thanks for the feedback! We will do that in future videos,2022-01-10T16:42:43Z
UgyrcWBoqhrTLuFTy5d4AaABAg,@nathanrosidi4894,,1,ToEl_tpyoM4,0,1,2022-01-10T05:54:18Z,"Great job, Matt!",2022-01-10T05:54:18Z
UgwWwUHk30nWMhU7AHF4AaABAg,@erniehsieh7430,,1,jN5hpgBrz8k,1,0,2022-08-05T23:08:12Z,"Thank you for such a great video content. Just wanted to ask you on should we be preparing leetcode type of question as a data scientist? Also, would we be expected to write ML codes in the interview such as neural network ?",2022-08-05T23:08:12Z
UgwWwUHk30nWMhU7AHF4AaABAg.9eMpcdRYu6H9eOwl2OT1HI,@stratascratch,UgwWwUHk30nWMhU7AHF4AaABAg,2,jN5hpgBrz8k,0,0,2022-08-06T18:48:59Z,Glad you like the video.  The videos are types of interview questions that you can expect.,2022-08-06T18:48:59Z
UgykmUkPT8tXMZaYjDp4AaABAg,@caiyu538,,1,jN5hpgBrz8k,0,0,2022-04-05T15:53:21Z,Great tutorials.,2022-04-05T15:53:21Z
UgwNWmbp5gCSIsQAZ8Z4AaABAg,@rick2591,,1,jN5hpgBrz8k,1,0,2022-04-05T14:40:27Z,"I guess I am not mid level developer. As I would do this in SQL...This is why I disagree with my prof. Syntax is the most complex part of coding, not logic.",2022-04-05T14:40:27Z
UgwNWmbp5gCSIsQAZ8Z4AaABAg.9_RmY0Pk2WR9_RuUqsIP-m,@stratascratch,UgwNWmbp5gCSIsQAZ8Z4AaABAg,2,jN5hpgBrz8k,0,0,2022-04-05T15:49:55Z,"I&#39;d do it in SQL too =). But a lot of my viewers want to see how it could be done in python so thus....this video. I also think up to a certain point, syntax is important. But once you get really good, syntax is the easy part and the logic becomes much more complex. At least that&#39;s how it was in my experience.",2022-04-05T15:49:55Z
UgxzxqxW-t3NBW0CEbZ4AaABAg,@cobekingston,,1,jN5hpgBrz8k,1,0,2022-04-05T13:40:30Z,"Before reaching this stage, anywhere you recommend learning the fundamentals to hard stuff from?",2022-04-05T13:40:30Z
UgxzxqxW-t3NBW0CEbZ4AaABAg.9_RffuWcq5h9_RuLXJWjt8,@stratascratch,UgxzxqxW-t3NBW0CEbZ4AaABAg,2,jN5hpgBrz8k,0,0,2022-04-05T15:48:39Z,"I think the path is different for everyone, depending on your experience and learning style. For me, I went through the Mode Analytics SQL tutorial (it&#39;s free) first. Then I practiced on LeetCode SQL for a while. This was enough to get me into an entry level data analyst role. Once on the job, I was able to develop more data science skills and transfer to a DS role. But it took me a few years of working in industry to develop those skills. But to learn the basics, a SQL tutorial and SQL platform (LC, StrataScratch, HackerRank) will help.",2022-04-05T15:48:39Z
UgyjjPy0PgYgy4XHxEt4AaABAg,@balajivenkatraman2991,,1,jN5hpgBrz8k,3,0,2022-04-03T14:55:05Z,"Hi Bro, Your videos are awesome, happy learning I liked it very much. Thought of subscribing but haven&#39;t seen these problems 1. Linked list 2. Singly and dubly linked list 3. Tree (dfs, bfs). I dont see blind 75 topics.. Please let me know on that or am I missing something.",2022-04-03T14:55:05Z
UgyjjPy0PgYgy4XHxEt4AaABAg.9_MecQ9AIJ49_PGvmF_uub,@Aidan_Au,UgyjjPy0PgYgy4XHxEt4AaABAg,2,jN5hpgBrz8k,0,0,2022-04-04T15:16:59Z,These are data structure and algorithm questions.....,2022-04-04T15:16:59Z
UgyjjPy0PgYgy4XHxEt4AaABAg.9_MecQ9AIJ49_PfaDYtRId,@stratascratch,UgyjjPy0PgYgy4XHxEt4AaABAg,2,jN5hpgBrz8k,0,0,2022-04-04T19:01:14Z,"Hi, thanks for your msg. The Blind75 problems are data structure and algo problems. We currently don&#39;t have this on our platform but we do plan on adding these questions to the platform this year. Hope you stick around!",2022-04-04T19:01:14Z
UgyjjPy0PgYgy4XHxEt4AaABAg.9_MecQ9AIJ49acJL6ii2L4,@leslielu283,UgyjjPy0PgYgy4XHxEt4AaABAg,2,jN5hpgBrz8k,0,0,2022-05-04T22:38:49Z,"@@stratascratch Hi! wondering for data scientist coding preparation, what kind of coding questions shall we prepare for (data structures &amp; alg questions or any other specific coding questions only for data scientists)? thank you so much!",2022-05-04T22:38:49Z
UgwhUJMo6X5jBlGqsrt4AaABAg,@castcrus,,1,jN5hpgBrz8k,1,0,2022-04-01T00:40:15Z,"tbh, for anyone without an omniscient point of view, first step would be facebook_web_log.head(), get to know the real data a bit, then tackle each requirement.  The highly abstract approach would come as a conclusion. This does not show how a normal person would approach the problem, thus making it difficult to understand.",2022-04-01T00:40:15Z
UgwhUJMo6X5jBlGqsrt4AaABAg.9_FzCg8xzZM9_G-OmVfaOu,@castcrus,UgwhUJMo6X5jBlGqsrt4AaABAg,2,jN5hpgBrz8k,0,0,2022-04-01T00:50:38Z,"Just remember, not all people watching this video knows this question. You need to give them context, or at least tell them to have a try themselves first, then watch this video.",2022-04-01T00:50:38Z
UgwGj8HOfXiIH2oLxal4AaABAg,@TheElementFive,,1,jN5hpgBrz8k,0,0,2022-03-30T01:02:33Z,Why not just cast the duration column to type int?,2022-03-30T01:02:33Z
UgyYXGndsSmxGwvtGqF4AaABAg,@ryandavis280,,1,jN5hpgBrz8k,0,0,2022-03-29T21:49:05Z,"Very clear, thanks",2022-03-29T21:49:05Z
UgzX4a4Yarg2sFIzkmF4AaABAg,@astronemir,,1,jN5hpgBrz8k,0,0,2022-03-23T15:44:06Z,"Thanks for sharing, was fun to go through.",2022-03-23T15:44:06Z
UgxEDx7Qrda5YNfx13h4AaABAg,@yasamans2436,,1,jN5hpgBrz8k,1,0,2022-03-23T07:25:55Z,"A question, shouldnt we sort the timestamps for loads and exits first?",2022-03-23T07:25:55Z
UgxEDx7Qrda5YNfx13h4AaABAg.9ZuXU5xBiGs9Zw6GT_qTga,@stratascratch,UgxEDx7Qrda5YNfx13h4AaABAg,2,jN5hpgBrz8k,0,0,2022-03-23T22:06:37Z,You don&#39;t actually need to because the timestamp combinations between loads and exit will be in the merged frame &quot;sessions&quot;. Then you do the time filtering in row 11. So there&#39;s no need to sort your timestamps with this technique.,2022-03-23T22:06:37Z
UgxTu14VuFZRDj2iYj14AaABAg,@nishantkumar-lw6ce,,1,jN5hpgBrz8k,1,0,2022-03-23T01:13:45Z,"Great, can you share more details on the syntax of last step?",2022-03-23T01:13:45Z
UgxTu14VuFZRDj2iYj14AaABAg.9ZtrtCFaBdU9Zw66ts6of6,@stratascratch,UgxTu14VuFZRDj2iYj14AaABAg,2,jN5hpgBrz8k,0,0,2022-03-23T22:05:18Z,"You mean the lambda function with the groupby and agg? <a href=""https://stackoverflow.com/questions/30718231/aggregating-lambda-functions-in-pandas-and-numpy"">https://stackoverflow.com/questions/30718231/aggregating-lambda-functions-in-pandas-and-numpy</a>",2022-03-23T22:05:18Z
UgwrI6e2j4l4Z3fYcxx4AaABAg,@shivamroy1775,,1,tnoOz6MzTPg,1,2,2022-09-02T21:16:14Z,"There can be an edge case that the code missed, a train arriving just before the end of the day and departing at the start of another day, for example, arrival at <a href=""https://www.youtube.com/watch?v=tnoOz6MzTPg&amp;t=23m58s"">23:58</a> and departure at <a href=""https://www.youtube.com/watch?v=tnoOz6MzTPg&amp;t=00m02s"">00:02</a> would give a wrong answer. Nevertheless, this is a great way of solving the question after accounting for the edge case and was very helpful. Thanks.",2022-09-02T21:17:34Z
UgwrI6e2j4l4Z3fYcxx4AaABAg.9fUj45RVodp9fdQzJcqdgM,@stratascratch,UgwrI6e2j4l4Z3fYcxx4AaABAg,2,tnoOz6MzTPg,0,0,2022-09-06T15:41:49Z,Glad to hear it!  Thank you.,2022-09-06T15:41:49Z
UgxywCsCbPsSczipSZB4AaABAg,@InfoJunky,,1,tnoOz6MzTPg,1,0,2022-07-19T21:11:24Z,"Very elegant, thank you much!",2022-07-19T21:11:24Z
UgxywCsCbPsSczipSZB4AaABAg.9dfqkMfmb1U9dhHA_Yg56r,@stratascratch,UgxywCsCbPsSczipSZB4AaABAg,2,tnoOz6MzTPg,0,0,2022-07-20T10:30:17Z,Thank you! üòä,2022-07-20T10:30:17Z
UgxA01_tAD9G7WpUUl94AaABAg,@jianhongsong6140,,1,tnoOz6MzTPg,0,0,2022-06-15T20:36:41Z,Very clever. Never thought cumsum() can be used to solve questions like this. Also it takes a genius to mark arrival as 1 and departure as -1.,2022-06-15T20:36:41Z
UgwDH7JqodAlXLmsdZ14AaABAg,@williamausenka9251,,1,tnoOz6MzTPg,1,0,2022-05-23T23:14:49Z,when is the next SQL video?,2022-05-23T23:14:49Z
UgwDH7JqodAlXLmsdZ14AaABAg.9bOIZJUMU5X9bQMJpW8d9y,@stratascratch,UgwDH7JqodAlXLmsdZ14AaABAg,2,tnoOz6MzTPg,0,0,2022-05-24T18:26:08Z,It&#39;s coming! Apologizes for the delays. There&#39;s just so much work on our plates these days =/,2022-05-24T18:26:08Z
UgzgsEn5QhCDWa-OSGZ4AaABAg,@AkshayKumar-uz4xo,,1,tnoOz6MzTPg,3,1,2022-05-16T06:19:48Z,Hey bro can  you  help me in this question;<br>Question ID: 9629.  Find how many hosts are verified by the Airbnb staff and how many aren&#39;t.,2022-05-16T06:19:48Z
UgzgsEn5QhCDWa-OSGZ4AaABAg.9b4Sq4ecU7a9b5XWU8lqI9,@stratascratch,UgzgsEn5QhCDWa-OSGZ4AaABAg,2,tnoOz6MzTPg,0,1,2022-05-16T16:19:55Z,Happy to help you with this question. Go to the question on the platform and ask leave your question on the discussion forum there. Someone from my team will help you!,2022-05-16T16:19:55Z
UgzgsEn5QhCDWa-OSGZ4AaABAg.9b4Sq4ecU7a9b5Zj5Gyshp,@AkshayKumar-uz4xo,UgzgsEn5QhCDWa-OSGZ4AaABAg,2,tnoOz6MzTPg,0,1,2022-05-16T16:39:15Z,@@stratascratch ok thanks,2022-05-16T16:39:15Z
UgzgsEn5QhCDWa-OSGZ4AaABAg.9b4Sq4ecU7a9hWtLMZLys1,@SP-db6sh,UgzgsEn5QhCDWa-OSGZ4AaABAg,2,tnoOz6MzTPg,0,0,2022-10-23T10:27:14Z,"Interesting, let me see! Have you silver it ?",2022-10-23T10:27:14Z
UgyfGyfIUZdJPCZvjnZ4AaABAg,@vishwaschaudhary9833,,1,tnoOz6MzTPg,0,7,2022-05-14T11:56:18Z,Cumulative Sum:    -_-<br>Cumsum:    Neuron Activation,2022-05-14T11:56:18Z
UgzTV8d-vW2lOadCNgJ4AaABAg,@rakibraihan1572,,1,tnoOz6MzTPg,0,1,2022-05-12T17:04:01Z,excellent,2022-05-12T17:04:01Z
UgyLsRmPIIOM_swRzxh4AaABAg,@stratascratch,,1,ggZzHnzYH1s,0,1,2022-08-03T18:27:55Z,Very sorry that the code is small. We will fix this in the future videos!,2022-08-03T18:27:55Z
Ugz2t214t6SUcDJ7vrx4AaABAg,@chillvibe4745,,1,ggZzHnzYH1s,1,0,2022-10-03T13:07:48Z,"Hello. In SQL we have the dense_rank window function :<br>dense_rank () over (partition by something order by something desc)<br>In pandas we have:<br>rank(method=&#39;dense&#39;, ascending=False)<br>I am missing the partition by in pandas . How do I do that? Please reply?",2022-10-03T13:08:46Z
Ugz2t214t6SUcDJ7vrx4AaABAg.9gifoqpsMFt9xEFmcym1OE,@175off66,Ugz2t214t6SUcDJ7vrx4AaABAg,2,ggZzHnzYH1s,0,0,2023-11-17T21:08:49Z,We use groupby only in pandas to achieve partitions with in the dataframe,2023-11-17T21:08:49Z
Ugw2hwaj_tpUUFjEqr14AaABAg,@JoseAlvarez-di2eo,,1,ggZzHnzYH1s,1,1,2022-08-19T02:57:52Z,Seen quite a few videos on rolling windows funcs. This one is the best explained by far. Thanks,2022-08-19T02:57:52Z
Ugw2hwaj_tpUUFjEqr14AaABAg.9etiFDa5dDq9evbZbdTLiw,@stratascratch,Ugw2hwaj_tpUUFjEqr14AaABAg,2,ggZzHnzYH1s,0,0,2022-08-19T20:37:58Z,Appreciate it!  You are welcome.,2022-08-19T20:37:58Z
UgxrrjtMe2FFzLKBz5R4AaABAg,@TheRaju991,,1,ggZzHnzYH1s,1,0,2022-08-03T07:35:53Z,Thanks for this. Super useful,2022-08-03T07:35:53Z
UgxrrjtMe2FFzLKBz5R4AaABAg.9eG0LNa7O6R9eGmXTgYUQJ,@stratascratch,UgxrrjtMe2FFzLKBz5R4AaABAg,2,ggZzHnzYH1s,0,1,2022-08-03T14:45:42Z,Wonderful.  Glad you liked it.,2022-08-03T14:45:42Z
UgyfiRbso3M3_X7Wl0R4AaABAg,@ThePragyashree,,1,ggZzHnzYH1s,1,0,2022-08-03T03:52:37Z,Wao thank you,2022-08-03T03:52:37Z
UgyfiRbso3M3_X7Wl0R4AaABAg.9eFbnB_S6369eGmVAS0WQ4,@stratascratch,UgyfiRbso3M3_X7Wl0R4AaABAg,2,ggZzHnzYH1s,0,0,2022-08-03T14:45:23Z,You&#39;re welcome.  Glad you find it helpful.,2022-08-03T14:45:23Z
UgxgBuXj3rI31DWcvsZ4AaABAg,@jianhongsong6140,,1,yQSZT65WET4,1,1,2022-10-22T11:18:51Z,"From step 4 to step 5 when you try to put Q3 sales and Q4 sales side by side, a neater way is to run an ‚Äúunstack‚Äù operation on the inner index, to avoid a reset index operation and merge operation.",2022-10-22T11:18:51Z
UgxgBuXj3rI31DWcvsZ4AaABAg.9hUPSRWXmxy9hUQxtE7Enp,@stratascratch,UgxgBuXj3rI31DWcvsZ4AaABAg,2,yQSZT65WET4,0,0,2022-10-22T11:32:01Z,Wonderful.  Thank you for your insight.,2022-10-22T11:32:01Z
UgxOA5QbTWjT2M7SOZh4AaABAg,@johnalvinm,,1,yQSZT65WET4,1,0,2022-10-20T21:20:38Z,"&quot;Can only use .dt accessor with datetimelike values&quot; when trying to execute : df = fct_customer_sales[fct_customer_sales.order_date.dt.quarter.isin([3,4])]  Why doesnt it work for me?",2022-10-20T21:20:38Z
UgxOA5QbTWjT2M7SOZh4AaABAg.9hQKjKAq_4F9hQLR2Uxzca,@johnalvinm,UgxOA5QbTWjT2M7SOZh4AaABAg,2,yQSZT65WET4,0,1,2022-10-20T21:26:44Z,"Nevermind, in the link order_date is an object, so need to convert to datetime format.",2022-10-20T21:26:44Z
Ugy4tjdCA9KaNmI9NoN4AaABAg,@firstname4337,,1,yQSZT65WET4,0,0,2022-10-13T17:56:02Z,"jesus christ, that&#39;s shit <br>just do the whole thing in sql",2022-10-13T17:56:02Z
UgyvbHLX9BXq95JQ_qN4AaABAg,@faisalbargi5903,,1,RmMuS5iiviI,0,0,2023-03-25T18:33:23Z,"@<a href=""https://www.youtube.com/watch?v=RmMuS5iiviI&amp;t=7m13s"">7:13</a> why do we need to take pid.unique() anyways not null means it will give unique value only ?",2023-03-25T18:33:23Z
UgxrdvOVn3SjPPOyseJ4AaABAg,@faisalbargi5903,,1,a8T88wCrmAQ,0,0,2023-03-07T06:25:20Z,"forbes_global_2010_2014[&#39;ranks&#39;]=forbes_global_2010_2014[&#39;profits&#39;].rank(method=&#39;min&#39;,ascending=False)<br><br>forbes_global_2010_2014.query(&#39;ranks&lt;=3&#39;)<br><br>I solved it with only 2 above steps",2023-03-07T06:25:20Z
UgzLt3BqJlzX-aFTXvl4AaABAg,@AIFashionistaGuide,,1,a8T88wCrmAQ,2,1,2022-12-20T12:10:14Z,Please give question solutions of premium questions,2022-12-20T12:10:14Z
UgzLt3BqJlzX-aFTXvl4AaABAg.9jrQEQImdD89kB6eqU32Zu,@stratascratch,UgzLt3BqJlzX-aFTXvl4AaABAg,2,a8T88wCrmAQ,0,1,2022-12-28T13:03:16Z,"Please visit our website <a href=""https://www.stratascratch.com/"">https://www.stratascratch.com</a>.  We have free access and a Premium subscription, where have all access to questions and solutions.  We have 25% OFF our yearly and lifetime plans. If you are interested use the code StrataHolidays - our deal expires on Tuesday, January 3rd!",2022-12-28T13:03:16Z
UgzLt3BqJlzX-aFTXvl4AaABAg.9jrQEQImdD89kBjr7Imchq,@AIFashionistaGuide,UgzLt3BqJlzX-aFTXvl4AaABAg,2,a8T88wCrmAQ,0,0,2022-12-28T18:54:28Z,@@stratascratch Thanksüòáüòáüòá,2022-12-28T18:54:28Z
UgzLnSwKdag_69kGvtV4AaABAg,@AIFashionistaGuide,,1,a8T88wCrmAQ,2,1,2022-12-20T12:09:17Z,"Number Of Units Per Nationality<br><br><br>Airbnb<br>Easy<br>General Practice<br>ID 10156<br><br><br>import numpy<br>import pandas as pd<br><br># step 1 : <br>            # we have two tables merge them here.<br>            # join them using merge() and common table is host_id.<br>            # syntax : <br>            # pd.merge(df1,df2,on=&#39;common column&#39;,how=&#39;type of join required.&#39;)<br>pd.merge(airbnb_hosts,airbnb_units,on=&#39;host_id&#39;,how=&#39;inner&#39;)<br><br># step 2 :<br>            # remove duplicates.<br>            # syntax:<br>            # .drop_duplicates()<br>pd.merge(airbnb_hosts,airbnb_units,on=&#39;host_id&#39;,how=&#39;inner&#39;).drop_duplicates()<br><br># step 3:<br>            # conditions applied using bullet indexing<br>            # df[() &amp; ()]<br>            # df[(df[&#39;unit_type&#39;]==&#39;Apartment&#39;) &amp; (df[&#39;age&#39;]&lt;30)]<br>df=pd.merge(airbnb_hosts,airbnb_units,on=&#39;host_id&#39;,how=&#39;inner&#39;).drop_duplicates()<br>df<br>df[(df[&#39;unit_type&#39;]==&#39;Apartment&#39;) &amp; (df[&#39;age&#39;]&lt;30)]<br><br># step 4:<br>            # count the number of apartments based on natioanlity<br>            # only one natinality present here...USA<br>            # count() function is an aggreate function can be used only with groupby() function<br>            <br>            # use groupby function <br>                    # df[(df[&#39;unit_type&#39;]==&#39;Apartment&#39;) &amp; (df[&#39;age&#39;]&lt;30)].groupby(&#39;nationality&#39;)<br>            # count() function on unit_id.<br>df[(df[&#39;unit_type&#39;]==&#39;Apartment&#39;) &amp; (df[&#39;age&#39;]&lt;30)].groupby(&#39;nationality&#39;)[&#39;unit_id&#39;].count()<br>            <br>            # Output the nationality along with the number of apartments.<br># df[(df[&#39;unit_type&#39;]==&#39;Apartment&#39;) &amp; (df[&#39;age&#39;]&lt;30)].groupby(&#39;nationality&#39;)[&#39;unit_id&#39;].count()[&#39;nationality&#39;]<br>            # gives error natiobality is not part of this dataframe output.<br><br># both will work as_index and reset_index()<br>df[(df[&#39;unit_type&#39;]==&#39;Apartment&#39;) &amp; (df[&#39;age&#39;]&lt;30)].groupby(&#39;nationality&#39;,as_index=False)[&#39;unit_id&#39;].count<br>df[(df[&#39;unit_type&#39;]==&#39;Apartment&#39;) &amp; (df[&#39;age&#39;]&lt;30)].groupby(&#39;nationality&#39;)[&#39;unit_id&#39;].count().reset_index()",2022-12-20T12:09:17Z
UgzLnSwKdag_69kGvtV4AaABAg.9jrQ7TXtje79k8wz4vbHN5,@stratascratch,UgzLnSwKdag_69kGvtV4AaABAg,2,a8T88wCrmAQ,0,1,2022-12-27T16:51:25Z,Thanks for sharing!,2022-12-27T16:51:25Z
UgzLnSwKdag_69kGvtV4AaABAg.9jrQ7TXtje79kAclWmzOlV,@AIFashionistaGuide,UgzLnSwKdag_69kGvtV4AaABAg,2,a8T88wCrmAQ,0,0,2022-12-28T08:33:17Z,@@stratascratch Please give question solutions of premium questions....or how to access the solutions of premium questions,2022-12-28T08:33:17Z
UgxCycNHPCYVrZURCad4AaABAg,@AIFashionistaGuide,,1,a8T88wCrmAQ,0,1,2022-12-20T12:08:52Z,"Find matching hosts and guests in a way that they are both of the same gender and nationality<br><br>Airbnb<br>Easy<br>General Practice<br>ID 10078<br><br><br>import numpy<br>import pandas as pd<br><br># step 1 : <br>            # we have two tables merge them here.<br>            # join them using merge() and common table is host_id.<br>            # syntax : <br>            # pd.merge(df1,df2,on=&#39;common column&#39;,how=&#39;type of join required.&#39;)<br>pd.merge(airbnb_hosts,airbnb_guests,on=[&#39;nationality&#39;,&#39;gender&#39;],how=&#39;inner&#39;)<br><br># step 2 :<br>            # remove duplicates.<br>            # syntax:<br>            # .drop_duplicates()<br>pd.merge(airbnb_hosts,airbnb_guests,on=[&#39;nationality&#39;,&#39;gender&#39;],how=&#39;inner&#39;).drop_duplicates()<br><br># step 3 :<br>            # Output the host id and the guest id of matched pair.<br>df=pd.merge(airbnb_hosts,airbnb_guests,on=[&#39;nationality&#39;,&#39;gender&#39;],how=&#39;inner&#39;).drop_duplicates()<br>df<br>df[[&#39;host_id&#39;,&#39;guest_id&#39;]]",2022-12-20T12:08:52Z
UgzE04e1CAw9Q3LPNph4AaABAg,@AIFashionistaGuide,,1,a8T88wCrmAQ,0,1,2022-12-20T12:08:21Z,"Ranking Most Active Guests<br><br><br>Airbnb<br>Medium<br>General Practice<br>ID 10159<br><br>import pandas as pd<br>import numpy as np<br><br># step 1:<br>        # get th total number of rows.<br>airbnb_contacts.info()<br>&#39;&#39;&#39;<br>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;<br>RangeIndex: 100 entries, 0 to 99<br>Data columns (total 11 columns):<br>&#39;&#39;&#39;<br># step 2:<br>        # get th total number of rows.<br>        # use groupby and the sum of messages given here.<br>        # gives only &#39;n_messages&#39; column<br>df= airbnb_contacts.groupby(&#39;id_guest&#39;)[&#39;n_messages&#39;].sum()<br>        # gives only &#39;n_messages&#39;,&#39;id_guest&#39; column<br>df=airbnb_contacts.groupby(&#39;id_guest&#39;,as_index=False)[&#39;n_messages&#39;].sum()<br><br># step 3:<br>        # Order by the highest number of total messages first.<br>df=airbnb_contacts.groupby(&#39;id_guest&#39;,as_index=False)[&#39;n_messages&#39;].sum()<br>df.sort_values(&#39;n_messages&#39;,ascending=False)<br>                #<br>&#39;&#39;&#39;<br>                id_guest	n_messages<br>                882f3764-05cc-436a-b23b-93fea22ea847	20<br>                62d09c95-c3d2-44e6-9081-a3485618227d	20<br>&#39;&#39;&#39;<br># step 4:<br>        # Order by the guest id also check the expected output.<br>&#39;&#39;&#39;<br>            ranking	id_guest	n_messages<br>            1	62d09c95-c3d2-44e6-9081-a3485618227d	20<br>            1	882f3764-05cc-436a-b23b-93fea22ea847	20<br>&#39;&#39;&#39;<br>df=airbnb_contacts.groupby(&#39;id_guest&#39;,as_index=False)[&#39;n_messages&#39;].sum()<br>df1=df.sort_values(by=[&#39;n_messages&#39;,&#39;id_guest&#39;],ascending=[False,True])<br><br># step 5 : <br>            # we have rank based on n_messages .<br>            # using rank() <br>            # syntax : <br>                # dataframename[&#39;name of column name of rank&#39;]=dataframenaem.rank()<br>            # rank() works on average <br>            # signature : rank(axis=0,method=&#39;average&#39;,...) check this in pop up.<br>            # shows in decimal format.<br>df[&quot;Ranking&quot;] = df[&#39;n_messages&#39;].rank()<br><br># step 6 : <br>            # Guests with the same number of messages as other guests should have the same rank. <br>            # Do not skip rankings if the preceding rankings are identical.<br>            # using rank(method=&#39;dense&#39;)<br>df1[&quot;Ranking&quot;] = df1[&#39;n_messages&#39;].rank(method=&#39;dense&#39;)<br>df1<br>            # we want highest number of n_messages to be ranked 1<br>df1[&quot;Ranking&quot;] = df1[&#39;n_messages&#39;].rank(method=&#39;dense&#39;,ascending=False)<br>df1<br><br># step 7 : <br>            # Guests with the same number of messages as other guests should have the same rank. <br>            # Do not skip rankings if the preceding rankings are identical.<br>            # using rank(method=&#39;dense&#39;)<br>df1[&quot;Ranking&quot;] = df1[&#39;n_messages&#39;].rank(method=&#39;dense&#39;)<br>df1<br>            # we want highest number of n_messages to be ranked 1<br>df1[&quot;Ranking&quot;] = df1[&#39;n_messages&#39;].rank(method=&#39;dense&#39;,ascending=False)<br>df1<br><br># step 8 : <br>            # here raking column column_index[2] must at start <br>            # df1.iloc[:,[0,1,2]]<br>df1.iloc[:,[2,0,1]]",2022-12-20T12:08:21Z
UgxpBDpMfk4Rz-HwmHF4AaABAg,@shashankemani1609,,1,a8T88wCrmAQ,0,2,2022-11-28T19:42:06Z,thank you!,2022-11-28T19:42:06Z
UgzCZq1WtIMlBM8UbYl4AaABAg,@faisalbargi5903,,1,_XbrEcgQYSA,0,0,2023-03-28T17:19:52Z,Thanks for your videos..i think anyone ca crack interviews after practicing some questions from here..very beneficial,2023-03-28T17:19:52Z
UgxBDiaiPOTxKOdDInF4AaABAg,@mdriad4521,,1,_XbrEcgQYSA,1,0,2023-01-28T17:33:09Z,Please make a playlist for interview question which frequently asked by tech company.,2023-01-28T17:33:09Z
UgxBDiaiPOTxKOdDInF4AaABAg.9lRQBd30S5J9lS9iuN9XSr,@stratascratch,UgxBDiaiPOTxKOdDInF4AaABAg,2,_XbrEcgQYSA,0,0,2023-01-29T00:28:31Z,I think we do have a playlist like this!,2023-01-29T00:28:31Z
Ugzl81pGUyMa_umRJhx4AaABAg,@Aidan_Au,,1,_XbrEcgQYSA,0,4,2023-01-26T11:10:57Z,"I really like the new Data Projects. Thank you so much, StrataScratch team!",2023-01-26T11:10:57Z
UgyCfBoOHNCCXKt2mEB4AaABAg,@huanchenli4137,,1,f-9SnlALFac,0,0,2023-12-18T08:07:13Z,not sure if the lambda function like that will work on real python,2023-12-18T08:07:13Z
UgxbGqicEnhKMoTBzBd4AaABAg,@faisalbargi5903,,1,f-9SnlALFac,0,0,2023-03-28T18:11:40Z,"such a sweet voice <br><a href=""about:invalid#zCSafez""></a>",2023-03-28T18:11:40Z
Ugz0HeB4E0ksQpETPgV4AaABAg,@SACHINKUMAR-px8kq,,1,f-9SnlALFac,0,0,2023-02-02T07:15:59Z,Thankyou so much üòÉ,2023-02-02T07:15:59Z
UgwOQLp9PKC28GB61794AaABAg,@rollinas1,,1,f-9SnlALFac,1,0,2023-02-01T16:39:43Z,"you can probably fit the 3-5 steps into one with:<br>.groupby([&#39;customer_id&#39;, &#39;store_brand&#39;], as_index=False)<br>.agg(...)",2023-02-01T16:39:43Z
UgwOQLp9PKC28GB61794AaABAg.9lacFmG7VQi9lath0DJzot,@stratascratch,UgwOQLp9PKC28GB61794AaABAg,2,f-9SnlALFac,0,0,2023-02-01T19:12:07Z,"Yup, you can! I like to add a lot of functions like you did in my code. But for learning, it can be confusing since so many intermediate steps and outputs are not exposed. So we&#39;re breaking it down for you in the video. In practice, feel free to combine your code like you did.",2023-02-01T19:12:07Z
UgzDbsmPe_tX-9CvLqR4AaABAg,@johnalvinm,,1,YN-5NQO4iSU,1,0,2023-03-01T21:33:03Z,There is no Python language option in the question link.,2023-03-01T21:33:03Z
UgzDbsmPe_tX-9CvLqR4AaABAg.9mjF4etxc9a9mpPr16rKqZ,@faisalbargi5903,UgzDbsmPe_tX-9CvLqR4AaABAg,2,YN-5NQO4iSU,0,0,2023-03-04T07:02:36Z,now it is there,2023-03-04T07:02:36Z
UgyiI_2cQ-LFAKJgyUB4AaABAg,@DeadJDona,,1,2uu0prXMhfU,0,0,2023-04-12T19:44:18Z,—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Å–∫—É–ª–µ–º -_+,2023-04-12T19:44:18Z
UgzzLY4BLD7ObGtPG-l4AaABAg,@oizik,,1,Cg66PLiW9L0,0,0,2023-05-12T12:15:51Z,Fantastic overview of the ranking function and methods! Thank you,2023-05-12T12:15:51Z
UgwFardkZON_BX99-XN4AaABAg,@davidmatthewquindoza2349,,1,GSMCs3EmlmU,0,0,2023-12-08T03:20:51Z,thanks for this!!,2023-12-08T03:20:51Z
UgyDBbJvpsLGknQBhmF4AaABAg,@metinunlu_,,1,GSMCs3EmlmU,0,0,2023-11-29T20:59:57Z,Thank youu,2023-11-29T20:59:57Z
UgxCYtyIeiKQwigTzeR4AaABAg,@jorgesanabria6484,,1,GSMCs3EmlmU,0,0,2023-11-29T17:20:30Z,Perfect (streetfighter voice),2023-11-29T17:20:30Z
UgxUGRAHQiO_SYGfeK14AaABAg,@deniskolmykov3988,,1,oJglmpO_esU,0,0,2023-09-13T13:43:09Z,"if video doesn&#39;t have integrated marketing, probably the whole video is integrated marketing",2023-09-13T13:43:09Z
Ugw-fqjD__nEsl3p0Nl4AaABAg,@sandrahu7122,,1,oJglmpO_esU,1,0,2023-09-10T16:34:09Z,can you walk us through each project solution in the future?,2023-09-10T16:34:09Z
Ugw-fqjD__nEsl3p0Nl4AaABAg.9uUfINb8Ebz9uVok1sWTm7,@stratascratch,Ugw-fqjD__nEsl3p0Nl4AaABAg,2,oJglmpO_esU,0,0,2023-09-11T03:15:56Z,I have a few projects in the video list. Take a look! We&#39;ll make more in the future.,2023-09-11T03:15:56Z
UgxOW6lHk8f0eoMktzd4AaABAg,@quishzhu,,1,oJglmpO_esU,1,0,2023-09-08T13:05:53Z,This is exactly what i need and i love your platform so much (so much that I have done 700+ questions on it) :D,2023-09-08T13:05:53Z
UgxOW6lHk8f0eoMktzd4AaABAg.9uP8sFImXOM9uPQkjLCaDE,@stratascratch,UgxOW6lHk8f0eoMktzd4AaABAg,2,oJglmpO_esU,0,0,2023-09-08T15:42:08Z,Glad you enjoyed the video and have done so many questions on the platform!,2023-09-08T15:42:08Z
UgxRJ-1fI9m7KkScPQZ4AaABAg,@krimsonsun10,,1,oJglmpO_esU,3,5,2023-09-06T14:28:00Z,That moment you realize you know 80% of these  tools and just need to build a portfolio.,2023-09-06T14:28:00Z
UgxRJ-1fI9m7KkScPQZ4AaABAg.9uK8fwb2Mgj9uKICGXuU-b,@stratascratch,UgxRJ-1fI9m7KkScPQZ4AaABAg,2,oJglmpO_esU,0,2,2023-09-06T15:51:11Z,Keep building your portfolio so you know 100% of the tools =),2023-09-06T15:51:11Z
UgxRJ-1fI9m7KkScPQZ4AaABAg.9uK8fwb2Mgj9uL1W0orFYg,@krimsonsun10,UgxRJ-1fI9m7KkScPQZ4AaABAg,2,oJglmpO_esU,0,0,2023-09-06T22:44:35Z,"@@stratascratch thank you for this  video. helped me know where to  start. a lot  newbies like me struggle to know where to start and how to think through a problem ,  which tools/libraries to use. A video like that  would be really nice  too.",2023-09-06T22:44:35Z
UgxRJ-1fI9m7KkScPQZ4AaABAg.9uK8fwb2Mgj9uvWPWKhBgy,@user-mj4ms3wi5b,UgxRJ-1fI9m7KkScPQZ4AaABAg,2,oJglmpO_esU,0,1,2023-09-21T12:06:28Z,I feel in the same place. Most we he is commenting I feel I can achieve yet I have learned so much separately that I feel each phase of the projects as mini projects themselves.,2023-09-21T12:06:28Z
UgyOM-LYmH3-f5G-ERd4AaABAg,@AhmedAdel-TheAiologist,,1,kcTrK4xxhPk,0,1,2023-06-25T06:06:25Z,"First, it`s an awesome and handy project, we need some more informative vids explain how could we do some correct and quick algorithm experiments  to improve the performance of a model.<br>You also can help us by create a list of algorithms that matches each type of machine learning.<br><br>Thank you very much for your awesome effort ü•∞üôè",2023-06-25T06:06:25Z
UgwQjaYx761G6jU2Tnd4AaABAg,@user-dz9pc7gt9j,,1,Ih6G5hnn30Q,0,0,2023-07-19T00:28:35Z,Is the selection of number of leaf nodes dependent on the number of final classes we have?,2023-07-19T00:28:35Z
Ugxy34BfYkXcbnUxPBl4AaABAg,@carolinelu6721,,1,cM12QtrhdLo,1,0,2023-07-06T20:08:50Z,Does subscription means you get access to the raw python notebook?,2023-07-06T20:08:50Z
Ugxy34BfYkXcbnUxPBl4AaABAg.9rq6P2yUA5m9rqGnMJaRrL,@stratascratch,Ugxy34BfYkXcbnUxPBl4AaABAg,2,cM12QtrhdLo,0,0,2023-07-06T21:39:40Z,"Yes, you&#39;re able to download the datasets and access the python notebook solution with a subscription. There&#39;s also a few free data projects to download. Take a look before upgrading!",2023-07-06T21:39:40Z
UgygPVNlukxEDGSlwDR4AaABAg,@DenisKristak,,1,cM12QtrhdLo,0,0,2023-05-30T22:25:55Z,this video is so frickin underrated! :D,2023-05-30T22:25:55Z
UgwpBoL5qA0GOdTry1B4AaABAg,@philtoa334,,1,-Lzi4Qoy2v4,0,0,2023-06-04T09:41:37Z,Nice.,2023-06-04T09:41:37Z
UgwxlEI-0DYbKrmps9B4AaABAg,@arpitakar3384,,1,-Lzi4Qoy2v4,0,0,2023-04-23T06:38:59Z,please  provide your git hub repository,2023-04-23T06:38:59Z
UgzBu503YJojMruDgWZ4AaABAg,@kayodeoladele2928,,1,-Lzi4Qoy2v4,0,1,2023-03-22T03:02:43Z,"Quality video, just found your channel.",2023-03-22T03:02:43Z
UgxISJwRr5UuX-OSFSR4AaABAg,@shyamsridhar8407,,1,pqQG_tNXncc,0,0,2023-10-05T08:42:14Z,The RMSE for no scaling is not calculated I guess bcoz its kind of repetitive,2023-10-05T08:42:14Z
UgxQfvHEStALWZHO6814AaABAg,@user-qc3qu2bu8z,,1,pqQG_tNXncc,0,0,2023-03-19T17:50:27Z,RMSE of LGBM is 1079 does this mean the delivery duration on an average is off by 1079 seconds (~18 minutes),2023-03-19T17:50:27Z
UgypQfvpAk8fP5HFRFZ4AaABAg,@johnalvinm,,1,pqQG_tNXncc,1,0,2023-01-21T20:16:58Z,RMSE of LGBM was 1079 and RMSE of Neural Network 1. 05. Why was LGBM chosen as the final model. Should Neural Network model be chosen?,2023-01-21T20:16:58Z
UgypQfvpAk8fP5HFRFZ4AaABAg.9l9gNP0Kzqm9lC_i91lT80,@stratascratch,UgypQfvpAk8fP5HFRFZ4AaABAg,2,pqQG_tNXncc,0,0,2023-01-22T23:16:30Z,"I don&#39;t see the NN RMSE what was the score (I think you have a typo in your message). Either way, seems like the results are similar so it doesn&#39;t matter in practice.",2023-01-22T23:16:30Z
UgxcjaLlfkQ1xQhZ53t4AaABAg,@Chefanieee,,1,gh5JzALBQvU,0,1,2023-06-18T07:13:05Z,I&#39;m still unclear what the purpose of PCA was. IMO the interpretation of the results weren&#39;t thoroughly explained.,2023-06-18T07:13:05Z
UgyzS3tzDQtSRWdCz-94AaABAg,@ridhamshah7217,,1,gh5JzALBQvU,0,0,2023-03-14T18:39:58Z,I installed statsmodel library but compute_vif function code cell is taking more than 5 minutes to run.any idea what&#39;s the issue?,2023-03-14T18:39:58Z
UgxkjByJFau-g8NOaTt4AaABAg,@philtoa334,,1,gh5JzALBQvU,0,0,2023-02-26T15:04:37Z,Really Nice.,2023-02-26T15:04:37Z
UgzoBOGZuOo9RUCAHJR4AaABAg,@DiegoSilva-kk8mv,,1,gh5JzALBQvU,4,0,2023-01-07T13:02:38Z,You have no idea how this tutorial was helpful to me. Thx,2023-01-07T13:02:38Z
UgzoBOGZuOo9RUCAHJR4AaABAg.9k_rY0jTOvY9kbXzTBXOvE,@stratascratch,UgzoBOGZuOo9RUCAHJR4AaABAg,2,gh5JzALBQvU,0,1,2023-01-08T04:41:30Z,Glad to hear. What do you think about the level of detail? We kept it at a high(ish) level and focused more on theory and the &quot;why&quot; you are doing certain things. We didn&#39;t go in-depth and explain every line of code.,2023-01-08T04:41:30Z
UgzoBOGZuOo9RUCAHJR4AaABAg.9k_rY0jTOvY9kfX56et8um,@DiegoSilva-kk8mv,UgzoBOGZuOo9RUCAHJR4AaABAg,2,gh5JzALBQvU,0,0,2023-01-09T17:50:38Z,@@stratascratch I think that this way of teach is great. Because by making a solid foundation with the theory the code can flow naturally,2023-01-09T17:50:38Z
UgzoBOGZuOo9RUCAHJR4AaABAg.9k_rY0jTOvY9m7U-Rth7cO,@ryanm.6790,UgzoBOGZuOo9RUCAHJR4AaABAg,2,gh5JzALBQvU,0,1,2023-02-14T20:13:01Z,"‚Äã@@stratascratch As a data science teacher I&#39;ve been looking for reliable learning resources to show students. There&#39;s just one problem with your material, it&#39;s actually the lack of &quot;why.&quot; In this series you never really address why you were removing things, it felt very much like you knew a deeper explanation that you didn&#39;t share, or just are copying code without knowing why yourself. VIF score is the red flag in this video. While it&#39;s a deeper topic the reality is there are many recommended VIF thresholds, so when you choose 20 you need to clarify exactly why, because viewers will google it and quickly realize many articles contradict your method of choosing 20. Now the validity of your material is thrown into question and there&#39;s no way for you to defend your reasoning because it&#39;s a video and they&#39;ll likely just move on without questioning you.<br><br>Balancing high level and low level is naturally the hardest part of teaching so I completely get it. Keep at it, these videos are really promising!",2023-02-14T20:13:01Z
UgzoBOGZuOo9RUCAHJR4AaABAg.9k_rY0jTOvY9m7Xr1e8C3t,@stratascratch,UgzoBOGZuOo9RUCAHJR4AaABAg,2,gh5JzALBQvU,0,1,2023-02-14T20:46:41Z,@@ryanm.6790 Thank you for the feedback! This is exactly the type of input we need to improve our videos. We struggle with how much detail to communicate to our viewers. We will definitely keep this in mind for future videos. We aim to publish a data science project once a month and split it into several parts. So stay tuned and thank you.,2023-02-14T20:46:41Z
UgwBEvRT6JqczYklCSx4AaABAg,@fortask5897,,1,gh5JzALBQvU,0,1,2023-01-06T05:44:53Z,Amaing effort keep going.,2023-01-06T05:44:53Z
Ugy2ZxBir26Cy5VPDFd4AaABAg,@VV-gu9bd,,1,m3zEV10qvE8,0,0,2023-11-24T08:28:03Z,"Hi, I understand removing items with high corr coef but why say those with a coeff of .33 as correlated? Isn&#39;t it only low correlations",2023-11-24T08:28:54Z
Ugxcg1iK0uVHVmww1ct4AaABAg,@user-ib5jn8jk3z,,1,m3zEV10qvE8,1,0,2023-06-02T15:33:28Z,"When creating dummy variables using get_dummies, why was one dummy variable not dropped? (using drop_first or choosing a specific column to drop)",2023-06-02T15:33:28Z
Ugxcg1iK0uVHVmww1ct4AaABAg.9qT3rR0ha0N9qaPNEvKWk4,@stratascratch,Ugxcg1iK0uVHVmww1ct4AaABAg,2,m3zEV10qvE8,0,0,2023-06-05T21:15:22Z,"Hey, I don&#39;t understand. Which dummy variable was not dropped that you felt should be?",2023-06-05T21:15:22Z
UgyepXbGO006cPR7cMV4AaABAg,@sanketalane2033,,1,m3zEV10qvE8,0,0,2023-04-22T06:14:00Z,Thank You.,2023-04-22T06:14:00Z
UgwAgGl2CEZgm1hob8F4AaABAg,@SP-db6sh,,1,m3zEV10qvE8,1,3,2022-12-30T03:57:57Z,Post a video on <br>1.hyperparameter tuning &amp;<br>2. model retraining based upon checking the explanation of model predictions with Shap.<br>Soon people will start taking subscription from Strata scratch.,2022-12-30T03:57:57Z
UgwAgGl2CEZgm1hob8F4AaABAg.9kFHqbZ2YqS9kGfKY5p9Oy,@stratascratch,UgwAgGl2CEZgm1hob8F4AaABAg,2,m3zEV10qvE8,0,0,2022-12-30T16:51:08Z,Thanks for the suggestions. We plan on releasing more data projects videos and will try to find projects that have these 2 components.,2022-12-30T16:51:08Z
UgykpmDQCUR9j-YDtSN4AaABAg,@emmanuelczarpascua6721,,1,m3zEV10qvE8,1,1,2022-12-29T17:38:21Z,Thank you for this.,2022-12-29T17:38:21Z
UgykpmDQCUR9j-YDtSN4AaABAg.9kEAwTtRCbc9kEQrLOC6pT,@stratascratch,UgykpmDQCUR9j-YDtSN4AaABAg,2,m3zEV10qvE8,0,0,2022-12-29T19:57:27Z,Thank you for watching! Parts 3 and 4 will come out next week!,2022-12-29T19:57:27Z
UgyPdE--323fwqNsBiR4AaABAg,@Chefanieee,,1,Sf6jn8QZHhc,3,2,2023-06-09T04:41:15Z,"some of the code is blocked off since they need to scroll to the right to see the remaining code. For example, when calculating &quot;actual total delivery duration&quot; you only see (historical_data[&quot;actual_delivery_time&quot;] - historical_data[&quot;created_at&quot;]) but it doesn&#39;t show how thats converted using datetime. if they just scrolled to the right you could see the entire code. this happens a couple times throughout the video and makes it difficult to following along, especially since I don&#39;t know how to use the datetime to covert the duration to seconds",2023-06-09T04:41:15Z
UgyPdE--323fwqNsBiR4AaABAg.9qivmYJpYx19qkZ-6_8g82,@stratascratch,UgyPdE--323fwqNsBiR4AaABAg,2,Sf6jn8QZHhc,0,1,2023-06-09T19:51:51Z,Apologies about this. We&#39;ll make sure to show the entire code when we&#39;re filming. You can go on the platform and download the solution to see all the code.,2023-06-09T19:51:51Z
UgyPdE--323fwqNsBiR4AaABAg.9qivmYJpYx19qkZlO8Los5,@Chefanieee,UgyPdE--323fwqNsBiR4AaABAg,2,Sf6jn8QZHhc,0,0,2023-06-09T19:58:35Z,@@stratascratch thanks for the quick reply! I really appreciate you taking my recommendation. I tried to download the solution but hit a paywall. I‚Äôm not ready to upgrade but i will in the future when searching for jobs,2023-06-09T19:58:35Z
UgyPdE--323fwqNsBiR4AaABAg.9qivmYJpYx19w79jiQ1gNN,@vijaykumarcn8425,UgyPdE--323fwqNsBiR4AaABAg,2,Sf6jn8QZHhc,0,0,2023-10-21T06:29:54Z,dt.total_seconds() use this method to convert timedelta values into seconds,2023-10-21T06:30:27Z
Ugzgce9mAQ2apsMPzIN4AaABAg,@justinchen4152,,1,Sf6jn8QZHhc,0,0,2023-03-08T20:37:39Z,Why use ratio of busy dashers and onshift dashers as opposed to subtracting the values?,2023-03-08T20:37:39Z
UgzN5iwViOtTpSLib354AaABAg,@rizalrifai6010,,1,Sf6jn8QZHhc,1,0,2023-02-16T22:10:59Z,Can u tell me the reason why dont use one hot encode from sklearn instead get dummies because from i read one hot encode can handle more easly variance for test data for example if the test data on the diffrence dataframe which unknow categorycal value,2023-02-16T22:11:35Z
UgzN5iwViOtTpSLib354AaABAg.9mCq5BDhMOH9mD5mkOjAwF,@stratascratch,UgzN5iwViOtTpSLib354AaABAg,2,Sf6jn8QZHhc,0,0,2023-02-17T00:36:52Z,We just chose to do it in our way. There&#39;s no right or wrong with how you want to do the projects. Just be sure to defend what you did.,2023-02-17T00:36:52Z
UgyMjdvXU01TBRUdbAp4AaABAg,@rizalrifai6010,,1,Sf6jn8QZHhc,2,0,2023-02-10T14:17:22Z,"from this video, we saw that on features we trying to change it into numerical type, is there&#39;s a reason why we don&#39;t use feature besides numerical, or its because the machine learning model only accepts numerical variable ?",2023-02-10T14:17:22Z
UgyMjdvXU01TBRUdbAp4AaABAg.9lxY72VqWVq9m05BT0dSbq,@stratascratch,UgyMjdvXU01TBRUdbAp4AaABAg,2,Sf6jn8QZHhc,0,0,2023-02-11T23:21:31Z,"Yes that&#39;s right. Most machine learning models only accepts numerical variables. Even if it&#39;s a category, it&#39;s often changed into numbers. That&#39;s the only way the model can run.",2023-02-11T23:21:31Z
UgyMjdvXU01TBRUdbAp4AaABAg.9lxY72VqWVq9mCmR86s6IQ,@rizalrifai6010,UgyMjdvXU01TBRUdbAp4AaABAg,2,Sf6jn8QZHhc,0,0,2023-02-16T21:39:01Z,Thanks for answered my question.,2023-02-16T22:10:12Z
UgwhdTt6hRzv-AmWI0B4AaABAg,@rizalrifai6010,,1,Sf6jn8QZHhc,1,0,2023-02-09T21:51:33Z,"can i ask it about the actual_total_delivery_duration columns, the result of that column will be like  0 days <a href=""https://www.youtube.com/watch?v=Sf6jn8QZHhc&amp;t=01h02m59s"">01:02:59</a>  or should we just convert it into total second ?",2023-02-09T21:51:33Z
UgwhdTt6hRzv-AmWI0B4AaABAg.9lvmIoM2CiF9m04dHYUkcN,@stratascratch,UgwhdTt6hRzv-AmWI0B4AaABAg,2,Sf6jn8QZHhc,0,0,2023-02-11T23:16:43Z,"Yes, you can convert into total seconds. Then convert it to any unit of time, depending on what you want in the end. Just make sure you know what all the units are.",2023-02-11T23:16:43Z
Ugzx44x2cYTy6UDV7Cx4AaABAg,@MunaExplores,,1,Sf6jn8QZHhc,0,0,2023-01-20T06:01:31Z,Wonderful project...can you please add the dataset so we can follow through.  That&#39;d be great üëç,2023-01-20T06:01:31Z
UgyM1Ql9tsP0DZR_G3N4AaABAg,@emmanuelczarpascua6721,,1,Sf6jn8QZHhc,3,0,2022-12-29T08:57:51Z,"Hi, loved the video. when are you going to release the second part?",2022-12-29T08:57:51Z
UgyM1Ql9tsP0DZR_G3N4AaABAg.9kDFNFrPtuq9kEQdLmCU9j,@stratascratch,UgyM1Ql9tsP0DZR_G3N4AaABAg,2,Sf6jn8QZHhc,0,1,2022-12-29T19:55:33Z,"It&#39;s already released! <a href=""https://www.youtube.com/watch?v=m3zEV10qvE8"">https://www.youtube.com/watch?v=m3zEV10qvE8</a>. Part 3 and 4 will be released this week or next.",2022-12-29T19:55:33Z
UgyM1Ql9tsP0DZR_G3N4AaABAg.9kDFNFrPtuq9kJluJ0_QOo,@emmanuelczarpascua6721,UgyM1Ql9tsP0DZR_G3N4AaABAg,2,Sf6jn8QZHhc,0,0,2022-12-31T21:46:18Z,"@@stratascratch I just want to know why did we need to drop the actual total delivery duration. Tho, it looks like output from cell [49] didn&#39;t really dropped it. Any help clarifying this for me will be appreciated. Thanks!",2022-12-31T21:50:21Z
UgyM1Ql9tsP0DZR_G3N4AaABAg.9kDFNFrPtuq9kU1hKbLLye,@stratascratch,UgyM1Ql9tsP0DZR_G3N4AaABAg,2,Sf6jn8QZHhc,0,0,2023-01-04T21:25:29Z,"@@emmanuelczarpascua6721 this column is actually not being removed. It&#39;s a target variable so it wouldn&#39;t make sense to remove it. The problem is that there is a mistake in a block comment in cell [49] of the video (<a href=""https://www.youtube.com/watch?v=Sf6jn8QZHhc&amp;t=07m44s"">07:44</a> timestamp) - this comment says that we remove, among the others, this target variable column. But the mistake is only in the comment and the code of that cell is correct and is not removing this column.",2023-01-04T21:25:29Z
UgyDDe2sUjvXatilVZt4AaABAg,@Aidan_Au,,1,Sf6jn8QZHhc,1,3,2022-12-27T14:12:44Z,I noticed that many thumbnails of videos are changed. Thank you so much for making these! <br>The new theme/color scheme looks great!,2022-12-27T14:12:44Z
UgyDDe2sUjvXatilVZt4AaABAg.9k8eon8bKA-9k8wozzCY47,@stratascratch,UgyDDe2sUjvXatilVZt4AaABAg,2,Sf6jn8QZHhc,0,1,2022-12-27T16:50:03Z,Glad you like them!  Thanks for noticing.,2022-12-27T16:50:03Z
UgwUH1wyKivoAjxHgVF4AaABAg,@selena30201,,1,Sf6jn8QZHhc,1,0,2022-12-22T13:35:31Z,Love the video! Very helpful. Would love to see more videos that walk through the solutions to Python data projects.,2022-12-22T13:35:31Z
UgwUH1wyKivoAjxHgVF4AaABAg.9jwi_mS4_df9jxBiy9vgJr,@stratascratch,UgwUH1wyKivoAjxHgVF4AaABAg,2,Sf6jn8QZHhc,0,2,2022-12-22T17:58:54Z,"That&#39;s the plan! You can expect more python data projects on our channel. We&#39;ll split the projects into bite size pieces. For example, this DoorDash project has 4 episodes that we&#39;ll release back-to-back.",2022-12-22T17:58:54Z
Ugx5hTqC_9mo-_WtyXh4AaABAg,@llKaiserx0ll,,1,Sf6jn8QZHhc,1,1,2022-12-22T06:31:04Z,"üëç thanks, I wish you&#39;d explain the methods just a it more.",2022-12-22T06:31:04Z
Ugx5hTqC_9mo-_WtyXh4AaABAg.9jvy002Hudc9jwqBbZVMOP,@stratascratch,Ugx5hTqC_9mo-_WtyXh4AaABAg,2,Sf6jn8QZHhc,0,0,2022-12-22T14:41:59Z,Thank you very much for the feedback.  We&#39;ll aim to focus more on explaining the methods in future videos. We&#39;re still experimenting with how much detail to give.,2022-12-22T17:58:00Z
Ugx7-DsYk0hnRPcNOt14AaABAg,@varshithreddy8236,,1,AM4nPwvUtTo,1,1,2022-12-18T13:43:07Z,thanks for this  video,2022-12-18T13:43:07Z
Ugx7-DsYk0hnRPcNOt14AaABAg.9jmRGxNjiDb9jmwaT2EXDe,@stratascratch,Ugx7-DsYk0hnRPcNOt14AaABAg,2,AM4nPwvUtTo,0,0,2022-12-18T18:25:32Z,Most welcome,2022-12-18T18:25:32Z
Ugwp7ClF9IMz5UWQkcp4AaABAg,@swapniljikar1914,,1,AM4nPwvUtTo,1,0,2022-12-16T12:57:29Z,"Thanks for sharing this video!!!üôåüòä<br>And for the project, how about &quot;Credit Score Classification&quot; ?",2022-12-16T12:57:29Z
Ugwp7ClF9IMz5UWQkcp4AaABAg.9jhCSyNsPRD9jk-fAGmz3i,@stratascratch,Ugwp7ClF9IMz5UWQkcp4AaABAg,2,AM4nPwvUtTo,0,1,2022-12-17T15:03:24Z,Great suggestion!,2022-12-17T15:03:24Z
UgxTUVmUh0k9L9FLJj14AaABAg,@TheLeoGamer17,,1,AM4nPwvUtTo,0,0,2022-12-15T22:30:23Z,Great video!<br>Thank you so much for this!,2022-12-15T22:30:23Z
UgyGpXHgJ6lxw9NKI6p4AaABAg,@stuckcamping,,1,sVdNVgtpkD4,0,0,2022-10-08T04:25:31Z,Why do this in Python instead of importing it into SQL?,2022-10-08T04:25:31Z
Ugx-AOyaZ0sne1sEef94AaABAg,@Headlikeanorange84,,1,sVdNVgtpkD4,0,1,2022-10-06T06:05:19Z,Thank you so much. This content is gold! I hope you know how much you&#39;re helping/teaching people with it.,2022-10-06T06:05:19Z
UgxGeT_AhutgbsSx5ER4AaABAg,@paularaja,,1,sVdNVgtpkD4,0,1,2022-10-04T14:09:55Z,Now this is the type of challenge a newbie to Python can really find useful from a practical experience. Thank you guys.,2022-10-04T14:09:55Z
Ugx3fuUAnkCWMkmm7bR4AaABAg,@pup-arazzisworld3469,,1,sVdNVgtpkD4,1,2,2022-09-27T19:49:11Z,Hey thanks for the wonderful explanation. Could you PLEASE share the code as well?,2022-09-27T19:49:11Z
Ugx3fuUAnkCWMkmm7bR4AaABAg.9gUwzP0mm0P9gdHTVzICHh,@stratascratch,Ugx3fuUAnkCWMkmm7bR4AaABAg,2,sVdNVgtpkD4,0,1,2022-10-01T10:50:06Z,"You can check the links in the description that will bring you to the data projects page. You can also check this link, <a href=""https://platform.stratascratch.com/coding?utm_source=youtube&amp;utm_medium=click&amp;utm_campaign=YT+description+link%29&amp;code_type=1"">https://platform.stratascratch.com/coding?utm_source=youtube&amp;utm_medium=click&amp;utm_campaign=YT+description+link%29&amp;code_type=1</a>",2022-10-01T10:50:06Z
Ugz7RwERRuUgpAMWIa94AaABAg,@abogame6852,,1,sVdNVgtpkD4,0,1,2022-09-26T15:29:54Z,"Why would u give up, it is scary like everytNice tutorialng new but when u get good you will be glad u tried",2022-09-26T15:29:54Z
UgxpcCpte76_fOkyEhR4AaABAg,@diaconescutiberiu7535,,1,sVdNVgtpkD4,1,1,2022-09-23T06:53:39Z,"Yet another brilliant material. Thank you for this, I&#39;ve learned some new tricks (.at ...I&#39;m looking at you :)) )",2022-09-23T06:53:39Z
UgxpcCpte76_fOkyEhR4AaABAg.9gJG2kjS7GY9gK2quL8df0,@stratascratch,UgxpcCpte76_fOkyEhR4AaABAg,2,sVdNVgtpkD4,0,0,2022-09-23T14:17:33Z,Thanks and you&#39;re welcome.,2022-09-23T14:17:33Z
UgyYXda6jdhnhEsZfg94AaABAg,@kadourkadouri3505,,1,sVdNVgtpkD4,0,3,2022-09-22T21:45:54Z,"Exceptional content however, it would have been even better if there was not any music in the background.",2022-09-22T21:45:54Z
Ugy0uB9cZ5xmHNSQ8sN4AaABAg,@dwaipayansaha4443,,1,sVdNVgtpkD4,3,3,2022-09-22T19:17:34Z,Can you please provide the dataset?,2022-09-22T19:18:55Z
Ugy0uB9cZ5xmHNSQ8sN4AaABAg.9gI0OP7fPfx9gSvJRsjBxM,@stratascratch,Ugy0uB9cZ5xmHNSQ8sN4AaABAg,2,sVdNVgtpkD4,0,1,2022-09-27T00:56:06Z,"Yup, there&#39;s a link in the description!",2022-09-27T00:56:06Z
Ugy0uB9cZ5xmHNSQ8sN4AaABAg.9gI0OP7fPfx9gYLk39v-9_,@monkeydluffy1156,Ugy0uB9cZ5xmHNSQ8sN4AaABAg,2,sVdNVgtpkD4,0,0,2022-09-29T03:32:00Z,@@stratascratch I can&#39;t find in the discription ;(,2022-09-29T03:32:00Z
Ugy0uB9cZ5xmHNSQ8sN4AaABAg.9gI0OP7fPfx9g_-hrA2xvf,@stratascratch,Ugy0uB9cZ5xmHNSQ8sN4AaABAg,2,sVdNVgtpkD4,0,0,2022-09-29T18:57:57Z,"@@monkeydluffy1156 Here it is <a href=""https://platform.stratascratch.com/data-projects/marketing-campaign-results"">https://platform.stratascratch.com/data-projects/marketing-campaign-results</a> Sorry about that. It was just released today. The dataset and solutions are under a paywall but the project description is free for anyone to see",2022-09-29T18:57:57Z
UgzwVRejruOaNKd0xgh4AaABAg,@Trazynn,,1,sVdNVgtpkD4,1,2,2022-09-22T17:56:21Z,"Would love some projects on file (text, image, pdf) handling, generating and such. So many companies ask for it.",2022-09-22T17:56:21Z
UgzwVRejruOaNKd0xgh4AaABAg.9gHs5_tiNrp9gHziscWiGQ,@stratascratch,UgzwVRejruOaNKd0xgh4AaABAg,2,sVdNVgtpkD4,0,0,2022-09-22T19:03:01Z,"We do have a few videos on handling text files and uploading them to dbs. We also have a catalog of data projects on <a href=""http://www.stratascratch.com/"">www.stratascratch.com</a> and will continue to keep adding projects to our list. We&#39;ll make sure to cover your requests. Thanks for the feedback!",2022-09-22T19:03:01Z
Ugz0NRz-kddbwavdylt4AaABAg,@ramaswamyg5919,,1,sVdNVgtpkD4,0,1,2022-09-22T16:40:29Z,This is brilliant.,2022-09-22T16:40:29Z
UgwjxuL5cFfCy2yyDQx4AaABAg,@quishzhu,,1,JDOAcKTy9Mk,0,0,2023-08-31T23:41:30Z,Love it!,2023-08-31T23:41:30Z
UgxHLnaEYUDHjHHDp754AaABAg,@williamausenka9251,,1,JDOAcKTy9Mk,0,1,2022-09-01T23:55:26Z,does anyone have the link for the amazon API (docs)?,2022-09-01T23:55:26Z
Ugyv37hXmq88YxCUScB4AaABAg,@mubarakhamza7141,,1,JDOAcKTy9Mk,2,2,2022-08-28T19:13:40Z,"I see great improvement in your videos üí´, thanks a great deal for amazing contents always",2022-08-28T19:13:40Z
Ugyv37hXmq88YxCUScB4AaABAg.9fHd41y7eKx9fHflCBW97s,@stratascratch,Ugyv37hXmq88YxCUScB4AaABAg,2,JDOAcKTy9Mk,0,2,2022-08-28T19:37:10Z,"Thank you.  I am glad that you noticed.  Yeah, my team continues to improve on our content and video production.  We love providing you better content always.",2022-08-28T19:37:10Z
Ugyv37hXmq88YxCUScB4AaABAg.9fHd41y7eKx9fHmChPU2dI,@mubarakhamza7141,Ugyv37hXmq88YxCUScB4AaABAg,2,JDOAcKTy9Mk,0,1,2022-08-28T20:33:30Z,Good to know üòÅ,2022-08-28T20:33:30Z
Ugxs_v7-RmMKK3NV7e14AaABAg,@aashishmalhotra,,1,JDOAcKTy9Mk,1,0,2022-08-22T03:55:41Z,Wow thanku,2022-08-22T03:55:41Z
Ugxs_v7-RmMKK3NV7e14AaABAg.9f0YFFrhfE89fHpMdZbJwx,@stratascratch,Ugxs_v7-RmMKK3NV7e14AaABAg,2,JDOAcKTy9Mk,0,0,2022-08-28T21:01:04Z,You&#39;re welcome!,2022-08-28T21:01:04Z
UgxZP1iyrHjgPqrqBN94AaABAg,@awesomeGuss,,1,JDOAcKTy9Mk,1,2,2022-08-21T20:13:43Z,I&#39;ll try detecting fake news and one of my own ideas which are predicting top performers in fantasy premier league,2022-08-21T20:13:43Z
UgxZP1iyrHjgPqrqBN94AaABAg.9f-iNfxKa0I9hbWikLPn0y,@gerardoaboulafia7389,UgxZP1iyrHjgPqrqBN94AaABAg,2,JDOAcKTy9Mk,0,0,2022-10-25T14:56:18Z,did you finish the fpl one?,2022-10-25T14:56:18Z
Ugwl4WMv2RPMsR34DHV4AaABAg,@prome4985,,1,JDOAcKTy9Mk,1,5,2022-08-19T13:05:20Z,I will try Spotify and detecting fake news. Thanks guys,2022-08-19T13:05:20Z
Ugwl4WMv2RPMsR34DHV4AaABAg.9eunlKmF-DM9evbeJjQfHv,@stratascratch,Ugwl4WMv2RPMsR34DHV4AaABAg,2,JDOAcKTy9Mk,0,2,2022-08-19T20:38:45Z,That will be a great project!  Wonderful.,2022-08-19T20:38:45Z
Ugw1X0gAiJs5-jOF2Id4AaABAg,@69nukeee,,1,77IVf0zgmwI,0,0,2023-10-25T13:17:14Z,"This tutorial, along with the first part dealing with working with APIs, is one of the best ones I have ever watched here on YouTube, kudos Nate!¬†<br><br>I tried to follow your every step to train up, and the only thing I did different is using a free instance on ElephantSQL, as funnily enough I could not get anything for free in AWS Free Tier üôÑ<br><br>Still, your tutorials are truly amazing and have learnt a lot. I will definitely keep on coming for more tutorials/videos of yours!<br><br>Thank you very much!",2023-10-25T13:21:10Z
Ugw2zwGVgcgkC7gQ3sR4AaABAg,@hardikvig2723,,1,77IVf0zgmwI,1,0,2023-09-23T19:38:44Z,Where to get the name of database??,2023-09-23T19:38:44Z
Ugw2zwGVgcgkC7gQ3sR4AaABAg.9v0TkfBLcTo9v62791m8Kj,@stratascratch,Ugw2zwGVgcgkC7gQ3sR4AaABAg,2,77IVf0zgmwI,0,0,2023-09-25T23:32:42Z,It&#39;s your db that you need to setup so it can be any name you give it.,2023-09-25T23:32:42Z
UgwuLeHfvC-4X2--rxt4AaABAg,@quadrialli3715,,1,77IVf0zgmwI,0,0,2023-03-17T15:21:56Z,Great and detailed explanation,2023-03-17T15:21:56Z
UgyfzQXjUSzE4VR3k4d4AaABAg,@Digital-Light,,1,77IVf0zgmwI,2,0,2023-01-05T17:07:58Z,"i&#39;m geting this error &quot;DatatypeMismatch: column &quot;upload_date&quot; is of type date but expression is of type double precision<br>LINE 3: ... VALUES(&#39;mTL23Gd-T3g&#39;,&#39;Engagement_Ring_Animation&#39;,&#39;NaN&#39;::flo...&quot; please help",2023-01-05T17:07:58Z
UgyfzQXjUSzE4VR3k4d4AaABAg.9kW912rz-_c9kWCq6_GjH8,@stratascratch,UgyfzQXjUSzE4VR3k4d4AaABAg,2,77IVf0zgmwI,0,1,2023-01-05T17:41:17Z,The error is saying that you have a dtype mismatch in the upload_date column. So clean up the values in the column and get it to a dtype that the db is expecting.,2023-01-05T17:41:17Z
UgyfzQXjUSzE4VR3k4d4AaABAg.9kW912rz-_c9kWMB5cVpPk,@Digital-Light,UgyfzQXjUSzE4VR3k4d4AaABAg,2,77IVf0zgmwI,0,1,2023-01-05T19:02:55Z,@@stratascratch thank you for reply! i love your videos.,2023-01-05T19:02:55Z
Ugyu7_Yeiv6e8sfqcQF4AaABAg,@denyseperezdevera7188,,1,77IVf0zgmwI,2,0,2022-12-14T01:08:23Z,"Hi Nate, thank you so much for these amazing resources ! Do you know why when i try to connect the database i get&quot;OperationalError: FATAL:  database &quot;database-1&quot; does not exist&quot; ?",2022-12-14T01:08:54Z
Ugyu7_Yeiv6e8sfqcQF4AaABAg.9jamicNFGol9jb8vOEjZma,@stratascratch,Ugyu7_Yeiv6e8sfqcQF4AaABAg,2,77IVf0zgmwI,0,1,2022-12-14T04:31:06Z,Do you have a db? Did you create one for the project? You need to add your credentials.,2022-12-14T04:31:06Z
Ugyu7_Yeiv6e8sfqcQF4AaABAg.9jamicNFGol9jbXmDrUdTs,@denyseperezdevera7188,Ugyu7_Yeiv6e8sfqcQF4AaABAg,2,77IVf0zgmwI,0,0,2022-12-14T08:08:18Z,just solved my issue! i switched DB instance identifier with DB username.,2022-12-14T08:08:18Z
UgxCtPFKZ2aAB1gngHF4AaABAg,@wackyprofessor,,1,77IVf0zgmwI,1,2,2022-12-08T19:04:11Z,Just what I needed!   This will take you to a whole new level.  Thank you.,2022-12-08T19:38:30Z
UgxCtPFKZ2aAB1gngHF4AaABAg.9jOG3rDeauO9jONadJVj5l,@stratascratch,UgxCtPFKZ2aAB1gngHF4AaABAg,2,77IVf0zgmwI,0,0,2022-12-08T20:09:58Z,You are welcome.  Thank you for the nice comment. üòÄ,2022-12-08T20:09:58Z
UgyieIUSQOgeZ4OMGt14AaABAg,@RenatoCosta1,,1,77IVf0zgmwI,1,1,2022-12-05T00:53:06Z,"Hey Nate, amazing stuff! I think it would be great if you could do two more videos covering the topics you&#39;ve mentioned in you video about the only project one would need to get a job in data science. I guess the next videos would be about machine learning modelling and the last one on how to deploy it online. That would be awesome! Thanks anyway",2022-12-05T00:53:57Z
UgyieIUSQOgeZ4OMGt14AaABAg.9jE_opMUVTz9jH-0mpOvZI,@stratascratch,UgyieIUSQOgeZ4OMGt14AaABAg,2,77IVf0zgmwI,0,2,2022-12-05T23:20:32Z,"We&#39;re planning to publish 1 data project a month! When they are released, it&#39;d be great if you can provide feedback. For example, did we dive deep enough into the technical aspects of the project? Did we dive too deep? Is the video too long/too short? etc? This will help us iterate on the videos so we can provide something of value for all viewers. Thanks for the feedback!",2022-12-05T23:20:32Z
UgxCHQVo0jrJmAmsjLl4AaABAg,@hoanglam2814,,1,77IVf0zgmwI,4,0,2022-09-28T10:31:00Z,"Hi Nate, thanks for your videos. I have to retrieve data from almost 100 API and your video help a lot. But normally it takes me almost 10 hours to update all the data/records each day.<br><br>Yesterday, I just came across some articles about Webhooks, and it mentioned that Webhooks will push data whereas we have to pull data from API. What do you think if I can use Webhooks to update data and insert new data into table? It will only retrieve data when there are events happen, instead of retrieving all data like pulling through API.<br><br>For API, I think I can use it to retrieve old data from last year.",2022-09-28T10:31:42Z
UgxCHQVo0jrJmAmsjLl4AaABAg.9gWWu-5KAHd9gXb937F5nK,@stratascratch,UgxCHQVo0jrJmAmsjLl4AaABAg,2,77IVf0zgmwI,0,1,2022-09-28T20:36:07Z,"How many records do you have where it would take 10 hrs a day? If it takes that long, I wouldn&#39;t implement this method. Seems like you have a lot of data. I would use Airflow and setup DAGs to get those jobs scheduled. <br><br>Without an understanding of what type of data you&#39;re trying to move and for what purpose, I can&#39;t tell you if webhooks is the right way",2022-09-28T20:36:33Z
UgxCHQVo0jrJmAmsjLl4AaABAg.9gWWu-5KAHd9gYllt_PJpY,@hoanglam2814,UgxCHQVo0jrJmAmsjLl4AaABAg,2,77IVf0zgmwI,0,0,2022-09-29T07:28:11Z,"‚Äã@@stratascratch  I retrieve  about 40 thousand records each day, these are orders from a CRM system. My company helps around 100 clients manage their business, and each of them have one account in this CRM provided by a third party ( this means 100 API source). Actually, we are building an analytic system to work on it  (included ETL process and dashboard), but until that, I to collect data from 100 APIs one by one. So I tried to implement your method and it works.<br><br>But, yeah, like you said, it took too much time. I also did research on Airflow but I don&#39;t know what the difference between using Airflow and using a cron tool like Power automate/Zapier is. For me, Power automate/Zapier is much easier to learn.",2022-09-29T07:28:11Z
UgxCHQVo0jrJmAmsjLl4AaABAg.9gWWu-5KAHd9g_01mknGYV,@stratascratch,UgxCHQVo0jrJmAmsjLl4AaABAg,2,77IVf0zgmwI,0,1,2022-09-29T19:00:48Z,"@@hoanglam2814 The difference between Airflow and something like Zapier is that Zapier is good for low volume jobs that a non-technical person can setup. Airflow is the industry standard and can help manage huge databases and data lakes.<br><br>For your use case, an automated ETL process where you collect data from APIs seems like it should work just fine for 40K records. Just make sure you automate the pulls since there are 100 of them. You can try webhooks if you&#39;d like. I just don&#39;t have any experience using webhooks for your use case.",2022-09-29T19:00:48Z
UgxCHQVo0jrJmAmsjLl4AaABAg.9gWWu-5KAHd9g_gcfR93rs,@hoanglam2814,UgxCHQVo0jrJmAmsjLl4AaABAg,2,77IVf0zgmwI,0,0,2022-09-30T01:21:43Z,@@stratascratch Thank you for your short and concise answer :)) it is really helpful!,2022-09-30T01:21:43Z
Ugxw2rH4kjC-c-e412V4AaABAg,@JeffersonCanedo,,1,77IVf0zgmwI,1,1,2022-09-15T05:44:49Z,I so glad I found this video  Thankyou,2022-09-15T05:44:49Z
Ugxw2rH4kjC-c-e412V4AaABAg.9fzXoYj8pfq9g0JJS64JwR,@stratascratch,Ugxw2rH4kjC-c-e412V4AaABAg,2,77IVf0zgmwI,0,0,2022-09-15T22:16:35Z,You&#39;re welcome.,2022-09-15T22:16:35Z
UgxGWXvQdJ0fqM8wdTZ4AaABAg,@bodhid3091,,1,77IVf0zgmwI,0,1,2022-09-07T20:20:21Z,"Good video. A couple of questions:<br>Why do you store the new rows into another dataframe rather than calling insert_into_table directly?<br>Any reason you don&#39;t use the postgresql INSERT ON CONFLICT option to create an upsert, combining the insert and update into one call?",2022-09-07T20:20:21Z
UgzYmaqXszps9Gathq14AaABAg,@acsrr4288,,1,77IVf0zgmwI,2,1,2022-08-30T19:42:59Z,what library shall we use to load data to azure DB Cloud?,2022-08-30T19:42:59Z
UgzYmaqXszps9Gathq14AaABAg.9fMq0SntQvy9fN-m_S3Q87,@stratascratch,UgzYmaqXszps9Gathq14AaABAg,2,77IVf0zgmwI,0,0,2022-08-30T21:17:04Z,You can use various libraries and frameworks to connect.,2022-08-30T21:17:04Z
UgzYmaqXszps9Gathq14AaABAg.9fMq0SntQvy9fPJ4JlhbKs,@acsrr4288,UgzYmaqXszps9Gathq14AaABAg,2,77IVf0zgmwI,0,0,2022-08-31T18:44:08Z,@@stratascratch can you recommend a couple for us to explore<br>? thx,2022-08-31T18:44:08Z
Ugx3w9kr2gMkqvoXGfF4AaABAg,@khairulhakimi508,,1,77IVf0zgmwI,1,0,2022-08-26T11:32:53Z,"Im new to this field. I have one noob question. Why people(DA, DE, DS) using pyhton(PANDAS DATAFRAME) for data ingestion to transfer sql . Why not just upload the csv into the ssms using query or using import flat file?",2022-08-26T11:32:53Z
Ugx3w9kr2gMkqvoXGfF4AaABAg.9fBekLyHGap9fCDbuZtHIz,@stratascratch,Ugx3w9kr2gMkqvoXGfF4AaABAg,2,77IVf0zgmwI,0,1,2022-08-26T16:46:18Z,"It&#39;s basically to allow processing or manipulation of the column names and data, if you want to do that.",2022-08-26T16:46:18Z
Ugzvw_aPIs2b0sVI4vZ4AaABAg,@suprithm3802,,1,77IVf0zgmwI,1,0,2022-07-26T11:49:01Z,I am having data type clob i am not able to update data but the append function is working fine,2022-07-26T11:49:01Z
Ugzvw_aPIs2b0sVI4vZ4AaABAg.9dwrxNtN6RN9dwryBCKgKp,@suprithm3802,Ugzvw_aPIs2b0sVI4vZ4AaABAg,2,77IVf0zgmwI,0,0,2022-07-26T11:49:07Z,Any suggestions,2022-07-26T11:49:07Z
Ugyc1NzzT9x4st2oZv94AaABAg,@suprithm3802,,1,77IVf0zgmwI,0,0,2022-07-26T10:16:51Z,The code is not updating the only itz appending the new values,2022-07-26T10:16:51Z
UgywyRASusMyM14WRj54AaABAg,@yashgupte3044,,1,77IVf0zgmwI,4,0,2022-07-12T12:10:39Z,Getting a connection timed out error while connecting to the RDS instance. I have allowed all traffic in the inbound rules of the security group.,2022-07-12T12:10:39Z
UgywyRASusMyM14WRj54AaABAg.9dNrIeh29Mk9dOWxrQ6eNv,@stratascratch,UgywyRASusMyM14WRj54AaABAg,2,77IVf0zgmwI,0,0,2022-07-12T18:23:25Z,"Hi Yash, how about this link could help to troubleshoot. <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/rds-oracle-connection-errors/"">https://aws.amazon.com/premiumsupport/knowledge-center/rds-oracle-connection-errors/</a>",2022-07-12T18:23:25Z
UgywyRASusMyM14WRj54AaABAg.9dNrIeh29Mk9dcs1ANhsIH,@jakubjankovic4144,UgywyRASusMyM14WRj54AaABAg,2,77IVf0zgmwI,0,0,2022-07-18T17:24:51Z,Have you managed to solve it? I am having the same issue,2022-07-18T17:24:51Z
UgywyRASusMyM14WRj54AaABAg.9dNrIeh29Mk9dvKkJiM4xf,@llamashockz,UgywyRASusMyM14WRj54AaABAg,2,77IVf0zgmwI,0,0,2022-07-25T21:30:53Z,"having the same issue for hours, nothing seems to work to help fix it",2022-07-25T21:30:53Z
UgywyRASusMyM14WRj54AaABAg.9dNrIeh29Mk9dwK60I0f1w,@jakubjankovic4144,UgywyRASusMyM14WRj54AaABAg,2,77IVf0zgmwI,0,0,2022-07-26T06:44:29Z,@@llamashockz  I ended up using ElephantSQL instead of AWS to host the db and it resolved the issue for me.,2022-07-26T06:44:29Z
Ugx3J8qqxR_ydb_KyBV4AaABAg,@wisjnujudho3152,,1,77IVf0zgmwI,1,1,2022-07-06T12:37:44Z,"hi Nate, is there any alternative to aws rds, a cloud database server which doesn&#39;t require credit card even though it&#39;s free tier. alternatively, is it possible (maybe) to use heroku postgrest as database? thx",2022-07-06T23:10:09Z
Ugx3J8qqxR_ydb_KyBV4AaABAg.9d8ScgC9kiZ9dOVmu7XtGK,@stratascratch,Ugx3J8qqxR_ydb_KyBV4AaABAg,2,77IVf0zgmwI,0,1,2022-07-12T18:13:11Z,"Hi Winsjnu, I find this article helpful. <br> You might want to check out. <a href=""https://www.lastweekinaws.com/blog/10-free-cloud-databases-you-should-consider-and-1-you-shouldnt/"">https://www.lastweekinaws.com/blog/10-free-cloud-databases-you-should-consider-and-1-you-shouldnt/</a>",2022-07-12T18:13:11Z
UgxWvEmzmzzsXAJJiep4AaABAg,@andrew6233,,1,77IVf0zgmwI,3,0,2022-07-02T23:04:32Z,"Newbie question: what is the database manager you show at <a href=""https://www.youtube.com/watch?v=77IVf0zgmwI&amp;t=11m00s"">11:00</a>? Thank you so much for these video",2022-07-02T23:04:32Z
UgxWvEmzmzzsXAJJiep4AaABAg.9d-HB6Foehr9d-lVq9vd6Z,@stratascratch,UgxWvEmzmzzsXAJJiep4AaABAg,2,77IVf0zgmwI,0,1,2022-07-03T03:38:15Z,It&#39;s Datagrip!,2022-07-03T03:38:15Z
UgxWvEmzmzzsXAJJiep4AaABAg.9d-HB6Foehr9d-lYrw8pw8,@stratascratch,UgxWvEmzmzzsXAJJiep4AaABAg,2,77IVf0zgmwI,0,1,2022-07-03T03:38:40Z,It&#39;s my favorite BTW. I hate the in-browser db managers.,2022-07-03T03:38:40Z
UgxWvEmzmzzsXAJJiep4AaABAg.9d-HB6Foehr9d1OyWNjvrs,@andrew6233,UgxWvEmzmzzsXAJJiep4AaABAg,2,77IVf0zgmwI,0,0,2022-07-03T18:51:04Z,@@stratascratch Thanks! It looks really nice + convenient to work with compared to what I&#39;ve been using.,2022-07-03T18:51:04Z
Ugzv4rUlQp4sksDPln54AaABAg,@hoganbrinson8375,,1,77IVf0zgmwI,0,0,2022-06-21T22:31:59Z,"<a href=""https://www.youtube.com/watch?v=77IVf0zgmwI&amp;t=2m20s"">2:20</a> in and know that this is about to be the video for me.",2022-06-21T22:31:59Z
UgzvoAgLkbFWV3iy1IZ4AaABAg,@r33666,,1,77IVf0zgmwI,1,0,2022-06-06T11:42:43Z,Thanks for this. I&#39;ve almost made it all the way through but getting &quot;IndexError: tuple index out of range&quot; when trying to run the command that adds the 2nd df. Using a very simple csv.,2022-06-06T11:51:35Z
UgzvoAgLkbFWV3iy1IZ4AaABAg.9bw6UUswmnK9fzqjaOzuJY,@hoanglam2814,UgzvoAgLkbFWV3iy1IZ4AaABAg,2,77IVf0zgmwI,0,0,2022-09-15T08:38:54Z,I had it too. But I found that my SQL syntax needed to be fixed,2022-09-15T08:38:54Z
UgxIatNhaZmu5knjsup4AaABAg,@noobshady,,1,77IVf0zgmwI,1,0,2022-05-26T18:17:06Z,That was really helpful.,2022-05-26T18:17:06Z
UgxIatNhaZmu5knjsup4AaABAg.9bVUsUxpria9dOVqDrCzaE,@stratascratch,UgxIatNhaZmu5knjsup4AaABAg,2,77IVf0zgmwI,0,0,2022-07-12T18:13:38Z,Glad it was helpful!,2022-07-12T18:13:38Z
UgwxtzpN8c_egKb553R4AaABAg,@ShahnazMalik.,,1,77IVf0zgmwI,1,0,2022-05-05T04:50:51Z,"To insert or update in a table having millions of records in SQL database takes hours to complete. What is the best solution , please advise <br><br>Thank you.",2022-05-05T04:50:51Z
UgwxtzpN8c_egKb553R4AaABAg.9acyuyfbQBv9aeB1-EU6py,@stratascratch,UgwxtzpN8c_egKb553R4AaABAg,2,77IVf0zgmwI,0,1,2022-05-05T16:04:39Z,"That&#39;s normal. Sometimes jobs just take hours to complete. I would optimize for ensuring that the process doesn&#39;t break and if it does, you don&#39;t lose all the work. One way to do this is to batch the update/ inserts so that if the process does break half way through, you at least have updated/inserted half the records.",2022-05-05T16:04:39Z
UgzLarSvJclWDDARRWZ4AaABAg,@Davidkiania,,1,77IVf0zgmwI,1,0,2022-04-23T17:57:44Z,"Hi Nate, any chance you might show us how to automatically run the script? it is a cron job or how does that work? Thanks",2022-04-23T17:57:44Z
UgzLarSvJclWDDARRWZ4AaABAg.9aAUQh6F2b99aAjpKStknQ,@stratascratch,UgzLarSvJclWDDARRWZ4AaABAg,2,77IVf0zgmwI,0,0,2022-04-23T20:21:02Z,"Yeah, it&#39;s essentially a cron/scheduled job. I use Airflow at work but I&#39;ve seen others use Jenkins.",2022-04-23T20:21:02Z
Ugyy0v7xECbZdZtCwvd4AaABAg,@TripPiper,,1,77IVf0zgmwI,1,0,2022-04-06T02:36:59Z,If your connection to AWS fails try creating a new inbound security group that allows All Access. <br>But also learn how to create a connection with TCP or some form of security because in production you‚Äôd never have inbound connections on All access,2022-04-06T02:36:59Z
Ugyy0v7xECbZdZtCwvd4AaABAg.9_T3Y4FAIJl9ja_esRioA4,@denyseperezdevera7188,Ugyy0v7xECbZdZtCwvd4AaABAg,2,77IVf0zgmwI,0,0,2022-12-13T23:14:16Z,Hello! do you have any idea why it returns &quot; database &quot;database-1&quot; does not exist &quot; ?,2022-12-13T23:14:16Z
UgyUP-JPtFasdtmVvKF4AaABAg,@andrewraad5626,,1,77IVf0zgmwI,1,0,2022-03-28T15:38:03Z,"Hey Nate, thanks for this video. how do you deal with foreign keys? My csv file has strings instead of the foreign keys needed to upload to a specific table that uses foreign key constraints. What do you suggest?",2022-03-28T15:38:03Z
UgyUP-JPtFasdtmVvKF4AaABAg.9_7Hlsi4NQ99_7O-ZV8dOA,@stratascratch,UgyUP-JPtFasdtmVvKF4AaABAg,2,77IVf0zgmwI,0,0,2022-03-28T16:32:29Z,I have no idea how to fix your situation. I would just change the strings to FK. Some data manipulations will need to be done with your issue it seems.,2022-03-28T16:32:29Z
Ugx8_Y6h2ZMJegZr0Vt4AaABAg,@nqomuleya,,1,77IVf0zgmwI,0,0,2022-03-04T02:33:37Z,"InterfaceError: (pyodbc.InterfaceError) (&#39;IM002&#39;, &#39;[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)&#39;)",2022-03-04T02:33:37Z
UgwKnngKtU74w9cDuZ94AaABAg,@mcmoodoo,,1,77IVf0zgmwI,1,0,2022-02-24T20:35:02Z,"Why not just use pandas.to_sql(&quot;table_name&quot;, engine) which will create the table based on the dataframe object? You can also pass types dictionary to specify the exact types for the columns like varchar and integer?",2022-02-24T20:35:02Z
UgwKnngKtU74w9cDuZ94AaABAg.9YqQK6tdCeU9YrBsE6MmVI,@stratascratch,UgwKnngKtU74w9cDuZ94AaABAg,2,77IVf0zgmwI,0,1,2022-02-25T03:48:00Z,Yes you can do that. It doesn&#39;t always work the way I want it to when I use to_sql() and I often like to have QA checks along the way so my method helps with that. But there are multiple ways to do what I&#39;m doing. I would choose the option that allows you to get your work done!,2022-02-25T03:48:00Z
UgzkXY6atULJBohrGNh4AaABAg,@andreajaquelinerazoordaz6003,,1,77IVf0zgmwI,0,0,2022-02-13T21:56:18Z,"I already play back the video like 4 times just to watch the ads because I don&#39;t know how to thank you! Great channel, great video. :)",2022-02-13T21:56:18Z
UgwgH2_sHSmAC2-YDYx4AaABAg,@skyace173,,1,77IVf0zgmwI,1,0,2022-02-06T20:14:39Z,Hey Nate! Great video! How much time do you expect to get this finish if this is a project at work?,2022-02-06T20:14:39Z
UgwgH2_sHSmAC2-YDYx4AaABAg.9Y71g4uJdsk9Y78Y95BYsZ,@stratascratch,UgwgH2_sHSmAC2-YDYx4AaABAg,2,77IVf0zgmwI,0,0,2022-02-06T21:14:36Z,This is probably a 1-2 day project depending on what is being asked of me. But it shouldn&#39;t take too long if you already know what you&#39;re doing.,2022-02-06T21:14:36Z
Ugxz7dv8psV1ptQlf4Z4AaABAg,@reality8221,,1,77IVf0zgmwI,1,0,2022-02-02T00:02:33Z,"Hi Nate, Thanks for the great videos. Won&#39;t it be simpler to insert it as a part of 1st for loop; instead of creating a df and then iterating through once again. Would like to know your thoughts.",2022-02-02T00:02:33Z
Ugxz7dv8psV1ptQlf4Z4AaABAg.9XvZmpZkBHW9XxL3QZLI6T,@stratascratch,Ugxz7dv8psV1ptQlf4Z4AaABAg,2,77IVf0zgmwI,0,0,2022-02-02T16:32:21Z,"My thinking about the temp df was to only update the new videos and metrics, and leave the rest alone. I think that&#39;s what I did in this project. Hope that makes sense.",2022-02-02T16:32:21Z
UgybQfoROZsteBB0pNZ4AaABAg,@ruthfussee5503,,1,77IVf0zgmwI,0,0,2022-02-01T07:36:29Z,"Hi Nate, thank you for this video super useful was scanning through the internet for such a solution but could not find it. I am thinking what if there are many columns (e.g. 100 columns) wouldnt it be inefficient to type in row[&#39;column1], ..., row[&#39;column100]. ISsthere a way we can put the column information in a tuple, list or dictionary and passing through all this, like after SET also or vars_to_update.<br>another issue is where the excel column names have space (different from SQL columns) how do I then write the code as I keep having sytnax errors<br>Lastly, in the real world sometimes the excel files do not follow the template we stipulated, how should I design a validation check that is useful?",2022-02-01T12:18:12Z
Ugx4tRnTIGSeAWRnF0J4AaABAg,@MartinoxxHD,,1,77IVf0zgmwI,1,1,2022-01-20T23:35:17Z,"Nice vid, I actually learned new things! One question, what&#39;s the point on creating a temporary DF for the new videos and then UPDATE them on a new for lopp. Can&#39;t we just UPDATE and INSERT the values in the same for loop?",2022-01-20T23:35:17Z
Ugx4tRnTIGSeAWRnF0J4AaABAg.9XRc7fqtOOl9XSIhOpHWzh,@stratascratch,Ugx4tRnTIGSeAWRnF0J4AaABAg,2,77IVf0zgmwI,0,1,2022-01-21T05:56:02Z,I hold the new videos in a temp df so that I only need to upload the new videos or replace existing videos with new metrics. I&#39;m trying to stay away from updating all videos for performance. Then I do the update and insert all at once but it&#39;s only for the new vids. Hope that makes sense. Thanks for watching,2022-01-21T05:56:02Z
UgwX-pJtucEDJ1SCoAR4AaABAg,@amadysvlog3449,,1,77IVf0zgmwI,1,0,2022-01-05T23:56:10Z,Great video! Thank you! But question:<br>How do i get access to the Amazon RDS free databases? I created an account on aws but can&#39;t find any database-yt.,2022-01-05T23:56:10Z
UgwX-pJtucEDJ1SCoAR4AaABAg.9Wq1aR7SiXJ9Wq3UjBceLJ,@stratascratch,UgwX-pJtucEDJ1SCoAR4AaABAg,2,77IVf0zgmwI,0,1,2022-01-06T00:12:43Z,you&#39;ll need to create your own db. That&#39;s a whole topic by itself. But you can try it by going on aws and then creating a db on the RDS service.,2022-01-06T00:12:43Z
UgyVyk0StKfqs6evYSp4AaABAg,@awds121,,1,77IVf0zgmwI,0,1,2022-01-03T01:46:31Z,"Hey Nate! Great video! I have a few feedback. First, the videos are very helpful and you should definitely continue doing more videos like this. Here are some ideas: How to schedule scripts to run on AWS; create tables, analyze data, and create a dashboard. Second, the playlist that includes this video is unordered. It is not hard to figure out which video should be watched first, but it doesn&#39;t hurt to sort it in the order that you want videos to be watched. Third, AWS doesn&#39;t include PostgreSQL in the free tier (at least anymore). Just a heads up. Keep up the good work!",2022-01-03T01:46:31Z
Ugx8R8iCAo7sZjW49dJ4AaABAg,@SensiStarToaster,,1,77IVf0zgmwI,1,0,2021-12-23T14:18:00Z,"Nice video. But writing sql directly gets messy to maintain, why not use something like Sqlalchemy to separate your python code from the having to worry about the specific details of the specific  sql implementation and make it independent of the type of database manager used?",2021-12-23T14:18:00Z
Ugx8R8iCAo7sZjW49dJ4AaABAg.9WIX5Tye-xQ9WIp7zRkOBF,@stratascratch,Ugx8R8iCAo7sZjW49dJ4AaABAg,2,77IVf0zgmwI,0,1,2021-12-23T17:04:22Z,"yes, you can definitely do that. I haven&#39;t used sqlalchemy but have seen the documentation and agree with you. It could be a much better solution.",2021-12-23T17:04:22Z
UgyFpGCN1iz3D7CljzV4AaABAg,@williamausenka9251,,1,77IVf0zgmwI,0,1,2021-12-15T12:00:07Z,"Great video! One question, what ML implementation would you do with that data? I&#39;m trying to come up with something, but no success so far.",2021-12-15T12:00:07Z
Ugy6yKk7kWCJE3GuJJ14AaABAg,@toilinginobscurity3091,,1,77IVf0zgmwI,1,3,2021-12-05T11:00:16Z,There&#39;s really no rhyme or reason in the universe. This channel and these tutorials are gold but instead I am bombarded by ML channels playing with toy datasets and brand themselves as all you need clickbaits. You Sir are a saint.,2021-12-05T11:00:16Z
Ugy6yKk7kWCJE3GuJJ14AaABAg.9VZp9Wqhlze9V__CBiG7NO,@stratascratch,Ugy6yKk7kWCJE3GuJJ14AaABAg,2,77IVf0zgmwI,0,0,2021-12-05T18:00:04Z,haha =) I know exactly what you mean. Too many of those channels around.,2021-12-05T18:00:04Z
UgwBDyeebPeBWuPN6_Z4AaABAg,@codingstyle9480,,1,77IVf0zgmwI,5,11,2021-12-02T14:06:06Z,"Hi, Thank you for the video. I have a question: How would you go around  if you wanted to update your database in real time without you manually running the code. Perhaps we could design the code such that it is triggered when something changes in the source data  to insert or update data automatically in real-time(perhaps with some delay, of course)",2021-12-02T14:06:06Z
UgwBDyeebPeBWuPN6_Z4AaABAg.9VSR2-NuHpa9X_Zzheqg1h,@muhammedmarong6645,UgwBDyeebPeBWuPN6_Z4AaABAg,2,77IVf0zgmwI,0,0,2022-01-24T11:01:01Z,hey did you figure that out?,2022-01-24T11:01:01Z
UgwBDyeebPeBWuPN6_Z4AaABAg.9VSR2-NuHpa9Yc4iZlNd72,@pashlenenaidoo3695,UgwBDyeebPeBWuPN6_Z4AaABAg,2,77IVf0zgmwI,0,0,2022-02-19T06:56:54Z,I would also like to know if you have achieved this,2022-02-19T06:56:54Z
UgwBDyeebPeBWuPN6_Z4AaABAg.9VSR2-NuHpa9a7xgipE-Yf,@stratascratch,UgwBDyeebPeBWuPN6_Z4AaABAg,2,77IVf0zgmwI,0,1,2022-04-22T18:24:28Z,You would need something like Airflow or Jenkins to schedule the trigger.,2022-04-22T18:24:28Z
UgwBDyeebPeBWuPN6_Z4AaABAg.9VSR2-NuHpa9cOFfQJlkUM,@caioriet458,UgwBDyeebPeBWuPN6_Z4AaABAg,2,77IVf0zgmwI,0,0,2022-06-17T19:20:57Z,I would like to know if you solved this problem!,2022-06-17T19:21:06Z
UgwBDyeebPeBWuPN6_Z4AaABAg.9VSR2-NuHpa9cOQ8gK301G,@stratascratch,UgwBDyeebPeBWuPN6_Z4AaABAg,2,77IVf0zgmwI,0,1,2022-06-17T20:52:28Z,"@@caioriet458 Yes, I left a comment. You&#39;ll need to use a scheduler like Airflow in order to keep updating your db without manually running the code. The scheduler will run it for you.",2022-06-17T20:52:28Z
Ugy1sbegRk-Pqx4GV_R4AaABAg,@zynkers401,,1,77IVf0zgmwI,1,0,2021-11-06T14:24:27Z,Great video Nate! You really have the spirit for teaching. The vocabulary and content is easy to understand and follow. I hope Strata Scratch has a continued exponential growth.,2021-11-06T14:24:27Z
Ugy1sbegRk-Pqx4GV_R4AaABAg.9UPWTYPlTF29UQ1uJy2TV-,@stratascratch,Ugy1sbegRk-Pqx4GV_R4AaABAg,2,77IVf0zgmwI,0,1,2021-11-06T19:16:36Z,Thanks for the kind words! And thanks for watching!,2021-11-06T19:16:36Z
Ugy4p_pmpzd8vqbYl9p4AaABAg,@PATRICKCHUAD,,1,77IVf0zgmwI,1,0,2021-10-05T23:45:43Z,"Thanks Nath for your video on Pandas. I will apply this to my project. in python. Thanks,",2021-10-05T23:45:43Z
Ugy4p_pmpzd8vqbYl9p4AaABAg.9T87GS4JXHm9T8FNBn2xO0,@stratascratch,Ugy4p_pmpzd8vqbYl9p4AaABAg,2,77IVf0zgmwI,0,1,2021-10-06T00:56:32Z,"Great! Happy to help, thanks for watching!",2021-10-06T00:56:32Z
UgzZI6_SSuITdwBato94AaABAg,@anthonyobrienvicenciopalla641,,1,77IVf0zgmwI,0,0,2021-08-19T04:03:04Z,Hi!!! nice video. I have a problem with the code. the error message is:  (could not connect to server: Connection timed out Is the server running on host &quot;nameofhost&quot; (107.20.xx.xx) and accepting 	TCP/IP connections on port 5432?).<br> How I fix this problem??. thanks for answers,2021-08-19T04:03:04Z
UgzhG1G1jl05bL2Me4t4AaABAg,@jessinthamathew3032,,1,77IVf0zgmwI,1,1,2021-08-15T11:43:07Z,"Hi Nate! Really appreciate your content, its relevance and quality. So much to learn with each video! I wanted to ask why you opted for postgreSQL specifically? Can I use MS SQL server too?",2021-08-15T11:43:07Z
UgzhG1G1jl05bL2Me4t4AaABAg.9R3W1KMmUCU9R4P96qsD7N,@stratascratch,UgzhG1G1jl05bL2Me4t4AaABAg,2,77IVf0zgmwI,0,2,2021-08-15T20:02:15Z,"I&#39;m just used to postgres and it&#39;s open source. MS SQL is fine to learn as well. In the future, the platform will be able to handle both db engines but currently it only uses postgres. Thanks for watching!",2021-08-15T20:02:15Z
Ugx4pDSvOErRe3ANEzl4AaABAg,@prateek2159,,1,77IVf0zgmwI,1,18,2021-07-24T16:30:21Z,"Hey Nate, your videos are just too good. I love how your channel is so dedicated towards real word data science. By the way I noticed that you started a video series, &quot;For your Data Science Project&quot; and I really want you to continue making videos for this particular series because there&#39;s literally no one on YouTube with such guidance on DS projects and I have been looking for one since a very long time because I have my placements just after 12 months and I really want to make a full stack data science project. Thank you.",2021-07-24T16:30:21Z
Ugx4pDSvOErRe3ANEzl4AaABAg.9QBNPm_dYxt9QBus1MdvdR,@stratascratch,Ugx4pDSvOErRe3ANEzl4AaABAg,2,77IVf0zgmwI,0,6,2021-07-24T21:31:26Z,Thanks for the kind words! I have created a series of these videos from importing a pd df to a db to grabbing data from an API. I think most of the videos in the series should get you started on grabbing and managing data.Are there other types of skills you&#39;re interested in?,2021-07-24T21:31:26Z
Ugwrdqks1EEdTqx60T14AaABAg,@GauravKumar-xl1fs,,1,77IVf0zgmwI,1,1,2021-05-14T15:21:05Z,please make more video of this series and pipelines,2021-05-14T15:21:05Z
Ugwrdqks1EEdTqx60T14AaABAg.9NKR2LiDTvX9NKWMSxpxdl,@stratascratch,Ugwrdqks1EEdTqx60T14AaABAg,2,77IVf0zgmwI,0,0,2021-05-14T16:07:31Z,glad you liked the videos. Will do if there are more views!,2021-05-14T16:07:31Z
UgxZ2EvwSXSSplzDAiZ4AaABAg,@IrakliChitishvili,,1,77IVf0zgmwI,3,6,2021-05-06T12:57:44Z,How the hell  this channel isn&#39;t more popular?,2021-05-06T12:57:44Z
UgxZ2EvwSXSSplzDAiZ4AaABAg.9N-_HSWwYra9N0-i1h6WRV,@stratascratch,UgxZ2EvwSXSSplzDAiZ4AaABAg,2,77IVf0zgmwI,0,1,2021-05-06T16:57:25Z,I know right?! I guess not many people like to watch me code...,2021-05-06T16:57:25Z
UgxZ2EvwSXSSplzDAiZ4AaABAg.9N-_HSWwYra9N01gBipOoI,@IrakliChitishvili,UgxZ2EvwSXSSplzDAiZ4AaABAg,2,77IVf0zgmwI,0,4,2021-05-06T17:14:39Z,@@stratascratch I think it&#39;s just matter of time before your channel and the work you do takes off.,2021-05-06T17:14:39Z
UgxZ2EvwSXSSplzDAiZ4AaABAg.9N-_HSWwYra9NDYrx0iTeb,@flipcase,UgxZ2EvwSXSSplzDAiZ4AaABAg,2,77IVf0zgmwI,0,1,2021-05-11T23:14:44Z,I second that!,2021-05-11T23:14:44Z
UgzAnhTbohZGNoER2dR4AaABAg,@javierjdaza,,1,77IVf0zgmwI,3,2,2021-05-05T21:19:43Z,"Brother, never miss one video. this is pure gold!! keeo doing this please, can i connect with u in linked in? <br>thanks bro, ure the best",2021-05-05T21:19:43Z
UgzAnhTbohZGNoER2dR4AaABAg.9MytwHBnPWb9Myx5psDQL-,@stratascratch,UgzAnhTbohZGNoER2dR4AaABAg,2,77IVf0zgmwI,0,0,2021-05-05T21:47:23Z,"Thanks so much for watching! And glad you like my videos =) Yes please connect with me on LinkedIn. You can search for StrataScratch and I should pop-up (I think?). If not, let me know.",2021-05-05T21:47:23Z
UgzAnhTbohZGNoER2dR4AaABAg.9MytwHBnPWb9MzW9t4p9pq,@javierjdaza,UgzAnhTbohZGNoER2dR4AaABAg,2,77IVf0zgmwI,0,0,2021-05-06T03:02:30Z,@@stratascratch i cant find u. :(,2021-05-06T03:02:30Z
UgzAnhTbohZGNoER2dR4AaABAg.9MytwHBnPWb9Mz_rOUdGhL,@stratascratch,UgzAnhTbohZGNoER2dR4AaABAg,2,77IVf0zgmwI,0,0,2021-05-06T03:43:32Z,"@@javierjdaza Sorry about that. Here you go! <a href=""http://linkedin.com/in/nathanrosidi"">http://linkedin.com/in/nathanrosidi</a>",2021-05-06T03:43:32Z
Ugw_J4aiNNcB74HupO94AaABAg,@its_me7363,,1,77IVf0zgmwI,5,0,2021-05-05T18:39:16Z,much awaited video by myself...thanks for your time and effort Nate,2021-05-05T18:39:16Z
Ugw_J4aiNNcB74HupO94AaABAg.9Myb_22Rci69MynL6dtwDv,@stratascratch,Ugw_J4aiNNcB74HupO94AaABAg,2,77IVf0zgmwI,0,0,2021-05-05T20:22:05Z,"Thanks for watching! I just updated the description with a link to the github repo that has the final version of this project. I&#39;d definitely take a look at that and follow along with the notebook. I felt a bit too rushed on this video trying to explain all the concepts. But if there&#39;s anything to take away from this video, it&#39;s (1) how to properly structure your functions and (2) how to solve for memory &amp; performance issues when potentially dealing with millions and billions of rows of data.",2021-05-05T20:22:05Z
Ugw_J4aiNNcB74HupO94AaABAg.9Myb_22Rci69Myp1R_LCdU,@its_me7363,Ugw_J4aiNNcB74HupO94AaABAg,2,77IVf0zgmwI,0,0,2021-05-05T20:36:52Z,@@stratascratch how industry people manage large data stored in database while exploring data? what are the best practices for data scientist or analyst in these cases?,2021-05-05T20:36:52Z
Ugw_J4aiNNcB74HupO94AaABAg.9Myb_22Rci69MyqlnrXU2j,@stratascratch,Ugw_J4aiNNcB74HupO94AaABAg,2,77IVf0zgmwI,0,1,2021-05-05T20:52:05Z,"@@its_me7363 The dbs are optimized to handle large amounts of data with speed. You definitely won&#39;t be using any postgres or mysql dbs. You&#39;ll be using HIVE or Greenplum or Snowflake. Also, depending on what you are trying to do, you will likely move away from using pandas and onto py spark to do some work. All of this is taught when you join the company. Most people know that not all entry level ds have used these tools.",2021-05-05T20:52:05Z
Ugw_J4aiNNcB74HupO94AaABAg.9Myb_22Rci69MysT0kmyU6,@its_me7363,Ugw_J4aiNNcB74HupO94AaABAg,2,77IVf0zgmwI,0,0,2021-05-05T21:06:51Z,@@stratascratch does these dbs are similar to postgresql? and what do you mean by moving away from pandas...does industry not use pandas in their projects?,2021-05-05T21:06:51Z
Ugw_J4aiNNcB74HupO94AaABAg.9Myb_22Rci69Myx-LyQDl7,@stratascratch,Ugw_J4aiNNcB74HupO94AaABAg,2,77IVf0zgmwI,0,0,2021-05-05T21:46:30Z,"@@its_me7363 yes the dbs are similar to postgres. There&#39;s just minor differences in syntax but it&#39;s easy to learn. The industry uses pandas but sometimes when dealing with large amounts of data, you want to work with something that can handle large amounts of data and that&#39;s pyspark.",2021-05-05T21:46:30Z
UgwsbTLyaRUiWYCKtiN4AaABAg,@patrickchan2503,,1,fklHBWow8vE,0,0,2023-11-14T03:25:39Z,"AI autofilled for me when I save data to df:<br>        `df.loc[len(df)] = [video_id, video_title, upload_date, view_count, like_count, favorite_count, comment_count]`",2023-11-14T03:25:39Z
Ugy7jLd6Qr49N_e1V4t4AaABAg,@69nukeee,,1,fklHBWow8vE,0,0,2023-10-23T13:35:38Z,"AMAZING tutorial, thanks for sharing!",2023-10-23T13:35:38Z
UgwelmOLDfrD_X-BPQB4AaABAg,@estevearru-gallart4010,,1,fklHBWow8vE,0,1,2023-10-06T00:06:37Z,AttributeError: &#39;DataFrame&#39; object has no attribute &#39;append&#39; How to deal with that now ?,2023-10-06T00:06:37Z
UgzDNRVcoPumWnahBrN4AaABAg,@samchimaobi3398,,1,fklHBWow8vE,0,0,2023-09-20T09:14:28Z,Thanks for this video. was really very helpful.,2023-09-20T09:14:28Z
UgxQ8dveg5zOfCRweM54AaABAg,@angelmaravilla4708,,1,fklHBWow8vE,0,0,2023-09-17T04:45:45Z,"Man this is amazing!!, just want to ask you about an error, hopping you can advice:<br>In the second call (<a href=""https://www.youtube.com/watch?v=fklHBWow8vE&amp;t=19m40s"">19:40</a>) my code is exactly the same as yours, but I get the Stsatus 400 telling me that my API is not valid.<br>(I created a second API key with another gmail account but the error remains)<br><br>Do you know how can I fix that error?<br><br>{<br>    &quot;error&quot;: {<br>        &quot;code&quot;: 400,<br>        &quot;message&quot;: &quot;API key not valid. Please pass a valid API key.&quot;,<br>        &quot;errors&quot;: [<br>            {<br>                &quot;message&quot;: &quot;API key not valid. Please pass a valid API key.&quot;,<br>                &quot;domain&quot;: &quot;global&quot;,<br>                &quot;reason&quot;: &quot;badRequest&quot;<br>            }<br>        ],<br>        &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,<br>        &quot;details&quot;: [<br>            {<br>                &quot;@type&quot;: &quot;<a href=""http://type.googleapis.com/google.rpc.ErrorInfo%22,"">type.googleapis.com/google.rpc.ErrorInfo&quot;,</a><br>                &quot;reason&quot;: &quot;API_KEY_INVALID&quot;,<br>                &quot;domain&quot;: &quot;<a href=""http://googleapis.com/"">googleapis.com</a>&quot;,<br>                &quot;metadata&quot;: {<br>                    &quot;service&quot;: &quot;<a href=""http://youtube.googleapis.com/"">youtube.googleapis.com</a>&quot;<br>                }<br>            }<br>        ]<br>    }<br>}",2023-09-17T04:53:11Z
UgzDq2zSd55M6C5ZIxN4AaABAg,@xyzstain,,1,fklHBWow8vE,1,0,2023-09-12T12:56:36Z,When i ran the code i only get the first 50 lines of the YT channel&#39;s stats and stuff. Do you know why that could be? I tried it with different channels and it was the same,2023-09-12T12:56:36Z
UgzDq2zSd55M6C5ZIxN4AaABAg.9uZQzhwh4UM9uZyBzAcL0F,@stratascratch,UgzDq2zSd55M6C5ZIxN4AaABAg,2,fklHBWow8vE,0,1,2023-09-12T17:55:30Z,I think it&#39;s because you&#39;re only grabbing the 1st page. You&#39;ll need to go to the other pages to grab the data.,2023-09-12T17:55:30Z
UgxOfqxI6tglBh06vYx4AaABAg,@stainxyz3373,,1,fklHBWow8vE,1,0,2023-09-09T16:21:36Z,thanks. this thing took me an entire day to finish.,2023-09-09T16:21:36Z
UgxOfqxI6tglBh06vYx4AaABAg.9uS43_o7PjT9uVog46xTvX,@stratascratch,UgxOfqxI6tglBh06vYx4AaABAg,2,fklHBWow8vE,0,0,2023-09-11T03:15:24Z,I&#39;m glad you did the project! It&#39;s a good one day project,2023-09-11T03:15:24Z
UgwtwGf9vIyeLcLM_O54AaABAg,@DB-nl9xw,,1,fklHBWow8vE,1,0,2023-08-25T00:59:19Z,how can I save to a Google Sheet?,2023-08-25T00:59:19Z
UgwtwGf9vIyeLcLM_O54AaABAg.9tonaTBBvoA9tqU3s_8eFj,@stratascratch,UgwtwGf9vIyeLcLM_O54AaABAg,2,fklHBWow8vE,0,0,2023-08-25T16:38:27Z,Probably the easiest way is to save it as a csv and then import it into your google drive. Otherwise you&#39;ll need to use the Google Sheet API,2023-08-25T16:38:27Z
UgzbOMH9C-QjrcYfLxN4AaABAg,@inigocuervo,,1,fklHBWow8vE,1,3,2023-08-20T11:36:25Z,"Hey there! Just a quick tip for anyone trying out the code: make sure you&#39;re using a version of pandas earlier than 2.0. Otherwise, you might need to use a different method instead of .append, or consider adding the data to a Python list first and then converting it to a pandas dataframe. Hope this helps! üòä",2023-08-20T11:36:25Z
UgzbOMH9C-QjrcYfLxN4AaABAg.9td3XOM10iH9uMmkJb1lfG,@lme918,UgzbOMH9C-QjrcYfLxN4AaABAg,2,fklHBWow8vE,0,1,2023-09-07T15:05:20Z,"Even if you will use append method, it&#39;s strongly adviced do not use append as in his video as it&#39;s awful for performance. Don&#39;t do it!",2023-09-07T15:05:20Z
Ugyefm38-A5udLPruG94AaABAg,@aymenchaqrouf7247,,1,fklHBWow8vE,3,0,2023-07-23T18:22:18Z,what about the pageToken you did mention you will get back to it after but you forget. anyways thank you fo this video,2023-07-23T18:22:18Z
Ugyefm38-A5udLPruG94AaABAg.9sWgigh3ts49sWk975u9GN,@stratascratch,Ugyefm38-A5udLPruG94AaABAg,2,fklHBWow8vE,0,0,2023-07-23T18:52:16Z,yeah totally forgot and ran out of time. You can add the pageToken to the API request. There&#39;s YT documentation on how to do so,2023-07-23T18:52:16Z
Ugyefm38-A5udLPruG94AaABAg.9sWgigh3ts49sWnIOAxqVx,@aymenchaqrouf7247,Ugyefm38-A5udLPruG94AaABAg,2,fklHBWow8vE,0,0,2023-07-23T19:19:44Z,"@@stratascratch I tried to give the variable pageToken the value : &quot;nextPageToken&quot; but It didn&#39;t work unfortunately, any suggestions please?",2023-07-23T19:19:44Z
Ugyefm38-A5udLPruG94AaABAg.9sWgigh3ts49sWpBukfK_z,@stratascratch,Ugyefm38-A5udLPruG94AaABAg,2,fklHBWow8vE,0,0,2023-07-23T19:36:20Z,I don&#39;t have the YT api memorized. My advice is to read the documentation.,2023-07-23T19:36:20Z
UgxpByxLngR9B_sc_fd4AaABAg,@YashSharma-zg1uk,,1,fklHBWow8vE,1,0,2023-07-23T12:46:51Z,thank you so much dude,2023-07-23T12:46:51Z
UgxpByxLngR9B_sc_fd4AaABAg.9sW5KmGUlJW9s_rQD21-g9,@stratascratch,UgxpByxLngR9B_sc_fd4AaABAg,2,fklHBWow8vE,0,0,2023-07-25T09:12:43Z,You&#39;re welcome!,2023-07-25T09:12:43Z
UgyFIpeV2EwmBBQEyQR4AaABAg,@urfavchickenlegs8797,,1,fklHBWow8vE,1,0,2023-07-18T14:34:19Z,you dont tell us about the &quot;page token&quot; as promised at the early part of the video :(,2023-07-18T14:34:35Z
UgyFIpeV2EwmBBQEyQR4AaABAg.9sJPeS3CoYz9sJVR-zXzpL,@stratascratch,UgyFIpeV2EwmBBQEyQR4AaABAg,2,fklHBWow8vE,0,0,2023-07-18T15:24:46Z,I know! I&#39;m sorry =(,2023-07-18T15:24:46Z
UgwaF3Hlq36fqrUkQLR4AaABAg,@rizalrifai6010,,1,fklHBWow8vE,1,0,2023-07-03T10:53:38Z,is there any api that we can try for free?,2023-07-03T10:53:38Z
UgwaF3Hlq36fqrUkQLR4AaABAg.9rhOTx5Y3-_9riV8O8BJXq,@stratascratch,UgwaF3Hlq36fqrUkQLR4AaABAg,2,fklHBWow8vE,0,0,2023-07-03T21:11:05Z,There&#39;s a few but you can try Twitter and Youtube APIs!,2023-07-03T21:11:05Z
UgwzA_CDJIUW8Oi18sF4AaABAg,@senyotsedze3388,,1,fklHBWow8vE,0,0,2023-06-02T01:59:37Z,"Interesting video. I was able to create my own api key from YouTube. But since I have no posted videos, it is difficult to conceptualize your teaching. I wish we could get a generic api key to see data populated in the dataframe.",2023-06-02T01:59:37Z
UgztfBRP6CdeThPljBt4AaABAg,@cristiansoto7581,,1,fklHBWow8vE,0,0,2023-05-31T17:42:49Z,"More videos about this, please. Great information.  I loved it!",2023-05-31T17:42:49Z
UgzWiSr2DUqAnUvJhXJ4AaABAg,@rashadnelson1873,,1,fklHBWow8vE,1,0,2023-05-24T20:27:05Z,"This is great, thanks for providing.  Sometimes, the apikey is in the header, so it has to be passed that way.  I used this to pull the data in from the API I&#39;m presently working with (ATTOM):<br><br>url = &#39;the get url&#39;<br><br>headers = {<br>         &#39;Accept&#39;:&#39;application/json&#39;,<br>         &#39;apikey&#39;:&#39;your api key&#39;,<br>         &#39;accept&#39;:&#39;application/json&#39;<br>}<br><br>response = requests.get(url, headers=headers)<br><br>response.json()",2023-05-24T20:27:05Z
UgzWiSr2DUqAnUvJhXJ4AaABAg.9q6QJ4DlKZT9q8fY921CAc,@stratascratch,UgzWiSr2DUqAnUvJhXJ4AaABAg,2,fklHBWow8vE,0,1,2023-05-25T17:27:26Z,Very cool! Thanks for sharing.,2023-05-25T17:27:26Z
UgxWerSK39tt5nnc0Fp4AaABAg,@darkflamedoctor9023,,1,fklHBWow8vE,1,0,2023-05-23T22:37:19Z,"guys i have doubt, now that we have collected the data how are we going to apply ml to this specific task(yt api) My Guess is we need little more info to predict something with good accuracy, these are the things I thought for this data: I think we can cluster the video with less views or likes or something like that, one more thing I can think of is predicting views or likes with the help of regression models, I cant think of anything else can u tell me something else",2023-05-23T22:56:54Z
UgxWerSK39tt5nnc0Fp4AaABAg.9q44Q3qVx8Z9q8fvT8u64j,@stratascratch,UgxWerSK39tt5nnc0Fp4AaABAg,2,fklHBWow8vE,0,0,2023-05-25T17:30:45Z,Great suggestion and I agree with you. There won&#39;t be enough data from my channel alone to do any ML. You&#39;ll need to collect much more data.,2023-05-25T17:30:45Z
Ugyj97rOCZwqLDfmabt4AaABAg,@sidechain0,,1,fklHBWow8vE,0,0,2023-05-15T20:14:15Z,"Great tutorial, thanks!",2023-05-15T20:14:15Z
Ugzz5rBIXN9GugU_wW14AaABAg,@ChatGPT-ef6sr,,1,fklHBWow8vE,0,0,2023-05-11T16:38:33Z,This is data engineering not data science!,2023-05-11T16:38:33Z
Ugzb-IRYZSTSrgdm2Ph4AaABAg,@soujanyamukkala1052,,1,fklHBWow8vE,0,0,2023-05-10T17:48:31Z,"Hello Nate, Nice Video, love it! I have a question here, instead of making a second API call for the stats, can we use fields parameter in the URL of the first API call to get the stats.  Pardon me if I am not understanding the fundamentals properly. Thanks",2023-05-10T17:48:31Z
UgyIdGjFnLP7XOhHyAd4AaABAg,@MohamedAhmed-cw7tg,,1,fklHBWow8vE,0,0,2023-05-04T01:25:34Z,where&#39;s the video i could store API data into DATA BASE ?,2023-05-04T01:25:34Z
Ugz3UTyIj6d_A2F7VLt4AaABAg,@pauljordan6346,,1,fklHBWow8vE,0,0,2023-04-27T05:41:14Z,"Hi Nate, I love the video, super clear and explains a lot. I am struggling with pagetoken, you said it would be explained later, is there another video I need to watch to get a handle on this? Thanks",2023-04-27T05:41:14Z
UgxBEp2VxfcaxqZq7J94AaABAg,@bekturbekboev6839,,1,fklHBWow8vE,0,0,2023-04-23T09:32:22Z,Update pls with new API ü§©,2023-04-23T09:32:22Z
UgynPsoVuJNumnHrZx94AaABAg,@1cutecouple,,1,fklHBWow8vE,2,0,2023-04-09T19:56:23Z,THIS IS ABSOLUTE BLISS TO WATCH!!! THANKS FOR THE AWESOME CONTENT &lt;3,2023-04-09T19:56:23Z
UgynPsoVuJNumnHrZx94AaABAg.9oIV0pEQxx69pP-yYJEUfj,@stratascratch,UgynPsoVuJNumnHrZx94AaABAg,2,fklHBWow8vE,0,0,2023-05-07T05:11:07Z,Glad you enjoy it!,2023-05-07T05:11:07Z
UgynPsoVuJNumnHrZx94AaABAg.9oIV0pEQxx69q44t4wQJjk,@darkflamedoctor9023,UgynPsoVuJNumnHrZx94AaABAg,2,fklHBWow8vE,0,0,2023-05-23T22:41:24Z,"@@stratascratch now that we have collected the data how are we going to apply ml to this specific task(yt api) My Guess is we need little more info to predict something with good accuracy<br>these are the things I thought for this data: I think we can cluster the video with less views or likes or something like that, one more thing I can think of is predicting views or likes with the help of regression models I cant think of anything else can u please tell me something else",2023-05-23T22:55:42Z
Ugy5zGj7e-45sCnrBTt4AaABAg,@joelngige5776,,1,fklHBWow8vE,0,0,2023-03-28T19:19:49Z,Super informative video on working with API&#39;s,2023-03-28T19:19:49Z
UgzDpM0cYsaG8V7ncml4AaABAg,@DatesAndMilk,,1,fklHBWow8vE,0,0,2023-03-17T01:20:47Z,"<a href=""https://www.youtube.com/watch?v=fklHBWow8vE&amp;t=25m38s"">25:38</a> I can  also attest to this with my experience lol",2023-03-17T01:20:47Z
UgxbO0KuYKxPRFwEWux4AaABAg,@mercantilism954,,1,fklHBWow8vE,0,0,2023-02-17T05:54:12Z,Oh my god. Your video is a treasure,2023-02-17T05:54:12Z
Ugxq2n4CaOT2x3nu_l14AaABAg,@sheafsmash142,,1,fklHBWow8vE,0,0,2023-02-15T16:55:06Z,10 year old code is underestimated,2023-02-15T16:55:06Z
UgyTbYvIBIorIrsU-mh4AaABAg,@stefano_er,,1,fklHBWow8vE,0,0,2023-01-23T08:06:43Z,Thanks! Really appreciated it,2023-01-23T08:06:43Z
UgzR-Ni0AIS2AjaA7Vl4AaABAg,@shashibhushansingh1628,,1,fklHBWow8vE,0,0,2023-01-22T15:17:53Z,Greatüëç,2023-01-22T15:17:53Z
UgzG7LCK2wSwI0OPre14AaABAg,@czr372,,1,fklHBWow8vE,0,0,2023-01-11T07:13:51Z,"I spend couple hours figuring out the url&#39;s and  the api key, but this is a great help, Thanx a lot!",2023-01-11T07:13:51Z
UgyMpp2oELmvLPrRODR4AaABAg,@danielmoss7133,,1,fklHBWow8vE,1,0,2023-01-08T14:17:07Z,Best video I have seen on APIs. Thank you so much.,2023-01-08T14:17:07Z
UgyMpp2oELmvLPrRODR4AaABAg.9kcZrR5-_209kd0xy_JCgW,@stratascratch,UgyMpp2oELmvLPrRODR4AaABAg,2,fklHBWow8vE,0,0,2023-01-08T18:31:25Z,"Thank you! If there are other projects you&#39;re interested in, let me know!",2023-01-08T18:31:25Z
Ugyh0Gs9JeFGDM7uDQB4AaABAg,@rohitchabukswar7266,,1,fklHBWow8vE,1,0,2022-12-28T05:56:22Z,I am getting a syntax error when I execute the url,2022-12-28T05:56:22Z
Ugyh0Gs9JeFGDM7uDQB4AaABAg.9kALo77azrk9uRCrGofxij,@stainxyz3373,Ugyh0Gs9JeFGDM7uDQB4AaABAg,2,fklHBWow8vE,0,0,2023-09-09T08:19:10Z,did you manage to get it to work?,2023-09-09T08:19:10Z
UgwkBv2gF5nYJfALMmZ4AaABAg,@AG-nn2pi,,1,fklHBWow8vE,1,1,2022-12-13T19:54:55Z,"When I do the for loop, it only prints a single row, but when a run just a response command in another cell it gives me all of the rows. Any ideas?<br><br><a href=""http://www.youtube.com/results?search_query=%23sell"">#Sell</a> Order<br>for order in response:<br>  <a href=""http://www.youtube.com/results?search_query=%23if"">#if</a> order[&#39;is_buy_order&#39;] == False:<br>    order_id = order[&#39;order_id&#39;]<br>    price = order[&#39;price&#39;]<br>    order_type = order[&#39;is_buy_order&#39;]<br><br>print(order_id)<br>print(price)<br>print(order_type)<br><br>=&gt;<br><br>6368669555<br>191400.0<br>True",2022-12-13T19:54:55Z
UgwkBv2gF5nYJfALMmZ4AaABAg.9jaDqjFDJ2F9jaGmCkvIcQ,@AG-nn2pi,UgwkBv2gF5nYJfALMmZ4AaABAg,2,fklHBWow8vE,0,1,2022-12-13T20:20:31Z,"when I make the df then it does actually seem to pull all the rows, seems weird still",2022-12-13T20:20:31Z
UgxVlfbYkzjeVzP2BJl4AaABAg,@ujjwaltarway1438,,1,fklHBWow8vE,0,0,2022-12-11T14:34:28Z,"One of the best videos regarding API call. Subscribed.<a href=""about:invalid#zCSafez""></a><a href=""about:invalid#zCSafez""></a><a href=""about:invalid#zCSafez""></a><a href=""about:invalid#zCSafez""></a>üòáüòä",2022-12-11T14:34:28Z
Ugw9tdIAj3Jlk0FLV1h4AaABAg,@wackyprofessor,,1,fklHBWow8vE,1,1,2022-12-08T18:56:33Z,"I thought my video speed was on max, but it wasn&#39;t;  there is just a lot of information being presented.   Very comprehensive.  Thank you.",2022-12-08T19:01:22Z
Ugw9tdIAj3Jlk0FLV1h4AaABAg.9jOFBuxkrct9jOMwKdtIyw,@stratascratch,Ugw9tdIAj3Jlk0FLV1h4AaABAg,2,fklHBWow8vE,0,0,2022-12-08T20:04:11Z,Thank you.  We love to share what we know.  Glad we have helped you.üòÄ,2022-12-08T20:04:11Z
UgxTyN3WdcDRPc05tZ14AaABAg,@luzestrada9536,,1,fklHBWow8vE,1,1,2022-11-27T11:47:51Z,You are the best! Thank you so much for sharing this valuable information &lt;3 It&#39;s pure gold,2022-11-27T11:47:51Z
UgxTyN3WdcDRPc05tZ14AaABAg.9iw9OLEoP069iwZtnW1kQh,@stratascratch,UgxTyN3WdcDRPc05tZ14AaABAg,2,fklHBWow8vE,0,0,2022-11-27T15:39:28Z,Thank you.  We are happy that we were  able to help.,2022-11-27T15:39:28Z
Ugw_2seGBHNbSkN8DVt4AaABAg,@mariumbegum7325,,1,fklHBWow8vE,0,0,2022-11-04T11:13:42Z,"Fantastic tutorial, easy to follow and presented in a way that is easy to follow",2022-11-04T11:13:42Z
UgxINmAdmaLm3wd5DWd4AaABAg,@myselfandpesit,,1,fklHBWow8vE,1,0,2022-11-02T06:28:03Z,"Thanks for the video. Nevertheless, some python libraries such as Tensorflow talk about APIs when they are actually saved already on our hard drive. I want to ask you how does, a web application such as tensorflow API function here ?",2022-11-02T06:28:03Z
UgxINmAdmaLm3wd5DWd4AaABAg.9hvCvDlbVRJ9hxlf1OUv8X,@stratascratch,UgxINmAdmaLm3wd5DWd4AaABAg,2,fklHBWow8vE,0,1,2022-11-03T06:18:54Z,"Hi. If I&#39;m understanding correctly, these APIs that are local are basically just ways of transmitting your data locally. You&#39;re just using an API locally to build your web app. You can move the API to another server or to the cloud and your web app would still work.",2022-11-03T06:18:54Z
UgwYRUFjgXqaKGxrqFl4AaABAg,@joaofernandes6349,,1,fklHBWow8vE,0,3,2022-10-26T10:37:24Z,"The best part is how you show in a easy way how to do the whole process and then you clean up the code according to good practices. This was really good, thank you!",2022-10-26T10:37:24Z
UgzEpcUG71l1LHLl_0N4AaABAg,@tianag4627,,1,fklHBWow8vE,1,1,2022-10-21T22:14:49Z,I can&#39;t follow along because don&#39;t have a youtube channel.,2022-10-21T22:14:49Z
UgzEpcUG71l1LHLl_0N4AaABAg.9hT-j0mgJxj9hUUPhEGFdk,@stratascratch,UgzEpcUG71l1LHLl_0N4AaABAg,2,fklHBWow8vE,0,0,2022-10-22T12:02:10Z,You may save and visit our channel whenever you need.,2022-10-22T12:02:10Z
Ugw6PLdRdr0Q0-84D1N4AaABAg,@alexwoodard2759,,1,fklHBWow8vE,0,0,2022-10-04T19:45:46Z,"good stuff, sir!",2022-10-04T19:45:46Z
UgyRAK_nVPZ62YLrilZ4AaABAg,@rishabkhuba2663,,1,fklHBWow8vE,3,1,2022-09-14T16:57:18Z,"How do I follow along this tutorial, if I don&#39;t have a youtube channel?",2022-09-14T16:57:18Z
UgyRAK_nVPZ62YLrilZ4AaABAg.9fy9z5jh3hp9fzOIxjK9k4,@stratascratch,UgyRAK_nVPZ62YLrilZ4AaABAg,2,fklHBWow8vE,0,0,2022-09-15T04:21:43Z,"You will have to sign up for YT to be able to follow us.  You may also sign up for our newsletter to get updates on our blogs ,<a href=""https://www.stratascratch.com/"">https://www.stratascratch.com/</a>",2022-09-15T04:21:43Z
UgyRAK_nVPZ62YLrilZ4AaABAg.9fy9z5jh3hp9fzQXHG9twK,@rishabkhuba2663,UgyRAK_nVPZ62YLrilZ4AaABAg,2,fklHBWow8vE,0,1,2022-09-15T04:41:09Z,"@@stratascratch I meant that, I don&#39;t<br>Have YouTube videos to get video likes, comments etc to follow along with your tutorial. I think I will try to follow along by trying to access API&#39;s of other apps <br>Thanks for the reply though ‚ò∫Ô∏è",2022-09-15T04:41:09Z
UgyRAK_nVPZ62YLrilZ4AaABAg.9fy9z5jh3hp9imxltwbN1G,@sprinter5901,UgyRAK_nVPZ62YLrilZ4AaABAg,2,fklHBWow8vE,0,0,2022-11-23T22:04:26Z,@@rishabkhuba2663 You can use others channel ID. YouTube how to get channel ID.,2022-11-23T22:04:26Z
UgwCxp0mD0iXgicH3FN4AaABAg,@hoanglam2814,,1,fklHBWow8vE,1,1,2022-09-06T07:53:00Z,love that,2022-09-06T07:53:00Z
UgwCxp0mD0iXgicH3FN4AaABAg.9fcaKUS_C0o9fdS3mY-RUU,@stratascratch,UgwCxp0mD0iXgicH3FN4AaABAg,2,fklHBWow8vE,0,0,2022-09-06T15:51:18Z,Thank you!,2022-09-06T15:51:18Z
Ugwo0rXP90e5y_FwLqh4AaABAg,@rohitprajapati2303,,1,fklHBWow8vE,1,1,2022-09-04T06:44:21Z,"Amazing video ,finally get the understanding how to extract data from the API ,Thanks man",2022-09-04T06:44:21Z
Ugwo0rXP90e5y_FwLqh4AaABAg.9fYJt0-c-oS9fdRlpwsGus,@stratascratch,Ugwo0rXP90e5y_FwLqh4AaABAg,2,fklHBWow8vE,0,0,2022-09-06T15:48:43Z,I appreciate your feedback!  Thank you too.,2022-09-06T15:48:43Z
UgyIpOsRtHmqpRR1gah4AaABAg,@useydkerimoglu5720,,1,fklHBWow8vE,1,0,2022-08-27T20:03:15Z,LOVE IT,2022-08-27T20:03:15Z
UgyIpOsRtHmqpRR1gah4AaABAg.9fF8xFxjhRs9fFEPEqwgXV,@stratascratch,UgyIpOsRtHmqpRR1gah4AaABAg,2,fklHBWow8vE,0,0,2022-08-27T20:50:53Z,Thank you!  Glad you enjoyed it.,2022-08-27T20:50:53Z
Ugz6VFOmqhlE-RNMOwB4AaABAg,@DylanOkyere,,1,fklHBWow8vE,0,1,2022-08-26T22:52:43Z,You never linked to the api key document,2022-08-26T22:52:43Z
Ugys7tenM0LvZ4-YPQF4AaABAg,@timeistreasure6061,,1,fklHBWow8vE,1,1,2022-08-20T16:19:41Z,Just Awesome,2022-08-20T16:19:41Z
Ugys7tenM0LvZ4-YPQF4AaABAg.9exinjvdF4O9eyGs-ETNeT,@stratascratch,Ugys7tenM0LvZ4-YPQF4AaABAg,2,fklHBWow8vE,0,0,2022-08-20T21:26:06Z,Wonderful.  Hope you find the video helpful.,2022-08-20T21:26:06Z
UgwcRJ-N5qeROR0BiYt4AaABAg,@chiomaeze8245,,1,fklHBWow8vE,1,1,2022-08-14T13:16:22Z,"wow, this is amazing! Thank you so much. It was concise and robust. Learnt a lot from it",2022-08-14T13:16:22Z
UgwcRJ-N5qeROR0BiYt4AaABAg.9ehx2oJoxBa9ei_o-JcvcH,@stratascratch,UgwcRJ-N5qeROR0BiYt4AaABAg,2,fklHBWow8vE,0,0,2022-08-14T19:12:28Z,You are welcome.  Happy that you found our video helpful.,2022-08-14T19:12:28Z
UgwiaftGg-aNVD_GkGl4AaABAg,@shuang7877,,1,fklHBWow8vE,1,0,2022-08-08T08:34:52Z,That diploma looks very familiar - hello Big Red!,2022-08-08T08:34:52Z
UgwiaftGg-aNVD_GkGl4AaABAg.9eT-3mkvDGI9eWXzS3KFAr,@stratascratch,UgwiaftGg-aNVD_GkGl4AaABAg,2,fklHBWow8vE,0,0,2022-08-09T17:37:41Z,Hello Big Red indeed!,2022-08-09T17:37:41Z
Ugz5H3YPFgRZ3MllMMd4AaABAg,@user-ur2en1zq4f,,1,fklHBWow8vE,0,0,2022-08-07T05:05:13Z,tHanKs Dude,2022-08-07T05:05:13Z
UgyagnkDxlNr1JxI1XF4AaABAg,@sarwatzabeen8523,,1,fklHBWow8vE,1,0,2022-07-25T03:21:57Z,"This is very helpful! Just a quick question, since the second set of variables were defined for the first video only with index 0 and no for loop, I am getting a dataframe with only one row. Am I missing something?",2022-07-25T03:21:57Z
UgyagnkDxlNr1JxI1XF4AaABAg.9dtO7eR_5YO9ehwXwTD0Y_,@chiomaeze8245,UgyagnkDxlNr1JxI1XF4AaABAg,2,fklHBWow8vE,0,1,2022-08-14T13:11:52Z,cause you used 0 index so you will get only one row. Your loop should be able iterate through the each index of the Json[&quot;item&quot;] object to grab the features you need.,2022-08-14T13:11:52Z
Ugw87UHlAcZswBt4eNZ4AaABAg,@ChrisMaringka,,1,fklHBWow8vE,0,0,2022-07-24T22:12:51Z,"Hello Nate, thank you for sharing this tutorial I appreciate it! I just had one quick question about the second API calling: when I tried to call the statistics, every now and then one of the stats (viewCount, likeCount, dislikeCount, or commentCount) might be missing and I could not assign them into a variable. Do you by any chance know the work around for it? Thanks!",2022-07-24T22:12:51Z
Ugwro7l1W15eTWoJAJx4AaABAg,@sajjad5345,,1,fklHBWow8vE,1,0,2022-07-13T18:35:54Z,I love this playlist and look forward to seeing more videos in it.üëç,2022-07-13T18:35:54Z
Ugwro7l1W15eTWoJAJx4AaABAg.9dR7B8f7sjd9dRDWHwkEfI,@stratascratch,Ugwro7l1W15eTWoJAJx4AaABAg,2,fklHBWow8vE,0,1,2022-07-13T19:31:13Z,Awesome! Thank you!,2022-07-13T19:31:13Z
Ugx1vTQiARSnJzvfKtN4AaABAg,@WIllyumisbeast41,,1,fklHBWow8vE,1,0,2022-07-09T22:34:34Z,The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.,2022-07-09T22:34:34Z
Ugx1vTQiARSnJzvfKtN4AaABAg.9dHFJjG19u99dW_6G4bD5I,@stratascratch,Ugx1vTQiARSnJzvfKtN4AaABAg,2,fklHBWow8vE,0,0,2022-07-15T21:24:50Z,Thank you very much for the reminder!,2022-07-15T21:24:50Z
UgyGWuC1lElLuCndZ-R4AaABAg,@matthewsmalatji6682,,1,fklHBWow8vE,1,1,2022-07-07T11:58:55Z,Please link the video you<br> are referring to in the Intro,2022-07-07T11:58:55Z
UgyGWuC1lElLuCndZ-R4AaABAg.9dAxzI1uXVb9g-VE-tRo4z,@stratascratch,UgyGWuC1lElLuCndZ-R4AaABAg,2,fklHBWow8vE,0,0,2022-09-15T14:41:27Z,"Here is the link to the video: <a href=""https://www.youtube.com/watch?v=c4Af2FcgamA"">https://www.youtube.com/watch?v=c4Af2FcgamA</a>",2022-09-15T14:41:27Z
UgzO8JjgWlIYf_yOI5N4AaABAg,@eulerthegreatestofall147,,1,fklHBWow8vE,0,0,2022-07-06T04:08:45Z,Great video!!!! very easy to follow through!,2022-07-06T04:08:45Z
UgzpuqkHWuXvGVMUPid4AaABAg,@andrew6233,,1,fklHBWow8vE,2,0,2022-06-30T00:47:33Z,"I really appreciate this video! One really weird thing that is happening for me though is that I have copied your code exactly (originally had my own project based on this, but to troubleshoot I copied exactly) and I&#39;m getting seemingly random videos from channels other than yours. Do you know why this would be? I also tried with my own channel ID and had the same issue.<br><br>EDIT: Ok after looking and looking, I finally noticed I&#39;d put &amp;channelID instead of &amp;channelId, oops",2022-06-30T05:16:17Z
UgzpuqkHWuXvGVMUPid4AaABAg.9csj_uSWy7s9cvOCBIIqb-,@stratascratch,UgzpuqkHWuXvGVMUPid4AaABAg,2,fklHBWow8vE,0,0,2022-07-01T01:29:39Z,That&#39;s really weird. I actually have no idea why that would happen. The channel ID should restrict what videos you pull. Maybe print out the channel id when you grab the videos? See if it&#39;s the right value.,2022-07-01T01:29:39Z
UgzpuqkHWuXvGVMUPid4AaABAg.9csj_uSWy7s9cvZ8PXiUY3,@andrew6233,UgzpuqkHWuXvGVMUPid4AaABAg,2,fklHBWow8vE,0,0,2022-07-01T03:05:15Z,"@@stratascratch my problem was that I put channelID instead of channelId in the api call, I expected that to throw an error but I‚Äôm guessing instead it just pulled from all videos. Oops! Thanks for the reply!",2022-07-01T03:05:47Z
UgxCcQJmI8KVyDQhglh4AaABAg,@jiangchengli2191,,1,fklHBWow8vE,1,0,2022-06-21T21:44:21Z,"good work man,  best tutorial of collecting data from api !!!!",2022-06-21T21:44:21Z
UgxCcQJmI8KVyDQhglh4AaABAg.9cYoG9194Hq9dM_v99ysem,@stratascratch,UgxCcQJmI8KVyDQhglh4AaABAg,2,fklHBWow8vE,0,0,2022-07-12T00:19:31Z,Glad you think so!,2022-07-12T00:19:31Z
UgzquHIE1Tci5h1IIVB4AaABAg,@badsanta01,,1,fklHBWow8vE,0,4,2022-06-20T05:17:14Z,"Thanks for the video Nate.<br>Update : One might get an error in the dislike count part.  &quot;The statistics.dislikeCount property was made private as of December 13, 2021. This means that the property is included in an API response only if the API request was authenticated by the video owner. See the revision history for more information.&quot;",2022-06-20T05:17:14Z
UgxR8BLVxxs7PX5H8qF4AaABAg,@lifecodes9155,,1,fklHBWow8vE,0,0,2022-06-18T06:16:04Z,Too good. Thanks a lot.<br>but i have problem that captcha is getting in the way of me scraping the website.,2022-06-18T06:16:04Z
Ugxro9vMlOQ654GnYER4AaABAg,@sophiez7952,,1,fklHBWow8vE,0,0,2022-06-14T08:40:22Z,Great,2022-06-14T08:40:22Z
UgzlQvpOXRNXiDTJKzF4AaABAg,@ivlivs.c3666,,1,fklHBWow8vE,0,0,2022-06-11T09:28:22Z,"Now this is real-world, practical knowledge. After watching your video on the one and only data science project you&#39;ll ever need, I knew I had found something amazing. After watching this one, I was sold. Just subscribed. Keep these gems coming!",2022-06-11T09:28:22Z
Ugz6UMMjvygNU3ZA0o14AaABAg,@Sreenu1523,,1,N0aHeKyNEto,0,0,2023-08-25T15:46:49Z,"This is one of the best tutorial ever seen. I have been searching this kind of tutorial. Thanks.<br>How to send csvfile, table as input parameter instead of read all files from folder. Please share link or video which can help",2023-08-25T15:46:49Z
UgzHKHRmnDdODx734jF4AaABAg,@esamelhosiny5615,,1,N0aHeKyNEto,2,0,2023-02-05T01:03:53Z,"Thank you so much! Nate, for all your help ‚ù§<br>could you please make projects from scratch about APIs &amp; pipelines - in sales or any major you want <br>I see your video about one and only and any advice for learning because I don&#39;t know what should i learn first in these fields.",2023-02-05T01:03:53Z
UgzHKHRmnDdODx734jF4AaABAg.9ljFL5ssSVp9ljLMO4GZuK,@stratascratch,UgzHKHRmnDdODx734jF4AaABAg,2,N0aHeKyNEto,0,0,2023-02-05T01:56:29Z,Absolutely! We are creating many data science project videos coming out this year so I&#39;m sure you&#39;ll find one interesting =),2023-02-05T01:56:29Z
UgzHKHRmnDdODx734jF4AaABAg.9ljFL5ssSVp9llmwQ3re82,@esamelhosiny5615,UgzHKHRmnDdODx734jF4AaABAg,2,N0aHeKyNEto,0,0,2023-02-06T00:44:42Z,"@@stratascratch Insha&#39;Allah, Thank you, bro üòç‚ù§",2023-02-06T00:44:42Z
Ugx4HYZML4APGtsDAfB4AaABAg,@karunakaranr2473,,1,N0aHeKyNEto,0,0,2023-02-01T03:04:52Z,Thank you for your time and effort to make this tutorial. Really helps.,2023-02-01T03:04:52Z
UgxevxjVGabT2uKtXB54AaABAg,@Gautam-lo5zy,,1,N0aHeKyNEto,0,0,2023-01-21T23:31:28Z,I really like these types of projects. helpful to get an understanding of how real-world projects works.,2023-01-21T23:31:28Z
Ugxx0-FuoO9yBluYdE14AaABAg,@subinivi,,1,N0aHeKyNEto,1,0,2022-09-28T07:46:47Z,The Best 3 videos of data migration I have ever seen before. Very impressive and stepwise explanations for all three videos. Thanks a lot.,2022-09-28T07:46:47Z
Ugxx0-FuoO9yBluYdE14AaABAg.9gWE6CXkQ1l9it0thsFf8N,@stratascratch,Ugxx0-FuoO9yBluYdE14AaABAg,2,N0aHeKyNEto,0,0,2022-11-26T06:35:54Z,Glad you enjoyed it!,2022-11-26T06:35:54Z
UgwHFh8act54zsb6Z7F4AaABAg,@sweety143sas,,1,N0aHeKyNEto,1,1,2022-08-29T07:32:14Z,"Any idea what should be the approach for oracle using cx_oracle, as for ddl statements it is not allowing to use bind variables.",2022-08-29T07:32:14Z
UgwHFh8act54zsb6Z7F4AaABAg.9fIxaU5Yimn9fezMAkuMpD,@stratascratch,UgwHFh8act54zsb6Z7F4AaABAg,2,N0aHeKyNEto,0,0,2022-09-07T06:10:09Z,I use Oracle only sporadically and not very extensively. I would advise you to post your question on Stack Overflow. Someone should be able to help you there. Try also Oracle Communities: Welcome | Oracle Communities and cx_Oracle community on Github: Issues ¬∑ oracle/python-cx_Oracle,2022-09-07T06:10:09Z
Ugz_eiuwFdOugDdBB_Z4AaABAg,@sweety143sas,,1,N0aHeKyNEto,1,1,2022-08-24T12:35:23Z,"Hey Nate,<br><br>I am getting this error while moving the files to the datasets directory:<br>mv &#39;Customer Contracts$.csv&#39; datasets<br>       ^<br>SyntaxError: invalid syntax<br><br>Dataset directory is created but the files are not moving",2022-08-24T12:35:23Z
Ugz_eiuwFdOugDdBB_Z4AaABAg.9f6cJSZ9Hlj9f6dynB83B-,@sweety143sas,Ugz_eiuwFdOugDdBB_Z4AaABAg,2,N0aHeKyNEto,0,0,2022-08-24T12:49:54Z,"I tried with this command<br>def move_csv_files(csv_files,origin,target):    # move files to directory<br>    print(origin)<br>    print(target)<br>    # Fetching csv files from origin to target directory<br>    for csv in csv_files:<br>        shutil.move(origin+csv, target)<br>    <br>    return",2022-08-24T12:49:54Z
UgyaxYBRdR8W7TZfxZZ4AaABAg,@sirojbekalimboyev6730,,1,N0aHeKyNEto,0,0,2022-07-26T20:33:56Z,Hi everybody!!<br>how can we count every 10000rows loaded from csv file to table in time interval?,2022-07-26T20:33:56Z
Ugw_Gb8nEPt2itKi6314AaABAg,@christopherwilhoite7856,,1,N0aHeKyNEto,0,1,2022-06-10T23:16:42Z,"Great content! Very thoroughly and clearly explained. I appreciate you taking the time to make such great content! I would love to see more of these series because they are not only educational, but implementable!",2022-06-10T23:16:42Z
UgyQlbww7i1-1arw0Qt4AaABAg,@diaconescutiberiu7535,,1,N0aHeKyNEto,2,0,2022-05-04T08:17:45Z,"How should we go about if we need to do some cleaning inside the CSV files? I have multiple CSV files which i need to ul to my postgre db; files have different columns ... so the cleaning is different for each column. If i just run this: &quot;dataframe[&quot;country&quot;] = dataframe[&quot;country&quot;].replace(to_replace=[&#39;Kingdom&#39;,&#39;States&#39;,&#39;Kong&#39;,&#39;Emirates&#39;,&#39;Rico&#39;], value=[&#39;United Kingdom&#39;,&#39;USA&#39;, &#39;Hong Kong&#39;,&#39;UAE&#39;,&#39;Puerto Rico&#39;])&quot; it will do the job and upload only the file that does have a country column, but it will fail for the other csv... script is giving me Keyerror:&#39;country&quot; (which is obvious as the other files don&#39;t have it)",2022-05-04T08:17:45Z
UgyQlbww7i1-1arw0Qt4AaABAg.9aalnO6CRFJ9abisSaSjTp,@stratascratch,UgyQlbww7i1-1arw0Qt4AaABAg,2,N0aHeKyNEto,0,1,2022-05-04T17:11:28Z,Could you add an if/else statement that will ignore columns that do not exist in other CSVs?,2022-05-04T17:11:28Z
UgyQlbww7i1-1arw0Qt4AaABAg.9aalnO6CRFJ9aedUh6XU9T,@diaconescutiberiu7535,UgyQlbww7i1-1arw0Qt4AaABAg,2,N0aHeKyNEto,0,0,2022-05-05T20:22:07Z,@@stratascratch Worked like a charm. Thank you!,2022-05-05T20:22:07Z
Ugw1mO4Dd_uxAyiy5c54AaABAg,@torontodataguy,,1,N0aHeKyNEto,0,0,2022-04-27T11:38:42Z,"@StrataScratch I would love to see the part 4, where you get the data from the database for some data analysis project( maybe simple data analysis)",2022-04-27T11:43:17Z
Ugy0cO7ShYv8H9DZuXZ4AaABAg,@Davidkiania,,1,N0aHeKyNEto,1,0,2022-04-19T16:54:43Z,Nate this content is soo good you don&#39;t even have to ask viewers to subscribe anyone you values it as much as we do will subscribe and do whatever it takes to keep in touch. This is extremely great content may you continue to sour in everything you do!,2022-04-19T16:54:43Z
Ugy0cO7ShYv8H9DZuXZ4AaABAg.9a041kBslkw9a0MBtiIIKT,@stratascratch,Ugy0cO7ShYv8H9DZuXZ4AaABAg,2,N0aHeKyNEto,0,0,2022-04-19T19:33:24Z,Thank you! I&#39;m glad you&#39;re enjoying the content,2022-04-19T19:33:24Z
UgxE2WbEv-I7hqMQbYR4AaABAg,@diaconescutiberiu7535,,1,N0aHeKyNEto,4,0,2022-04-18T13:42:13Z,"Awesome video (series). You, explaining the code.. and even going from beginner (video 1), advanced (video 2), expert (video 3) make these such a valuable asset for data analysts/scientists. I would really love for you to continue with automatizations like this. These beats any udemy/cursera videos... any day!. Is it possible to get a 4th video with more SQL stuff, such as: update data in the DB, with a new csv file (maybe something that gets updated daily), or append some new data to existing ones (with overwriting whatever gets duplicated)? I would also love some automatization with openpyxl (or similar libraries you are familiar with; maybe creating some charts with the cleaned data). I appreciate the efforts you&#39;ve put into this videos (i&#39;ve purchased the lifetime access on the platform) and I will make sure others will learn about your channel!",2022-04-18T13:42:13Z
UgxE2WbEv-I7hqMQbYR4AaABAg.9_y9Cv3-W019_z0WxqDhbl,@stratascratch,UgxE2WbEv-I7hqMQbYR4AaABAg,2,N0aHeKyNEto,0,0,2022-04-18T21:45:33Z,"Thanks for your support! there are some vids here - <a href=""https://www.youtube.com/watch?v=fklHBWow8vE"">https://www.youtube.com/watch?v=fklHBWow8vE</a> and <a href=""https://www.youtube.com/watch?v=77IVf0zgmwI"">https://www.youtube.com/watch?v=77IVf0zgmwI</a> that cover the topics you&#39;re talking about. The only difference is that it uses an API to collect data. I would follow the same guidelines but use a CSV and pd.dataframe rather than make an API call to collect the data. But the topics like overwriting with new data, etc is covered.",2022-04-18T21:45:33Z
UgxE2WbEv-I7hqMQbYR4AaABAg.9_y9Cv3-W019aN1v9ld9Zn,@diaconescutiberiu7535,UgxE2WbEv-I7hqMQbYR4AaABAg,2,N0aHeKyNEto,0,0,2022-04-28T14:58:45Z,"@@stratascratch I&#39;m having an issue with a specific csv i work with (i tested the final code with other csvs and they work just fine). I&#39;m getting this error: &quot;QueryCanceled: COPY from stdin failed: error in .read() call: UnicodeDecodeError &#39;charmap&#39; codec can&#39;t decode byte 0x81 in position 6589: character maps to &lt;undefined&gt;<br>CONTEXT:  COPY sfs_tool_t1, line 1&quot; . It passes these prints: &quot;opened db succesfully&quot;, &quot;csv was created succesfully&quot;, &quot;file opened in memory&quot; ... and next is the error ... any thoughts (googling doesn&#39;t provide with any helpful suggestion)",2022-04-28T14:58:45Z
UgxE2WbEv-I7hqMQbYR4AaABAg.9_y9Cv3-W019aOmkOVEpxf,@diaconescutiberiu7535,UgxE2WbEv-I7hqMQbYR4AaABAg,2,N0aHeKyNEto,0,0,2022-04-29T07:15:57Z,"I&#39;ve identified the columns that generates the issue (3 of them, containing text, like sentences). My csv is an export from a sharepoint/list. I suspect, while downloading some of the characters get mess up, so probably there is some issue with the encoding of that text. Perhaps i should do something with the text within those columns (some cleaning)",2022-04-29T07:15:57Z
UgxE2WbEv-I7hqMQbYR4AaABAg.9_y9Cv3-W019aPw9TMLao3,@stratascratch,UgxE2WbEv-I7hqMQbYR4AaABAg,2,N0aHeKyNEto,0,0,2022-04-29T17:57:23Z,@@diaconescutiberiu7535 Definitely the encoding. Try to force UTF-8 and that should fix it. I like to export from gSheets because they have the cleanest CSVs and I&#39;ve never had an issue loading a csv I exported from Google. I always have problems loading csvs that are exported from Excel. Hope that helps,2022-04-29T17:57:23Z
UgwV-M6J9VUUUPK-xRR4AaABAg,@AzureCz,,1,N0aHeKyNEto,0,0,2022-03-09T23:20:51Z,"About the LOTS of replaces, I found a better apporach XD<br><br>here:<br><br>def clean_tbl_name(filename):<br>    <a href=""http://www.youtube.com/results?search_query=%23rename"">#rename</a> csv, force lower case, no spaces, no dashes<br>    clean_tbl_name = filename.lower()<br>    <br>    exclude_list = [(&quot; &quot;, &quot;_&quot;), (&quot;-&quot;, &quot;_&quot;), (&quot;/&quot;, &quot;_&quot;), (&quot;\\&quot;, &quot;_&quot;), (&quot;$&quot;, &quot;&quot;), (&quot;%&quot;, &quot;&quot;),(&quot;&amp;&quot;,&quot;&quot;)]<br>    for character in exclude_list:<br>        clean_tbl_name = clean_tbl_name.replace(*character)<br>    .........<br>    .........<br><br>Hope it helps :D",2022-03-09T23:23:09Z
UgxcPukpw4feW3EslNZ4AaABAg,@AzureCz,,1,N0aHeKyNEto,4,0,2022-03-04T01:19:25Z,"This video is almost two years old and I have no hopes of getting an answer, but here it comes:<br><br>i don&#39;t understand DBs completely, but wouldn&#39;t be an bad practice to start a connection multiple times on the for loop? for what I know, I&#39;d start the connection above the for and them proceed with the for. What do you think? Would it work?",2022-03-04T01:19:25Z
UgxcPukpw4feW3EslNZ4AaABAg.9Z7xR2UBnX09ZAPfVWz0pn,@stratascratch,UgxcPukpw4feW3EslNZ4AaABAg,2,N0aHeKyNEto,0,1,2022-03-05T00:13:24Z,It could work. But why not just keep the connection open -),2022-03-05T00:13:24Z
UgxcPukpw4feW3EslNZ4AaABAg.9Z7xR2UBnX09ZAQrcGnT3T,@AzureCz,UgxcPukpw4feW3EslNZ4AaABAg,2,N0aHeKyNEto,0,0,2022-03-05T00:23:48Z,"‚Äã@@stratascratch as far as I understood the for will connect at each round, and disconnect at the end of it. did I get it wrong or something? hahah<br><br>What I thougth:<br><br>connect_function()<br><br>for_loop_function()<br><br>disconnect_function()<br><br>The way it is on the video, the connetion happens Inside the for procedure. Like we see on <a href=""https://www.youtube.com/watch?v=N0aHeKyNEto&amp;t=22m58s"">22:58</a>, the function &quot;upload_db&quot; is inside the for. I thought that starting a connection multiple times on the for loop could be a bad thing, and would be better to start it before the for. As I noted up there ü§îü§î",2022-03-05T00:23:48Z
UgxcPukpw4feW3EslNZ4AaABAg.9Z7xR2UBnX09ZATAPCjJty,@stratascratch,UgxcPukpw4feW3EslNZ4AaABAg,2,N0aHeKyNEto,0,1,2022-03-05T00:43:59Z,"@@AzureCz No it&#39;s not a big practice to open a connection only when you need it. To be honest, it really doesn&#39;t matter unless you&#39;re opening the connection for a long period of time as you might get a timeout.",2022-03-05T00:43:59Z
UgxcPukpw4feW3EslNZ4AaABAg.9Z7xR2UBnX09ZNBYTdk5cf,@AzureCz,UgxcPukpw4feW3EslNZ4AaABAg,2,N0aHeKyNEto,0,0,2022-03-09T23:20:06Z,@@stratascratch Thanks. I was thinkinh about paid APIs when you have a limit of data to pull from. but now I got it :D,2022-03-09T23:20:06Z
UgwXNZlSXkPWR0Nt7MJ4AaABAg,@prateek2159,,1,N0aHeKyNEto,0,0,2021-07-24T16:31:59Z,"Hey Nate, your videos are just too good. I love how your channel is so dedicated towards real word data science. By the way I noticed that you started a video series, &quot;For your Data Science Project&quot; and I really want you to continue making videos for this particular series because there&#39;s literally no one on YouTube with such guidance on DS projects and I have been looking for one since a very long time because I have my placements just after 12 months and I really want to make a full stack data science project. Thank you.",2021-07-24T16:31:59Z
UgwifFs_Eo5iivkQaw14AaABAg,@steven345lll1,,1,N0aHeKyNEto,4,0,2021-06-20T19:00:01Z,"Thank you for a great tutorial! How do you automate running scripts? Do you manually run the python script every time you want to upload csv files into AWS database or do you use any other scheduling software like airflow or Jenkins to do that so you don&#39;t need to worry about running it manually? Are you planning on covering the subject about scheduling your script that runs periodically?<br><br>As for making the interactive dashboard that ingests real time data, how can I refresh database automatically so what&#39;s displayed on the dashboard is real-time? Let&#39;s say that operators run a machine that outputs csv files with specific format which will be stored in the internal server in our company. I basically want my web application to ingest csv files in that directory and get them into aws database and displays the aggregated result real time. Any ideas? <br>Sorry for the long questions but I would appreciate your help!",2021-06-20T20:02:21Z
UgwifFs_Eo5iivkQaw14AaABAg.9Op5WHKdq5E9OpIj4IQW5C,@stratascratch,UgwifFs_Eo5iivkQaw14AaABAg,2,N0aHeKyNEto,0,0,2021-06-20T20:55:30Z,"Automate running scripts is the hardest thing about the process. It&#39;s not hard because it&#39;s technically difficult. It&#39;s hard because there are so many tools out there that can do it for you. I usually rely on whatever tools my company uses for automation which has included Jenkins, Airflow, and Domino. For personal use, I either will use Airflow (create an AWS EC2 instance and install Airflow) or just manually run it each time I need it. I wasn&#39;t going to cover scheduling scripts in my series. But you&#39;re spot on with mentioning Jenkins and Airflow.",2021-06-20T20:55:30Z
UgwifFs_Eo5iivkQaw14AaABAg.9Op5WHKdq5E9OpQJHHENKw,@steven345lll1,UgwifFs_Eo5iivkQaw14AaABAg,2,N0aHeKyNEto,0,0,2021-06-20T22:01:44Z,"@@stratascratch In case you are running Airflow in AWS EC2 instance, you still need to upload csv files manually from local to EC2 instance first and then use Airflow to move them into AWS Postgres right? Is there a way you can read files on a local machine from EC2 instance?",2021-06-20T22:02:39Z
UgwifFs_Eo5iivkQaw14AaABAg.9Op5WHKdq5E9OpR11eQ9rr,@stratascratch,UgwifFs_Eo5iivkQaw14AaABAg,2,N0aHeKyNEto,0,1,2021-06-20T22:07:59Z,"@@steven345lll1 Yea that&#39;s totally fair. For CSV files, I&#39;m not aware of any automated way to upload them from local to EC2. I supposed you could write an airflow py script that will ping your local for CSVs (but probably best to ping a Google Drive account or something more static). Otherwise, I would build the pipeline so that it doesn&#39;t even use CSV files. All data goes from API to db and all the transformations would be done on airflow. I&#39;ve never had to implement this use case but it does seem plausible.",2021-06-20T22:07:59Z
UgwifFs_Eo5iivkQaw14AaABAg.9Op5WHKdq5E9Opd7s2hTSF,@steven345lll1,UgwifFs_Eo5iivkQaw14AaABAg,2,N0aHeKyNEto,0,0,2021-06-21T00:02:31Z,@@stratascratch Thank you for your suggestion!,2021-06-21T00:02:31Z
UgxvOx-f0NLr7Kd8zgl4AaABAg,@andrefbillette2774,,1,N0aHeKyNEto,1,0,2021-04-28T00:33:32Z,Great series!,2021-04-28T00:33:32Z
UgxvOx-f0NLr7Kd8zgl4AaABAg.9MedklVywDN9MgKAg1CNp1,@stratascratch,UgxvOx-f0NLr7Kd8zgl4AaABAg,2,N0aHeKyNEto,0,0,2021-04-28T16:12:11Z,Thanks for watching!,2021-04-28T16:12:11Z
UgwvhYbuVWuCE2AhcyZ4AaABAg,@jordang8135,,1,N0aHeKyNEto,1,0,2021-01-05T01:12:38Z,Very nice series with lots of helpful info. Any reason you use Postgres over others? Do you find it easier to work with?,2021-01-05T01:12:38Z
UgwvhYbuVWuCE2AhcyZ4AaABAg.9I6kPKb964E9I75pnZDnq7,@stratascratch,UgwvhYbuVWuCE2AhcyZ4AaABAg,2,N0aHeKyNEto,0,1,2021-01-05T04:28:38Z,"The real answer is because I randomly chose it over 10 years ago when I was just starting and just kept with it. It&#39;s also super easy to deploy on AWS. And postgres is better than other free options like MySQL for analytics. Here&#39;s an article about the comparison (<a href=""https://hackr.io/blog/postgresql-vs-mysql)"">https://hackr.io/blog/postgresql-vs-mysql)</a>. On the job, most companies use industry grade dbs like HIVE, Greenplum, Snowflake, and MS-SQL server. There&#39;s only slight differences in terms of syntax.",2021-01-05T04:28:38Z
UgwPOSK8FP0MwsViYCJ4AaABAg,@majafuntv4538,,1,N0aHeKyNEto,1,1,2020-12-07T08:51:35Z,You explain your thought process which is much more valuable. You‚Äôre a true blessing!  Thank you so much!,2020-12-07T08:51:35Z
UgwPOSK8FP0MwsViYCJ4AaABAg.9GxtsQRsD_I9Gy_Z7e2e3T,@stratascratch,UgwPOSK8FP0MwsViYCJ4AaABAg,2,N0aHeKyNEto,0,0,2020-12-07T15:13:18Z,Thank you! Trying my best.,2020-12-07T15:13:18Z
UgzZsv7GuLUROL7tAKx4AaABAg,@classkori5507,,1,N0aHeKyNEto,1,1,2020-12-06T17:22:17Z,"Useful tutorial,think you so much",2020-12-06T17:22:17Z
UgzZsv7GuLUROL7tAKx4AaABAg.9GwEWveDFgg9GwFwS4Gw7F,@stratascratch,UgzZsv7GuLUROL7tAKx4AaABAg,2,N0aHeKyNEto,0,0,2020-12-06T17:34:38Z,thank you! Happy to take any ideas and feedback,2020-12-06T17:34:38Z
Ugyb9dprC_vtRJ0V-Z14AaABAg,@nargisparvin4267,,1,N0aHeKyNEto,1,1,2020-12-06T13:39:43Z,Thanks Sir,2020-12-06T13:39:43Z
Ugyb9dprC_vtRJ0V-Z14AaABAg.9Gvq2msZu899Gw7_IN-hFZ,@stratascratch,Ugyb9dprC_vtRJ0V-Z14AaABAg,2,N0aHeKyNEto,0,0,2020-12-06T16:21:34Z,Thanks for watching!,2020-12-06T16:21:34Z
UgyIbzWWbg8ski72b9B4AaABAg,@mysteriousbd3743,,1,N0aHeKyNEto,1,1,2020-12-06T13:35:13Z,"Thanks for part 3, I love this tutorial Series.",2020-12-06T13:35:13Z
UgyIbzWWbg8ski72b9B4AaABAg.9GvpXoqjP_i9Gw7ZFoLt-R,@stratascratch,UgyIbzWWbg8ski72b9B4AaABAg,2,N0aHeKyNEto,0,0,2020-12-06T16:21:26Z,"Thanks for watching. If there are any requests, please let me know and I&#39;ll try to make a video about it. <br><br>Also, let me know if you think the coding is too slow and should be faster. I&#39;m not always sure if people want to see me actually type code or if they would rather just seem me copy and paste the code in to make the video go faster.",2020-12-06T16:21:26Z
UgyBxzDx08cfBpDFDl54AaABAg,@camelrow,,1,N0aHeKyNEto,5,3,2020-12-04T06:24:10Z,Amazingly helpful and easy to follow. Love this series on automating common tasks with Python. <br><br>Can you do a series on automating with Python on calling an API and storing the JSON output in a database? <br>Thank you again!,2020-12-04T06:24:10Z
UgyBxzDx08cfBpDFDl54AaABAg.9Gpuc3m4j6N9Gqy30FdMzC,@stratascratch,UgyBxzDx08cfBpDFDl54AaABAg,2,N0aHeKyNEto,0,4,2020-12-04T16:13:26Z,"I&#39;m glad you like this series. I wasn&#39;t sure if it&#39;s a topic people liked it or found boring. But I&#39;ll aim to do a few more. My next one can definitely be automating an API call to storing the data into a database. I have a few SQL videos in queue right now but I&#39;ll aim to create another python video some time early next year. <br><br>I think what I&#39;ll also do is speed up the coding process too. Correct me if I&#39;m wrong but you don&#39;t actually need to see me coding so I might just show the code one line at a time and explain it. If you have a strong opinion about it one way or another, let me know.",2020-12-04T16:13:26Z
UgyBxzDx08cfBpDFDl54AaABAg.9Gpuc3m4j6N9Gr-DXoGc99,@camelrow,UgyBxzDx08cfBpDFDl54AaABAg,2,N0aHeKyNEto,0,2,2020-12-04T16:32:21Z,"@@stratascratch Personally, the coding part is very helpful to me, especially how you describe each step and each piece of your code. When you do it live it is slow enough for me to process what you are doing and understand. If you jump ahead and skip over the coding, it&#39;s too fast for me and I&#39;ll have to figure out each piece of what you are doing (lots of pausing). I&#39;m a novice, so that&#39;s my bias. Thank you!",2020-12-04T16:32:21Z
UgyBxzDx08cfBpDFDl54AaABAg.9Gpuc3m4j6N9Gr0kKpNauh,@stratascratch,UgyBxzDx08cfBpDFDl54AaABAg,2,N0aHeKyNEto,0,1,2020-12-04T16:45:42Z,@@camelrow That is really insightful and helpful. I will definitely keep the coding part in. Thanks for your input!,2020-12-04T16:45:42Z
UgyBxzDx08cfBpDFDl54AaABAg.9Gpuc3m4j6N9STeqhsbg0C,@grzegorzzawadzki3048,UgyBxzDx08cfBpDFDl54AaABAg,2,N0aHeKyNEto,0,0,2021-09-19T12:00:33Z,"‚Äã@@stratascratch  Hey Nate, I think this is one of the best data science channels out there. I&#39;ve spent the last few months learning DS from kaggle and tutorials on udemy/yt, but you&#39;re the first person to code the way I&#39;d like to learn. Unfortunately, most people focus on the DS part, so they completely ignore good software development practices. I would love to see more series on model building or cleaning data.",2021-09-19T12:00:33Z
UgyBxzDx08cfBpDFDl54AaABAg.9Gpuc3m4j6N9SU4L4soWIQ,@stratascratch,UgyBxzDx08cfBpDFDl54AaABAg,2,N0aHeKyNEto,0,1,2021-09-19T15:52:02Z,@@grzegorzzawadzki3048 Thanks so much! Glad you enjoy these videos. I agree with you on creating more videos on model building and cleaning data. I wish I had the time to create those videos =(. The python videos like this one takes so much effort and time that I am never able to do much else. I&#39;ll think about some other DS topics to create into videos! Thanks for the kind words and for watching my videos!,2021-09-19T15:52:02Z
Ugy8R3fHIhjtzlOxQYF4AaABAg,@stratascratch,,1,TDwy1lSjEZo,2,12,2020-11-24T16:38:44Z,"Find Part 1 where I create the script&#39;s functionality before automating the work here (<a href=""https://youtu.be/wqBFgaMgFQA)"">https://youtu.be/wqBFgaMgFQA)</a>.<br><br>Let me know what you guys think. Is this useful?",2020-11-24T16:38:44Z
Ugy8R3fHIhjtzlOxQYF4AaABAg.9GSG-ToWAao9GToUf4CApe,@trongtienhoang9522,Ugy8R3fHIhjtzlOxQYF4AaABAg,2,TDwy1lSjEZo,0,1,2020-11-25T07:08:04Z,"Hi Nate,<br>Your materials are fantastic<br>I wonder if you will provide discount for Black Friday this year",2020-11-25T07:08:04Z
Ugy8R3fHIhjtzlOxQYF4AaABAg.9GSG-ToWAao9GUhuq5o1FO,@stratascratch,Ugy8R3fHIhjtzlOxQYF4AaABAg,2,TDwy1lSjEZo,0,0,2020-11-25T15:29:51Z,"@@trongtienhoang9522 Thank you so much! Part 3 will come next week to finish off the series. I&#39;m glad you like it.<br><br>Yes I do have a Black Friday special which is 50% off any of the premium plans. You can use coupon code THANKS50. The discount is good until the end of the month. If you have any problems, feel free to email me at nate@<a href=""http://stratascratch.com/"">stratascratch.com</a>. Thank you!",2020-11-25T15:29:51Z
UgylrKlCcf-nGi_EwQh4AaABAg,@Veronica.cs.9,,1,TDwy1lSjEZo,0,0,2023-07-24T19:34:30Z,amazing!!!!,2023-07-24T19:34:30Z
Ugwx68WAZzsHOOAXieF4AaABAg,@T12F34,,1,TDwy1lSjEZo,1,0,2023-06-14T02:54:26Z,"This is awesome!  I&#39;m wondering how I can use this to automate a .csv load to a ms sql table, any ideas?",2023-06-14T02:54:26Z
Ugwx68WAZzsHOOAXieF4AaABAg.9qvbXcUZnFt9qxAdCi6i_8,@stratascratch,Ugwx68WAZzsHOOAXieF4AaABAg,2,TDwy1lSjEZo,0,0,2023-06-14T17:29:09Z,Yes absolutely. Most of the script can be used for csv --&gt; mysql load. You&#39;ll need to obviously change the credentials and use a mysql wrapper to connect to your db. You&#39;ll probably need a new SQL query (written in MySQL) to insert the data. That&#39;s probably all you need to do.,2023-06-14T17:29:09Z
UgxRFGj_UAPYaLcIYn94AaABAg,@prachipatel9388,,1,TDwy1lSjEZo,3,0,2023-03-13T16:32:37Z,"Hello Nate, How can i import live data (csv ) file from any application (both ios and android ) ?",2023-03-13T16:32:37Z
UgxRFGj_UAPYaLcIYn94AaABAg.9nCbEmGama89nCce6wj-cf,@stratascratch,UgxRFGj_UAPYaLcIYn94AaABAg,2,TDwy1lSjEZo,0,0,2023-03-13T16:44:57Z,"Sorry, what do you mean?",2023-03-13T16:44:57Z
UgxRFGj_UAPYaLcIYn94AaABAg.9nCbEmGama89nCd6nOwl7_,@prachipatel9388,UgxRFGj_UAPYaLcIYn94AaABAg,2,TDwy1lSjEZo,0,0,2023-03-13T16:49:00Z,"@@stratascratch I mean if i want data from any mobile application ( it is in .csv form ) , how do i fetch it ?",2023-03-13T16:49:00Z
UgxRFGj_UAPYaLcIYn94AaABAg.9nCbEmGama89nCedkn5WKz,@stratascratch,UgxRFGj_UAPYaLcIYn94AaABAg,2,TDwy1lSjEZo,0,0,2023-03-13T17:02:22Z,@@prachipatel9388 I have no idea. We don&#39;t cover data collection other than from an API in this series.,2023-03-13T17:02:22Z
UgyMFgiCA99EquJeZ5l4AaABAg,@lettalkaboutit,,1,TDwy1lSjEZo,0,0,2023-03-01T09:44:23Z,"Your video is fantastic, it help me, but this is  where I struggle and  I don&#39;t know if you can help with the following: my entire  csv file needs to go in the table results,  the dataframe created from the csv file have columns that contain foreign keys, I mean the csv files is a mix of different tables values, each column of my dataframe  represents a table, My use case is that: 1/ first step:  I have to loop through each column and compare the value in the column with the value from the corresponding table in the database, if the value does not exist then I create a SQL insert query to add this new value. After getting all new values in the databases then I do step 2<br><br>2/ Step : I get the value from each corresponding table from the database and for each columns I have to replace the data in each column by the table ID (foreign key) , and next send all the dataframe in the database<br>Can you help me achieve this please?",2023-03-01T09:44:23Z
Ugw-6yU9_Bv6__3Genl4AaABAg,@l00neytune40,,1,TDwy1lSjEZo,0,1,2023-01-12T05:01:29Z,You just got my sub. Nice tutorial. I really need to start learning to automate the repetitive tasks I do at work üëå,2023-01-12T05:01:29Z
Ugw-cPV9BdvBhnvXIHZ4AaABAg,@ltensail2267,,1,TDwy1lSjEZo,0,0,2023-01-02T12:16:42Z,Awesome thank you!,2023-01-02T12:16:42Z
UgzhZLnlGzjRImfi8XJ4AaABAg,@adiliophi,,1,TDwy1lSjEZo,0,1,2022-11-27T20:09:52Z,Your help is very useful! Thanks for sharing,2022-11-27T20:09:52Z
Ugzul_Ta5Kzi8WhzypN4AaABAg,@shubhanshukanungo5424,,1,TDwy1lSjEZo,0,1,2022-11-16T11:55:17Z,"hello sir, this video is really helpful. I tried to import 5 csv files to SQL Server database on my local machine. I didn&#39;t find any difficulties till creating the 5 different tables in database, but after that i got stuck while importing values. I tried numerous method. I don&#39;t to change the code manually for multiple files. Please make video on this if possible.",2022-11-16T11:55:17Z
UgziK_KjxxlH1746Ae94AaABAg,@DCCfhrue,,1,TDwy1lSjEZo,1,0,2022-11-11T13:22:27Z,"Thanks Nate for the great value! Here&#39;s a minor helpful modification for when ths CSV filnames start with a number/integer ‚Äîto prevent the <br><b>sqlite3.OperationalError: unrecognized token: &quot;0001test&quot;</b> error:<br>(using f-string notation for recent python versions)<br><br># drop table with same name<br># cursor.execute(&quot;drop table if exists %s;&quot; % (tbl_name)) # older placeholder notation style<br><b>cursor.execute(f&quot;drop table if exists \&#39;{tbl_name}\&#39;;&quot;)</b> # f-string version<br><br><br># cursor.execute(&quot;create table %s (%s);&quot; % (tbl_name, col_str))  # ERROR<br># cursor.execute(&quot;create table \&#39;%s\&#39; (%s);&quot; % (tbl_name, col_str)) # ERROR<br><b>cursor.execute(f&quot;create table \&#39;{tbl_name}\&#39; (\&#39;{col_str}\&#39;);&quot;)</b> # integer as filename&#39;s 1st character WORKS<br><br>Cheers!",2022-11-11T13:22:27Z
UgziK_KjxxlH1746Ae94AaABAg.9iI7VBrxRkU9iIyZ9waFd_,@DCCfhrue,UgziK_KjxxlH1746Ae94AaABAg,2,TDwy1lSjEZo,0,0,2022-11-11T21:14:51Z,"Just found the Primary Key setting from pandas.DataFrame.to_sql doc<br>dataframe.to_sql(tbl_name, con=conn, <b>dtype={&#39;MyPKColumnnamehere&#39;: &#39;INTEGER PRIMARY KEY&#39;})</b>",2022-11-11T21:14:51Z
Ugw30_1P3x8BBGVY0Cx4AaABAg,@monascholtz2440,,1,TDwy1lSjEZo,0,0,2022-11-11T11:25:20Z,Thank you for sharing your knowledge - I enjoy your tutorials!,2022-11-11T11:25:20Z
UgyfgAMcWQ8m2fJe_p94AaABAg,@jacobsaddler7664,,1,TDwy1lSjEZo,2,0,2022-11-02T22:52:42Z,"Hey, love the video! Just wondering if you&#39;ve ever come across this issue: Kernel is running properly, but nothing within the for loops are appearing in the output. For example, if I make a new kernel and simply print(&#39;hello world&quot;), the output shows as: hello world. However, within my for loops( for k in csv_files), nothing is printing even though the kernel is still running without error. I have print(&#39;opened database successfully&#39;) in this for loop, which DOES NOT show up in my output. However, print(&#39;all tables have been successfully imported into the db&#39;), outside of the for loop, DOES show up in the output. Any idea what could be causing this? I was not previously having this issue but at some point I re-ran all cells and am no longer getting the outputs within the for loops.",2022-11-02T22:52:42Z
UgyfgAMcWQ8m2fJe_p94AaABAg.9hwyaxqfC729hxlm9pzizl,@stratascratch,UgyfgAMcWQ8m2fJe_p94AaABAg,2,TDwy1lSjEZo,0,0,2022-11-03T06:19:52Z,Hard to really say without looking at the code. I would just try to print an output at every step to see what&#39;s going on. And try to see if there is any data being ingested into the loop.,2022-11-03T06:19:52Z
UgyfgAMcWQ8m2fJe_p94AaABAg.9hwyaxqfC729hzcPvaR-ZW,@jacobsaddler7664,UgyfgAMcWQ8m2fJe_p94AaABAg,2,TDwy1lSjEZo,0,0,2022-11-03T23:36:32Z,@@stratascratch I appreciate the response! Good friend who knows python was able to fix the bug for me. This vid was extremely helpful and keep up the great content!,2022-11-03T23:36:32Z
UgzLyXQ2iWQ2OXh4YgZ4AaABAg,@Lyriks_,,1,TDwy1lSjEZo,1,0,2022-09-27T12:07:59Z,"Hello Stratascratch, thanks for your videos (the whole serie on automating csv to psotgre), i couldn&#39;t help but notice your e-cornell diploma in the background, is it worth it ? Cheers from Switzerland",2022-09-27T12:07:59Z
UgzLyXQ2iWQ2OXh4YgZ4AaABAg.9gU7CSWNpe19gUdRzywJJ9,@stratascratch,UgzLyXQ2iWQ2OXh4YgZ4AaABAg,2,TDwy1lSjEZo,0,0,2022-09-27T16:58:28Z,"Thanks! I actually attended Cornell for my grad school (in person, not online). I don&#39;t know much about their online program but their in-person one is definitely worth it. I had a great time =)",2022-09-27T16:58:28Z
UgwpKupueVlUuHZFiQ14AaABAg,@danielschellhorn8838,,1,TDwy1lSjEZo,0,0,2022-09-10T16:22:17Z,Why don&#39;t you use a list comprehension to find the csv files?,2022-09-10T16:22:17Z
UgzN0HyFJj94FYLITpp4AaABAg,@rezahamzeh3736,,1,TDwy1lSjEZo,0,4,2022-09-07T00:22:05Z,That is a difference between pure tutorials and best-practices for real-world scenarios. Can you please continue producing more of ETL tutorials with the same style?,2022-09-07T00:22:05Z
UgwDtO4Hh9zgV-umICh4AaABAg,@gregNFL,,1,TDwy1lSjEZo,0,0,2022-05-31T06:08:40Z,Thank you. Beautiful. So clean! Even addresses Unicode errors. The cleanup bits are an added bonus. 10/10,2022-05-31T06:08:40Z
UgxzGUjdh4SWZ-bKJH94AaABAg,@Davidkiania,,1,TDwy1lSjEZo,0,0,2022-04-19T16:50:26Z,Most helpful youtube video series I have seen in a long long time ... thank you soo much.,2022-04-19T16:50:26Z
Ugx5uKzRizmXAZnv6Wx4AaABAg,@diaconescutiberiu7535,,1,TDwy1lSjEZo,2,0,2022-04-15T15:20:13Z,I am on windows and mv command is not recognized what can i replace it with,2022-04-15T15:20:13Z
Ugx5uKzRizmXAZnv6Wx4AaABAg.9_qb2-hmMJh9_qq2N3ySaE,@stratascratch,Ugx5uKzRizmXAZnv6Wx4AaABAg,2,TDwy1lSjEZo,0,0,2022-04-15T17:31:20Z,Try google to get the answer?,2022-04-15T17:31:20Z
Ugx5uKzRizmXAZnv6Wx4AaABAg.9_qb2-hmMJh9f-XqAfhrFm,@chk3967,Ugx5uKzRizmXAZnv6Wx4AaABAg,2,TDwy1lSjEZo,0,0,2022-08-21T18:32:53Z,If you are coding in windowa then replace    :-  <br>&quot;mv &#39;{0}&#39; {1}&quot;        with           &#39;move &quot;{0}&quot; {1}&#39;  <br>.... (replace &#39;single quote&#39; and &quot;double quote&quot; with each other ).,2022-08-21T18:32:53Z
UgzdM6yom7ZOsKRNx654AaABAg,@arthuramanyire526,,1,TDwy1lSjEZo,5,0,2022-02-28T13:05:54Z,"Hi Nathan, thanks alot for sharing this. I have tried to follow along but i have hit a wall. At the point where the datasets directory is created all is fine but the code to move the csv files to the datasets directory runs with no errors but the files are not moved. I&#39;m using windows and i noticed you are using mac, could the issue be the os i&#39;m using?<br><br>One other thing, would you consider continuing this series by showing how to pull the data from db to tableau/power bi for realtime analysis",2022-02-28T13:05:54Z
UgzdM6yom7ZOsKRNx654AaABAg.9Yzv60oKuR_9Z-_g141cLo,@stratascratch,UgzdM6yom7ZOsKRNx654AaABAg,2,TDwy1lSjEZo,0,0,2022-02-28T19:17:57Z,There is a difference between windows and mac related to how you write the path so that&#39;s one potential issue. But there could be a lot of different reasons why it&#39;s not working since everyone&#39;s setup is different. Happy to extend this series once I have some availability.,2022-02-28T19:17:57Z
UgzdM6yom7ZOsKRNx654AaABAg.9Yzv60oKuR_9f-XkXekQvD,@chk3967,UgzdM6yom7ZOsKRNx654AaABAg,2,TDwy1lSjEZo,0,1,2022-08-21T18:32:07Z,If you are coding in windowa then replace    :-  <br>&quot;mv &#39;{0}&#39; {1}&quot;        with           &#39;move &quot;{0}&quot; {1}&#39;  <br>.... (replace &#39;single quote&#39; and &quot;double quote&quot; with each other ).,2022-08-21T18:32:07Z
UgzdM6yom7ZOsKRNx654AaABAg.9Yzv60oKuR_9fWQxYFH-5G,@sushmita4713,UgzdM6yom7ZOsKRNx654AaABAg,2,TDwy1lSjEZo,0,0,2022-09-03T13:07:39Z,"Hi Arthur, I am sure you must have figured it out already. but here is what I did when I got the same error in windows.<br>for file in csv_files:<br>    original = r&#39;{0}&#39;.format(file)<br>    target = r&#39;{0}&#39;.format(data_dir)<br>    shutil.move(original, target)",2022-09-03T13:07:39Z
UgzdM6yom7ZOsKRNx654AaABAg.9Yzv60oKuR_9fWS06f-B9D,@arthuramanyire526,UgzdM6yom7ZOsKRNx654AaABAg,2,TDwy1lSjEZo,0,0,2022-09-03T13:16:53Z,@@sushmita4713 thanks Sushmita will try it out,2022-09-03T13:16:53Z
UgzdM6yom7ZOsKRNx654AaABAg.9Yzv60oKuR_9g_KsSQ_JLC,@majubodas6206,UgzdM6yom7ZOsKRNx654AaABAg,2,TDwy1lSjEZo,0,0,2022-09-29T22:02:53Z,"@@chk3967 Hey, I tried this and it finally worked! thank you so much!!",2022-09-29T22:02:53Z
UgzNkfhSTXDLcnyfTw14AaABAg,@TheCpopp,,1,TDwy1lSjEZo,1,0,2022-02-23T07:17:13Z,"If you are using AWS services, you might want to import boto3 and use SDK api calls instead of using psycopg2. Also, if you are using AWS, you don‚Äôt want to pass your database password as plaintext in a parameter. It‚Äôs better to use boto3 and pull the sensitive data from AWS Secrets Manager service.",2022-02-23T07:17:13Z
UgzNkfhSTXDLcnyfTw14AaABAg.9YmQDsJO9cx9Yngu_kBhUJ,@stratascratch,UgzNkfhSTXDLcnyfTw14AaABAg,2,TDwy1lSjEZo,0,0,2022-02-23T19:10:59Z,"That&#39;s a good point. I&#39;ve never played with boto3 but if it does what you say then I agree, it&#39;s much better than the current implementation.",2022-02-23T19:10:59Z
UgwLuqHjTi6AAKLsL154AaABAg,@pulakkabir2276,,1,TDwy1lSjEZo,0,0,2022-02-06T06:04:14Z,"bro, how to start with an (almost) empty CLASS with the headers of your data file?",2022-02-06T06:04:14Z
Ugxcp1MwNdyupOvCsSh4AaABAg,@Victoria-dl9uk,,1,TDwy1lSjEZo,1,0,2022-01-17T04:35:54Z,"This is absolutely amazing, Nathan. Can&#39;t thank you enough for sharing your talent in teaching across the community of DS. Would there be a &quot;quick&quot; way to change this code to open .xls instead of .csv? And if possible transform that .xls into a .csv file and proceed with the rest of your code? That would be super helpful if you have any advise (or other videos to refer to). Thanks!",2022-01-17T04:35:54Z
Ugxcp1MwNdyupOvCsSh4AaABAg.9XHrLyKHneM9XJcYSxTThq,@stratascratch,Ugxcp1MwNdyupOvCsSh4AaABAg,2,TDwy1lSjEZo,0,0,2022-01-17T21:05:01Z,"Try this resource (<a href=""https://pythonbasics.org/read-excel/)"">https://pythonbasics.org/read-excel/)</a>. My only advice is that Excel uploads can be message because they have a lot of encodings (like UTF) that can&#39;t be read by pandas. You&#39;ll see weird symbols if there are some decoding errors when you import your excel file. It&#39;s the main reason why I stick with CSV files.",2022-01-17T21:05:01Z
UgyfZD013YQQKo6jp4d4AaABAg,@iveependo9997,,1,TDwy1lSjEZo,4,0,2022-01-01T13:16:26Z,"hi! <br><br>for csv in csv_files:<br>    mv_file = &quot;mv &#39;{0}&#39; {1}&quot;.format(csv, dataset_dir)<br>    os.system(mv_file)<br><br>this part didn&#39;t move my files into the new datasets directory. Am I missing something? Thanks!",2022-01-01T13:16:26Z
UgyfZD013YQQKo6jp4d4AaABAg.9WeaCR7h_jm9WfLEsUXMnT,@stratascratch,UgyfZD013YQQKo6jp4d4AaABAg,2,TDwy1lSjEZo,0,0,2022-01-01T20:16:12Z,"Really hard to say and debug without the notebook. But I would print out the for loop and see what you have. If you think it&#39;s right, try running the code manually to see if it moved your files.",2022-01-01T20:16:12Z
UgyfZD013YQQKo6jp4d4AaABAg.9WeaCR7h_jm9ZRQXBpgHLx,@mohammadfaruque2865,UgyfZD013YQQKo6jp4d4AaABAg,2,TDwy1lSjEZo,0,0,2022-03-11T14:47:58Z,@@stratascratch I think it is happening because of the mv cmd is not recognized in windows. I have found ppl are using shutil to move files in windows systems.,2022-03-11T14:47:58Z
UgyfZD013YQQKo6jp4d4AaABAg.9WeaCR7h_jm9f0kgmJzsKa,@chk3967,UgyfZD013YQQKo6jp4d4AaABAg,2,TDwy1lSjEZo,0,1,2022-08-22T05:53:10Z,If you are coding in windowa then replace    :-  <br>&quot;mv &#39;{0}&#39; {1}&quot;        with           &#39;move &quot;{0}&quot; {1}&#39;  <br>.... (replace &#39;single quote&#39; and &quot;double quote&quot; with each other ).,2022-08-22T05:53:10Z
UgyfZD013YQQKo6jp4d4AaABAg.9WeaCR7h_jm9fWR8Fdt0n8,@sushmita4713,UgyfZD013YQQKo6jp4d4AaABAg,2,TDwy1lSjEZo,0,0,2022-09-03T13:09:15Z,"Hi if you are  using windows use this  <br>for file in csv_files:<br>    original = r&#39;{0}&#39;.format(file)<br>    target = r&#39;{0}&#39;.format(data_dir)<br>    shutil.move(original, target)",2022-09-03T13:09:15Z
Ugykqlq5taA0EbiQmyN4AaABAg,@XxShadowzeekxX,,1,TDwy1lSjEZo,3,1,2021-11-30T09:40:17Z,what do you mean by running the os system and executing on the batch command? How do I do this because my files have not moved to the new datasets folder? Thank you!,2021-11-30T09:40:17Z
Ugykqlq5taA0EbiQmyN4AaABAg.9VMo1IziFIx9VOq-1aEKc1,@stratascratch,Ugykqlq5taA0EbiQmyN4AaABAg,2,TDwy1lSjEZo,0,0,2021-12-01T04:35:56Z,There&#39;s an os library in python that can help you navigate the folders. I don&#39;t remember specifics about the batch command. Probably means bulk inserting the rows in large CSV files. There&#39;s a parameter in the import function that allows for this.,2021-12-01T04:35:56Z
Ugykqlq5taA0EbiQmyN4AaABAg.9VMo1IziFIx9f0kkRT69bJ,@chk3967,Ugykqlq5taA0EbiQmyN4AaABAg,2,TDwy1lSjEZo,0,1,2022-08-22T05:53:40Z,If you are coding in windowa then replace    :-  <br>&quot;mv &#39;{0}&#39; {1}&quot;        with           &#39;move &quot;{0}&quot; {1}&#39;  <br>.... (replace &#39;single quote&#39; and &quot;double quote&quot; with each other ).,2022-08-22T05:53:40Z
Ugykqlq5taA0EbiQmyN4AaABAg.9VMo1IziFIx9md8400z8YH,@luzestrada9536,Ugykqlq5taA0EbiQmyN4AaABAg,2,TDwy1lSjEZo,0,0,2023-02-27T12:36:20Z,"@@chk3967 thank you so much! Making this changes, worked for me &lt;3",2023-02-27T12:36:20Z
Ugyw4PiI9K68iox6mOZ4AaABAg,@anthonyobrienvicenciopalla641,,1,TDwy1lSjEZo,1,0,2021-08-20T05:48:08Z,"Hi nate,nice video :)<br>I have a questions... How I fix this?? COPY from stdin failed: error in .read() call: UnicodeDecodeError &#39;charmap&#39; codec can&#39;t decode byte 0x81 in position 885: character maps to &lt;undefined&gt;<br>CONTEXT:  COPY tags, line 9260",2021-08-20T05:48:08Z
Ugyw4PiI9K68iox6mOZ4AaABAg.9RFkNiSyBAx9sHmBLKNkCf,@lucydavis6325,Ugyw4PiI9K68iox6mOZ4AaABAg,2,TDwy1lSjEZo,0,0,2023-07-17T23:21:26Z,"this is an issue with the encoding, you must have a special character in your file.. I was able to fix this when opening the file setting the encoding to latin_1 ie my_file= open(k, encoding=&quot;latin_1&quot;)",2023-07-17T23:21:26Z
UgywlVy_KSdeXllN2c14AaABAg,@sudarshanvu4807,,1,TDwy1lSjEZo,1,0,2021-08-08T13:15:27Z,Awesome learning platform.<br>Here my question is Instead of pandas df can we do same project task in Pyspark Pyspark-SQL and AWS RDS???,2021-08-08T13:15:27Z
UgywlVy_KSdeXllN2c14AaABAg.9Qme1QOCmBt9Qmr5N-FziY,@stratascratch,UgywlVy_KSdeXllN2c14AaABAg,2,TDwy1lSjEZo,0,1,2021-08-08T15:09:35Z,"For AWS RDS, change the language to Postgres in the platform. Unfortunately, I don&#39;t have pySpark on the platform.",2021-08-08T15:09:35Z
UgwOjJFiQ6IWiRG0uhZ4AaABAg,@prateek2159,,1,TDwy1lSjEZo,0,2,2021-07-24T16:31:48Z,"Hey Nate, your videos are just too good. I love how your channel is so dedicated towards real word data science. By the way I noticed that you started a video series, &quot;For your Data Science Project&quot; and I really want you to continue making videos for this particular series because there&#39;s literally no one on YouTube with such guidance on DS projects and I have been looking for one since a very long time because I have my placements just after 12 months and I really want to make a full stack data science project. Thank you.",2021-07-24T16:31:48Z
Ugx8YCErSSEa83L-RtJ4AaABAg,@harshamadhwani8438,,1,TDwy1lSjEZo,1,1,2021-05-27T12:03:50Z,Simply Superb... love the way you explain and make it sound so simple. Thank you so much!!,2021-05-27T12:03:50Z
Ugx8YCErSSEa83L-RtJ4AaABAg.9NqYo3CuMsA9NqvZXk3Fc7,@stratascratch,Ugx8YCErSSEa83L-RtJ4AaABAg,2,TDwy1lSjEZo,0,1,2021-05-27T15:31:25Z,Thanks so much for watching!,2021-05-27T15:31:25Z
UgxNlxh9XLdhXOfG7EJ4AaABAg,@stephensander3061,,1,TDwy1lSjEZo,2,0,2021-03-18T17:18:57Z,"Hi I love your videos, is it possible to make this script a rest API?",2021-03-18T17:18:57Z
UgxNlxh9XLdhXOfG7EJ4AaABAg.9L1sDnSwtJj9L2Q1qUA3Lm,@stratascratch,UgxNlxh9XLdhXOfG7EJ4AaABAg,2,TDwy1lSjEZo,0,1,2021-03-18T22:23:09Z,I think so? I think one way is to use Flask to turn it into a rest API? That&#39;s maybe what I&#39;d do but I usually collect from APIs not write one so I&#39;m a bit of a beginner there =),2021-03-18T22:23:09Z
UgxNlxh9XLdhXOfG7EJ4AaABAg.9L1sDnSwtJj9Lxx8WVzdDc,@stephensander3061,UgxNlxh9XLdhXOfG7EJ4AaABAg,2,TDwy1lSjEZo,0,0,2021-04-10T15:57:06Z,@@stratascratch Thanks man!,2021-04-10T15:57:06Z
UgxF2f5QPIcK438e4hJ4AaABAg,@wanderborn.,,1,TDwy1lSjEZo,1,0,2021-02-26T15:34:11Z,Thanks a lot. It&#39;s just what I was looking for to learn-how.<br>Just a though: wouldn&#39;t it be right/appropriate to clean the data in .csv and then move it to db?,2021-02-26T15:34:11Z
UgxF2f5QPIcK438e4hJ4AaABAg.9KEBKxvJLS79KEXlA8NJ1K,@stratascratch,UgxF2f5QPIcK438e4hJ4AaABAg,2,TDwy1lSjEZo,0,0,2021-02-26T18:50:08Z,"It would be very helpful to clean the data before moving it to the db. I created some automated clean up functions in this video that cleans up the column headers and converts data types. But I don&#39;t touch the actual data. Since the CSV is converted to a pd dataframe, it&#39;s easy to clean it up.",2021-02-26T18:50:08Z
UgwuZhwtX1UJa0JGEOt4AaABAg,@javierjdaza,,1,TDwy1lSjEZo,5,0,2021-02-10T19:32:52Z,dude why use copy instead of insert into sql???,2021-02-10T19:32:52Z
UgwuZhwtX1UJa0JGEOt4AaABAg.9JaPw7FgIOp9Jarv57RWcM,@stratascratch,UgwuZhwtX1UJa0JGEOt4AaABAg,2,TDwy1lSjEZo,0,5,2021-02-10T23:46:08Z,"You&#39;re like the only person that caught that! In reality, I prefer doing INSERTs because when I upload data to databases, it&#39;s usually in batches, repeated on a schedule. So I&#39;d create an automatic job that would grab new data and upload the new rows into the db table &lt;-- this would require me to use an INSERT. <br><br>In this video, I&#39;m just doing 1 time uploads to the database. The data and the table is basically static at that point. So in this case, it&#39;s more advantageous to use COPY because it can handle larger files. I was able to test with CSV files that were &gt;5gbs. If I used an INSERT, I would have had to batch my inserts. The end result would be the same but I just thought it was faster to do it using COPY. <br><br>Thanks for the question!",2021-02-10T23:46:08Z
UgwuZhwtX1UJa0JGEOt4AaABAg.9JaPw7FgIOp9Jb-3nzPYYw,@javierjdaza,UgwuZhwtX1UJa0JGEOt4AaABAg,2,TDwy1lSjEZo,0,1,2021-02-11T00:57:21Z,"@@stratascratch thanks for the answear dude. Another question, how would be the code for example if i had a previous table, let me call it table1 of the  DB1, and i just want to insert a data frame into this table. <br>Avoiding the ‚Äúcopy‚Äù query, what i need to use? <br>Thanks u man. <br>(Sorry for my english, im spanish guy)",2021-02-11T00:57:21Z
UgwuZhwtX1UJa0JGEOt4AaABAg.9JaPw7FgIOp9Jb9G6d5Q0W,@stratascratch,UgwuZhwtX1UJa0JGEOt4AaABAg,2,TDwy1lSjEZo,0,2,2021-02-11T02:26:25Z,"@@javierjdaza Basically you can use the pd.to_sql() method to append your data in the pandas df into table1. So it could look something like this <br><br>df.to_sql(&#39;table1&#39;, engine, if_exists=&#39;append&#39;)<br><br>where `engine` is your SQL engine like postgres. There are other ways but this way uses a method from the pandas library which is kind of nice. Let me know if that makes sense.",2021-02-11T02:26:25Z
UgwuZhwtX1UJa0JGEOt4AaABAg.9JaPw7FgIOp9Jb9l_C141w,@javierjdaza,UgwuZhwtX1UJa0JGEOt4AaABAg,2,TDwy1lSjEZo,0,0,2021-02-11T02:30:51Z,"@@stratascratch man thank you, ill check it out, please keep doing this kind of videos. <br>A hug!",2021-02-11T02:30:51Z
UgwuZhwtX1UJa0JGEOt4AaABAg.9JaPw7FgIOp9JbIOQWBj1U,@stratascratch,UgwuZhwtX1UJa0JGEOt4AaABAg,2,TDwy1lSjEZo,0,1,2021-02-11T03:46:12Z,@@javierjdaza Thanks so much man! Will keep trying to do coding videos!,2021-02-11T03:46:12Z
UgzuxyCXheYxjwDvZDJ4AaABAg,@oluwaseunatoyebi6796,,1,TDwy1lSjEZo,1,4,2021-02-05T04:03:53Z,This is incredible my friend. Can you do a video where you load each of the 3 files into separate tables? I&#39;d be curious to see how you dynamically handle that. Thanks for the great work.,2021-02-05T04:03:53Z
UgzuxyCXheYxjwDvZDJ4AaABAg.9JMsdh36u9M9JPWxAChOY7,@stratascratch,UgzuxyCXheYxjwDvZDJ4AaABAg,2,TDwy1lSjEZo,0,1,2021-02-06T04:43:17Z,"In this method, you should be able to load 3 tables pretty easily with a for loop. Just make sure you close and open the connection each time. This method can handle pretty large datasets and number of rows. I&#39;ve imported 5gb files to db tables. Other methods that reply in INSERTs have limitations on the number of rows, not necessarily the number of files. If there are over 100K+ rows, use the batch parameter found in the psycopg2 documentation. Then just batch the rows into the table 1K at a time. Hope that provides some more insights into the limitations of this method.",2021-02-06T04:43:17Z
Ugx_LL1V4ZIdAfkIQyh4AaABAg,@abdobourenane9294,,1,TDwy1lSjEZo,3,0,2021-02-01T18:31:20Z,"What an amazing code !! , i love your explanation , just if you can give us the github link to the data so we can practice , Thank you",2021-02-01T18:31:20Z
Ugx_LL1V4ZIdAfkIQyh4AaABAg.9JE7jYhsOJt9JEXYFA8ST7,@stratascratch,Ugx_LL1V4ZIdAfkIQyh4AaABAg,2,TDwy1lSjEZo,0,1,2021-02-01T22:16:50Z,"Here&#39;s the github repo URL: <a href=""https://github.com/Strata-Scratch/csv_to_db_automation"">https://github.com/Strata-Scratch/csv_to_db_automation</a><br><br>Thanks for watching!",2021-02-01T22:16:50Z
Ugx_LL1V4ZIdAfkIQyh4AaABAg.9JE7jYhsOJt9JFtpSx2k8N,@abdobourenane9294,Ugx_LL1V4ZIdAfkIQyh4AaABAg,2,TDwy1lSjEZo,0,0,2021-02-02T10:59:33Z,@@stratascratch Thank you,2021-02-02T10:59:33Z
Ugx_LL1V4ZIdAfkIQyh4AaABAg.9JE7jYhsOJt9JGSmInLyJ7,@stratascratch,Ugx_LL1V4ZIdAfkIQyh4AaABAg,2,TDwy1lSjEZo,0,0,2021-02-02T16:13:41Z,@@abdobourenane9294 Thanks for watching!,2021-02-02T16:13:41Z
UgyWF5F79LAvtYiWQoJ4AaABAg,@everettwitt9464,,1,TDwy1lSjEZo,5,0,2021-01-28T22:29:09Z,this is an incredible channel - thanks a ton man,2021-01-28T22:29:09Z
UgyWF5F79LAvtYiWQoJ4AaABAg.9J4FlwXIPbm9J4LRYw5Zqb,@stratascratch,UgyWF5F79LAvtYiWQoJ4AaABAg,2,TDwy1lSjEZo,0,0,2021-01-28T23:18:40Z,Thanks so much and thanks for watching. Let me know if there&#39;s any topic you&#39;d like me to cover. I love getting ideas from my audience.,2021-01-28T23:18:40Z
UgyWF5F79LAvtYiWQoJ4AaABAg.9J4FlwXIPbm9J8wl3oFvLz,@everettwitt9464,UgyWF5F79LAvtYiWQoJ4AaABAg,2,TDwy1lSjEZo,0,0,2021-01-30T18:10:28Z,"@@stratascratch <br><br>This is more of a concept - Data Science is so much more than using algorithms to analyze data. I always heard and always think about the concept of &quot;garbage in garbage out&quot;, but I never really pushed further and asked how can you verify that the data you are using is not garbage?<br><br>Writing scripts to clean and sanitize parts of the data is great, but how can you confirm your data is useful? I guess in a sense making a prediction model on the data and seeing that your predictions match reality closely is probably a secondhand test confirming that your data is in fact quality. However, if you&#39;re model fails there are many possible reasons why - it would be nice to know that your data quality isn&#39;t one of them. When you have data sets that are massive it is impossible to go through everything - can you use sampling to test the validity so you can try and make a statement about the whole?<br><br>The process of having nothing, writing a scraper to get some public information, making sure the data you have is quality, adding new data every day, having the ability to fetch some other historical data then add it to your dataset. If the data is going to be massive, what platform should you use to store it? How should you query it efficiently? When you have a nice, big, quality dataset then you can do some fun analysis.<br><br>I guess I&#39;m rambling because I want to start collecting horse racing data, so I have thought about everything above in that context.",2021-01-30T18:10:28Z
UgyWF5F79LAvtYiWQoJ4AaABAg.9J4FlwXIPbm9J9x6eM7gIZ,@stratascratch,UgyWF5F79LAvtYiWQoJ4AaABAg,2,TDwy1lSjEZo,0,0,2021-01-31T03:32:48Z,"@@everettwitt9464 Wow there&#39;s a lot of topics to unpack there. <br><br>On the note about data quality -- you&#39;re absolutely right. How do you know if it&#39;s even quality data? How do you know the data you have collected and cleaned is going to be useful for predicting and building models? You never really know and even if you build a great model or a bad model, you still don&#39;t really know. It could have been luck or bad luck. What&#39;s important is iterating on the model to make it better every time you optimize your model, collect more data, and/or refine your features.<br><br>Which brings me to your last few thoughts about collecting data. &quot;A massive collection of data&quot; means a lot of things to a lot of people. You could collect millions of rows but still keep it on 1 db server. You could collect billions of rows and still keep it on a relational db that&#39;s distributed and sharded. Or you can go no SQL if you need to. I never really worry about the db too much for personal use even if I do collect millions of rows. Once you get bigger, you definitely want to go to industry tools like Snowflake and/or HIVE and use other means like pySpark. But if it comes to that -- there are more qualified people to handle the data so that you can keep churning out the analysis.<br><br>Those are some of my initial thoughts. There&#39;s so much more though....thanks for the brain dump!",2021-01-31T03:32:48Z
UgyWF5F79LAvtYiWQoJ4AaABAg.9J4FlwXIPbm9JBdgNzg0Lh,@everettwitt9464,UgyWF5F79LAvtYiWQoJ4AaABAg,2,TDwy1lSjEZo,0,0,2021-01-31T19:21:32Z,"@@stratascratch so what about a video or something on pySpark and handling &quot;massive datasets&quot; where using pandas simply doesn&#39;t cut it? Or using HIVE/Snowflake - things that you might potentially see on the job but not when doing a project on your own (because you arent working with nearly the same magnitude of data)?<br><br>and on the data quality part - there must be automated tests (like sampling from the data, testing that sample to generalize something about the population/whole dataset) no? The proxy for data quality cannot just be model accuracy I assume?<br><br>Regardless, thanks for the current videos and responses :)",2021-01-31T19:21:32Z
UgyWF5F79LAvtYiWQoJ4AaABAg.9J4FlwXIPbm9JCWtm4HRBV,@stratascratch,UgyWF5F79LAvtYiWQoJ4AaABAg,2,TDwy1lSjEZo,0,1,2021-02-01T03:32:42Z,"@@everettwitt9464 I would love to do a video on pySpark, HIVE/snowflake, but it&#39;s hard to get access to those tools and the massive datasets mainly because they are industry grade. I can&#39;t use my work resources for YT videos. For hobbyists, pandas is probably fine. Otherwise, if you do need a big compute resource, your job would be able to train you on those tools. I was trained on HIVe and pySpark because my job required it. But I interviewed on basic SQL and python questions.<br><br>That&#39;s a good question about data quality. You would think there are automated tests for sampling the data, testing for normality, etc but I haven&#39;t encountered one. It seems like they expect all data scientists to perform those data tests because they start building the model. But those tests do take a while to perform so automated tests are a good idea. Something to consider if you&#39;re interested in providing a service for other data scientists =)<br><br>I&#39;ll keep trying out new ideas and seeing what sticks. It&#39;s definitely hard when I don&#39;t have access to industry grade tools and data.",2021-02-01T03:32:42Z
UgxAiT173pt9Q-pcjf94AaABAg,@mdabulkalamazad6775,,1,TDwy1lSjEZo,1,2,2020-11-29T13:31:35Z,Love you teaching style.,2020-11-29T13:31:35Z
UgxAiT173pt9Q-pcjf94AaABAg.9GdnZ5YtVcr9Ge6DIxjjVW,@stratascratch,UgxAiT173pt9Q-pcjf94AaABAg,2,TDwy1lSjEZo,0,0,2020-11-29T16:23:22Z,Thanks so much. Glad you like it!,2020-11-29T16:23:22Z
UgxFGDeyolU9uuXKXtl4AaABAg,@mysteriousbd3743,,1,TDwy1lSjEZo,1,2,2020-11-27T09:23:24Z,Awesome... Looking for a while for such code. Thanks for sharing it,2020-11-27T09:23:24Z
UgxFGDeyolU9uuXKXtl4AaABAg.9GZCZans7f39GZw_OQMgPQ,@stratascratch,UgxFGDeyolU9uuXKXtl4AaABAg,2,TDwy1lSjEZo,0,0,2020-11-27T16:14:11Z,Thanks! Stay tuned for part 3 in this series where I turn the code into functions so the entire script is cleaner and can be used for other projects.,2020-11-27T16:14:11Z
Ugx5Fqa_hwJ0KsfsFuR4AaABAg,@nargisparvin4267,,1,TDwy1lSjEZo,1,2,2020-11-27T05:52:58Z,"Excellent work sir - you are making python so easy,great job and I respect for your work.",2020-11-27T05:52:58Z
Ugx5Fqa_hwJ0KsfsFuR4AaABAg.9GYpUPoHrQT9GZwbFb_nPa,@stratascratch,Ugx5Fqa_hwJ0KsfsFuR4AaABAg,2,TDwy1lSjEZo,0,0,2020-11-27T16:14:27Z,Thanks! Let me know if there&#39;s other topics you want to see.,2020-11-27T16:14:27Z
UgxwU9b7vP5ksr0n7bJ4AaABAg,@culpgrant21,,1,TDwy1lSjEZo,1,1,2020-11-27T02:24:45Z,Good video! Just a preference on using .format vs. f strings?,2020-11-27T02:24:45Z
UgxwU9b7vP5ksr0n7bJ4AaABAg.9GYSeKEEEHD9GZxJXcipHI,@stratascratch,UgxwU9b7vP5ksr0n7bJ4AaABAg,2,TDwy1lSjEZo,0,2,2020-11-27T16:20:38Z,"I think you can use .format or f strings when you&#39;re just trying to print something. F strings have gotten more popular over the years so I&#39;m starting to see more of that. But when you&#39;re passing SQL code to the server/database, you should use parameterized queries which is similar to F strings (but not quite the same). A parameterized query will pass the SQL statement to the db and then pass the values, so it&#39;s a 2 step process. If you used .format to pass SQL queries to the db, it passes the entire query in 1 step, which can make it vulnerable to SQL injection attacks. <br><br>Here&#39;s more info on it: <a href=""https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.execute"">https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.execute</a><br><br>I don&#39;t explain this in this video but it&#39;s good to know if you&#39;re going to build anything that would go into production.",2020-11-27T16:21:31Z
Ugz8OCPWuGL0TBBtowJ4AaABAg,@bitlinktv8204,,1,TDwy1lSjEZo,0,0,2020-11-24T17:47:07Z,"Your content is mind-blowing Nate at StrataScratch,  Join us @ <a href=""http://www.bitlink.tv/"">www.bitlink.tv</a> to claim your prize! we are a CryptoFriendly community",2020-11-24T17:47:07Z
Ugyg9rMzAdy3KL-bU-V4AaABAg,@stratascratch,,1,wqBFgaMgFQA,0,10,2020-11-24T16:37:40Z,"Part 2 where I fully automate the notebook is here <a href=""https://youtu.be/TDwy1lSjEZo"">https://youtu.be/TDwy1lSjEZo</a>",2020-11-24T16:37:40Z
UgyFzM8lbqavWb0xKLl4AaABAg,@mariusbotezatu6365,,1,wqBFgaMgFQA,1,0,2023-05-31T12:32:56Z,Try the same thing with 1M rows in csv file :),2023-05-31T12:32:56Z
UgyFzM8lbqavWb0xKLl4AaABAg.9qNabRh-HYv9qPD5bRcQsR,@stratascratch,UgyFzM8lbqavWb0xKLl4AaABAg,2,wqBFgaMgFQA,0,1,2023-06-01T03:37:13Z,It&#39;ll break! You&#39;ll need to import in batches. I think there&#39;s a parameter for batching in one of the import functions in python. =),2023-06-01T03:37:13Z
UgxjdM4zuHv3O6Lv19t4AaABAg,@brentsimpson3791,,1,wqBFgaMgFQA,0,0,2023-05-16T00:45:17Z,Mate! I&#39;m a newbie python &#39;programmer&#39; and this video has to be one of the most useful practical examples I&#39;ve found. It&#39;ll take me a while to absorb it all but THANK YOU!üòä<br>Brent,2023-05-16T00:45:17Z
Ugye5CVph8gvKbMFYRh4AaABAg,@cristiansoto7581,,1,wqBFgaMgFQA,0,0,2023-05-05T20:28:53Z,"Hi Nate, thank you for your this valiable information. Could you make more videos about this? <br><br>I&#39;m suscribed in stratascratch an it&#39;s amazing.",2023-05-05T20:28:53Z
Ugzp2VWy9W71AzuJqoJ4AaABAg,@arpitakar3384,,1,wqBFgaMgFQA,0,1,2023-03-29T10:19:10Z,I loved it man thAt&quot;s a serious effort vro,2023-03-29T10:19:10Z
UgzYHhpn_M4v7ZPoySB4AaABAg,@hameedmulani21,,1,wqBFgaMgFQA,0,2,2023-03-10T07:25:10Z,One of the best tutorial I have ever seen,2023-03-10T07:25:10Z
Ugy5zueKjemWwZEatih4AaABAg,@lettalkaboutit,,1,wqBFgaMgFQA,0,1,2023-03-01T09:34:55Z,"Your video is fantastic, it help me, but this is  where I struggle and  I don&#39;t know if you can help with the following: my entire  csv file needs to go in the table results,  the dataframe created from the csv file have columns that contain foreign keys, I mean the csv files is a mix of different tables values, each column of my dataframe  represents a table, My use case is that: 1/ first step:  I have to loop through each column and compare the value in the column with the value from the corresponding table in the database, if the value does not exist then I create a SQL insert query to add this new value. After getting all new values in the databases then I do step 2<br><br>2/ Step : I get the value from each corresponding table from the database and for each columns I have to replace the data in each column by the table ID (foreign key) , and next send all the dataframe in the database<br>Can you help me achieve this please?",2023-03-01T09:39:36Z
Ugxb22MV5tE1sXVmuvx4AaABAg,@prabirkumarsahoo6368,,1,wqBFgaMgFQA,0,0,2023-02-08T19:48:51Z,Could you tell me from where you copied the &#39;AWS connection string&#39;????,2023-02-08T19:48:51Z
UgxFOhjs7fiVdDPGci54AaABAg,@AndyLeal-pz7xw,,1,wqBFgaMgFQA,2,0,2023-01-26T21:46:12Z,"This is incredibly helpful. I have a question, I&#39;m using paramiko to connect to a SFTP to read csv files from there, how would I go about setting up the cwd to the sftp server? I can only navigate to local directories currently.",2023-01-26T21:46:12Z
UgxFOhjs7fiVdDPGci54AaABAg.9lMiZJLaY6B9lMjM8wFUWW,@stratascratch,UgxFOhjs7fiVdDPGci54AaABAg,2,wqBFgaMgFQA,0,0,2023-01-26T21:53:08Z,"Thanks for the kind words! Is there a way to navigate to the directory of the stfp server? It might be a permissions issue that you can have your IT dept resolve. If they are able to allow you to access the STFP server via your script, then you&#39;re all set. Otherwise, you&#39;ll need to manually move the files.",2023-01-26T21:53:08Z
UgxFOhjs7fiVdDPGci54AaABAg.9lMiZJLaY6B9lOeDmH1Q5k,@AndyLeal-pz7xw,UgxFOhjs7fiVdDPGci54AaABAg,2,wqBFgaMgFQA,0,0,2023-01-27T15:46:47Z,@@stratascratch Thanks for the quick feedback! I checked permissions and everything seemed fine. I ended up switching to pysftp and it worked on the first try! keep up the great work!,2023-01-27T15:46:47Z
UgziCAmZgHvkxj7efSx4AaABAg,@StefanoVerugi,,1,wqBFgaMgFQA,2,2,2023-01-23T07:44:12Z,"As a beginner I find this video useful and well explained, thanks for posting it<br>To remove non alphanumerical characters can&#39;t you use re.sub from re? <br>like: print(re.sub(&quot;[^a-zA-Z.0-9]+&quot;,&quot;&quot;,string))<br>more or less you have the same result, anything wrong with this method? <br>cheers",2023-01-23T07:44:12Z
UgziCAmZgHvkxj7efSx4AaABAg.9lDUogf9NC29lENiELwg4y,@stratascratch,UgziCAmZgHvkxj7efSx4AaABAg,2,wqBFgaMgFQA,0,2,2023-01-23T16:01:23Z,"Thanks for the kind words. Yes, you can use re.sub just fine!",2023-01-23T16:01:23Z
UgziCAmZgHvkxj7efSx4AaABAg.9lDUogf9NC29t9VXIHoa9J,@cubano100pct,UgziCAmZgHvkxj7efSx4AaABAg,2,wqBFgaMgFQA,0,0,2023-08-08T14:44:37Z,"@@stratascratch There is the issue of changing the whitespaces into one _ character, then removing all other non a-z and 0-9 characters and _<br>tablename = re.sub(&#39;[\x00-\x1F\s\-]+&#39;, &#39;_&#39;, filename.lower(), 0)<br>tablename = re.sub(&#39;[^0-9a-z_]*&#39;,&#39;&#39;,tablename, 0)",2023-08-08T14:44:37Z
UgxQuzaCZUJQcjMirkp4AaABAg,@luzestrada9536,,1,wqBFgaMgFQA,0,1,2023-01-16T17:12:20Z,You are the best! üíØ Thank you so much for sharing this valuable information üëè,2023-01-16T17:12:20Z
UgzDRLaYKyjkEk8v5KB4AaABAg,@rashadnelson1873,,1,wqBFgaMgFQA,0,1,2022-12-02T18:30:51Z,"Thanks again for the tutorial:<br><br>I ran this code:<br><br># Upload to database<br><br>SQL_STATEMENT = &quot;&quot;&quot;<br>COPY fast_food_data FROM STDIN WITH<br>    CSV<br>    HEADER<br>    DELIMITER AS &#39;,&#39;<br>&quot;&quot;&quot;<br><br>cursor.copy_expert(sql=SQL_STATEMENT, file=my_file)<br>print(&#39;File copied to database&#39;)<br><br><br>I received this error:<br><br>InFailedSqlTransaction                    Traceback (most recent call last)<br>Input In [34], in &lt;cell line: 10&gt;()<br>      1 # Upload to database<br>      3 SQL_STATEMENT = &quot;&quot;&quot;<br>      4 COPY fast_food_data FROM STDIN WITH<br>      5     CSV<br>      6     HEADER<br>      7     DELIMITER AS &#39;,&#39;<br>      8 &quot;&quot;&quot;<br>---&gt; 10 cursor.copy_expert(sql=SQL_STATEMENT, file=my_file)<br>     11 print(&#39;File copied to database&#39;)<br><br>InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block<br><br><br>Currently stuck.  Bummer.  I&#39;ll keep working it.",2022-12-02T18:30:51Z
UgwXPl6LyJUOHKy3zUF4AaABAg,@rashadnelson1873,,1,wqBFgaMgFQA,0,2,2022-12-01T17:52:13Z,"Good video.  I&#39;m presently looking for something that will process a massive dataset with thousands of rows of data, then upload that data into a database.  I&#39;ve yet to run across a tutorial that processes and uploads a massive dataset into a database.  I&#39;m running into some interesting errors.  <br><br>1st Code Attempt:  df = pd.read_csv(&#39;fast-food-data-2020.csv&#39;)<br>1st Error:  UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe9 in position 20: invalid continuation byte<br><br><br>2nd Code Attempt:  df = pd.read_csv(&#39;fast-food-data-2020.csv&#39;, encoding=&#39;UTF-8&#39;)<br>2nd Error:  UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe9 in position 52676: invalid continuation byte<br><br><br>3rd Code Attempt:  df = pd.read_csv(&#39;fast-food-data-2020.csv&#39;, encoding=&#39;latin-1&#39;)<br>3rd Error:  DtypeWarning: Columns (12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,37,49,52) have mixed types. Specify dtype option on import or set low_memory=False.<br><br>  df = pd.read_csv(&#39;fast-food-data-2020.csv&#39;, encoding=&#39;latin-1&#39;)<br><br><br>4th Code Attempt:  df = pd.read_csv(&#39;fast-food-data-2020.csv&#39;, error_bad_lines=False, index_col=False, dtype=&#39;unicode&#39;)<br>4th Error:  UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe9 in position 20: invalid continuation byte<br><br><br>5th Code Attempt:  df = pd.read_csv(&#39;fast-food-data-2020.csv&#39;, low_memory=False)<br>5th Error:  UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe9 in position 20: invalid continuation byte<br><br><br>6th Code Attempt:  df = pd.read_csv(&#39;fast-food-data-2020.csv&#39;, engine=&#39;python&#39;)<br>6th Code Error:  UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe9 in position 3524: invalid continuation byte",2022-12-01T17:52:13Z
UgyGmzUjfti3ntj-QaB4AaABAg,@CodingNinja-lt8ge,,1,wqBFgaMgFQA,1,0,2022-11-12T23:42:11Z,"Hi Nate,<br>Thanks for the wonderful explanation. I tried to create the SQL table with your data and code using jupyter notebook. The code works fine but it failed to create the table in Postgres database. I am wondering what could be the reason.  In the Pgadmin  I found this under SQL TAB. Would appreciate your hints to help me figure out why I am not getting the desired result although code works <br><br>Database: aw<br><br>-- DROP DATABASE IF EXISTS aw;<br><br>CREATE DATABASE aw<br>    WITH<br>    OWNER = postgres<br>    ENCODING = &#39;UTF8&#39;<br>    LC_COLLATE = &#39;English_Finland.1252&#39;<br>    LC_CTYPE = &#39;English_Finland.1252&#39;<br>    TABLESPACE = pg_default<br>    CONNECTION LIMIT = -1<br>    IS_TEMPLATE = False;",2022-11-12T23:42:11Z
UgyGmzUjfti3ntj-QaB4AaABAg.9iLoDECJh8x9iOIfywKlqc,@stratascratch,UgyGmzUjfti3ntj-QaB4AaABAg,2,wqBFgaMgFQA,0,0,2022-11-13T22:55:37Z,Maybe you need to commit the code to the db? It&#39;s a postgres thing to do that.,2022-11-13T22:55:37Z
UgxT_LG5XXr2DTVxzY54AaABAg,@adithya5604,,1,wqBFgaMgFQA,1,1,2022-09-28T05:52:14Z,Thank you so much üéâ,2022-09-28T05:52:14Z
UgxT_LG5XXr2DTVxzY54AaABAg.9gW1-IZkuA-9gWh3LJNZue,@stratascratch,UgxT_LG5XXr2DTVxzY54AaABAg,2,wqBFgaMgFQA,0,0,2022-09-28T12:08:32Z,You&#39;re welcome.,2022-09-28T12:08:32Z
UgxgFdRP2-zxxGoDpup4AaABAg,@chyang0107,,1,wqBFgaMgFQA,0,0,2022-09-21T14:01:34Z,"Hello Nate, I appreciate your valuable work! Could you provide your insights on these two questions, First, what is &#39;STDIN&#39; within the SQL Statement  &#39;&quot;&quot;&quot;Copy Customer_contract from STDIN With ....&quot;&quot;&quot; around (<a href=""https://www.youtube.com/watch?v=wqBFgaMgFQA&amp;t=28m51s"">28:51</a>)? <br><br>Second, if I would like to insert value to table from Excel file, My SQL Statement would be<br> &quot;&quot;&quot;&quot; <br>Copy Customer_contract from STDIN With<br>Excel<br>Header<br>&quot;&quot;&quot;&quot;<br><br>Am I on the right track?<br><br>Thanks,<br>Hank",2022-09-21T14:01:34Z
Ugx3p7PbG7AcC2juwzx4AaABAg,@sai6837,,1,wqBFgaMgFQA,2,1,2022-09-15T17:46:43Z,"Hai, I am using mysql...what keyword should i use instead of  copy_expert",2022-09-15T17:46:43Z
Ugx3p7PbG7AcC2juwzx4AaABAg.9g-pQtg-Wb89gtFKz0ZIg5,@stratascratch,Ugx3p7PbG7AcC2juwzx4AaABAg,2,wqBFgaMgFQA,0,0,2022-10-07T15:39:18Z,"I don&#39;t use MySQL that often in these situations, but shouldn&#39;t execute() or executemany() work?",2022-10-07T15:39:18Z
Ugx3p7PbG7AcC2juwzx4AaABAg.9g-pQtg-Wb89gtGEgiweY4,@sai6837,Ugx3p7PbG7AcC2juwzx4AaABAg,2,wqBFgaMgFQA,0,1,2022-10-07T15:47:11Z,@@stratascratch no worries I tried and it worked,2022-10-07T15:47:11Z
UgzlYylZcCFcd9zl4lt4AaABAg,@alanamantinodasilva4362,,1,wqBFgaMgFQA,1,2,2022-09-13T00:52:16Z,Awesome. Thank you for this content.,2022-09-13T00:52:16Z
UgzlYylZcCFcd9zl4lt4AaABAg.9ftrk7FFXa_9fvBpGYUmy-,@stratascratch,UgzlYylZcCFcd9zl4lt4AaABAg,2,wqBFgaMgFQA,0,1,2022-09-13T13:15:42Z,You&#39;re welcome.  Glad you liked it.,2022-09-13T13:15:42Z
UgzVKD2JhoFKfKPlchd4AaABAg,@full_bladder,,1,wqBFgaMgFQA,0,0,2022-09-08T15:29:56Z,"You did not block your password. If it was an important db or a password used before for other accounts, change it.",2022-09-08T15:29:56Z
UgyuRFILpWW4fT7l5Yt4AaABAg,@sd0n0158,,1,wqBFgaMgFQA,1,1,2022-08-19T01:03:36Z,Do you know if there is a work around for Redshift without using an S3 bucket? It doesn&#39;t allow COPY FROM STDIN.,2022-08-19T01:03:36Z
UgyuRFILpWW4fT7l5Yt4AaABAg.9etWAH2Ps4-9fezZ0WBQsG,@stratascratch,UgyuRFILpWW4fT7l5Yt4AaABAg,2,wqBFgaMgFQA,0,0,2022-09-07T06:11:55Z,"No, I don&#39;t know about such workaround. Have you tried asking on re: Post or Stack Overflow?",2022-09-07T06:11:55Z
Ugx9ERxboLVw-VRjkvd4AaABAg,@komaldiwe5667,,1,wqBFgaMgFQA,1,1,2022-08-11T11:02:30Z,"hi Nate, how can we do cursor.copy_expert(sql= SQL_statement, file = my_file) for MySQL ???",2022-08-11T11:02:30Z
Ugx9ERxboLVw-VRjkvd4AaABAg.9e_zLgR115O9fezzFj03Pi,@stratascratch,Ugx9ERxboLVw-VRjkvd4AaABAg,2,wqBFgaMgFQA,0,0,2022-09-07T06:15:38Z,Have you tried MySQL Connector/Python?,2022-09-07T06:15:38Z
Ugx_vvMgJF7oIuaaMrh4AaABAg,@darshpancholi4919,,1,wqBFgaMgFQA,0,0,2022-07-30T21:35:49Z,I use a macbook M1 and I am unable to import the psycopg2 library in my jupyter notebook. have tried almost all the solutions provided on youtube and StackOverflow. Can someone help me out with the same?,2022-07-30T21:35:49Z
UgxyWBbCOo1nCtvlAtl4AaABAg,@arpittrivedi6636,,1,wqBFgaMgFQA,1,1,2022-07-25T18:23:21Z,what about excel,2022-07-25T18:23:21Z
UgxyWBbCOo1nCtvlAtl4AaABAg.9dv-HmjFlfB9dwE8tmbdhW,@stratascratch,UgxyWBbCOo1nCtvlAtl4AaABAg,2,wqBFgaMgFQA,0,0,2022-07-26T05:52:27Z,"That can also work.  But for now, we can use Python.",2022-07-26T05:52:27Z
UgySY7CYr4rI5Mpg45F4AaABAg,@junaidmalik9593,,1,wqBFgaMgFQA,1,0,2022-07-22T18:17:39Z,"Fantastic Video , thanks Nate, u r awesome .<br>i just struggled understanding the col_str part at <a href=""https://www.youtube.com/watch?v=wqBFgaMgFQA&amp;t=17m50s"">17:50</a> of the Video , may be coz i am new to this!",2022-07-22T18:17:39Z
UgySY7CYr4rI5Mpg45F4AaABAg.9dnGFIYlo8S9dqpWdf7OAQ,@stratascratch,UgySY7CYr4rI5Mpg45F4AaABAg,2,wqBFgaMgFQA,0,0,2022-07-24T03:32:18Z,Glad it helped!,2022-07-24T03:32:18Z
UgyeXbDT3oOtWmUk_w14AaABAg,@Jay-eh5rw,,1,wqBFgaMgFQA,1,0,2022-07-15T06:34:18Z,Thank you very much. Great help:),2022-07-15T06:34:18Z
UgyeXbDT3oOtWmUk_w14AaABAg.9dUzBhgrOUl9dWZRZfjJDN,@stratascratch,UgyeXbDT3oOtWmUk_w14AaABAg,2,wqBFgaMgFQA,0,0,2022-07-15T21:19:01Z,You&#39;re welcome!,2022-07-15T21:19:01Z
Ugw2KhsxPzYjjNpPG7t4AaABAg,@jerrywang1550,,1,wqBFgaMgFQA,3,1,2022-07-09T00:43:55Z,connect to the AWS part is difficult,2022-07-09T00:43:55Z
Ugw2KhsxPzYjjNpPG7t4AaABAg.9dEuKAmfYas9dQvX9YMOhg,@stratascratch,Ugw2KhsxPzYjjNpPG7t4AaABAg,2,wqBFgaMgFQA,0,0,2022-07-13T16:45:18Z,"I hear you, could be tricky.",2022-07-13T16:45:18Z
Ugw2KhsxPzYjjNpPG7t4AaABAg.9dEuKAmfYas9dw5KxlHyjk,@prome4985,Ugw2KhsxPzYjjNpPG7t4AaABAg,2,wqBFgaMgFQA,0,0,2022-07-26T04:35:27Z,Yeah. I&#39;ve been trying for 2 days. Is there any solution for that? The error: could not connect to server: Connection timed out.,2022-07-26T04:35:27Z
Ugw2KhsxPzYjjNpPG7t4AaABAg.9dEuKAmfYas9ebg-37xv7q,@prome4985,Ugw2KhsxPzYjjNpPG7t4AaABAg,2,wqBFgaMgFQA,0,0,2022-08-12T02:51:52Z,@Andrew Levinton Yeah I did....I don&#39;t know what else to do. Thanks for your replyüôè,2023-11-16T20:07:02Z
UgytOftJjcmojcyBLT94AaABAg,@sergeitokarev6033,,1,wqBFgaMgFQA,2,0,2022-07-05T04:28:51Z,"If someone has problems with connecting to the database, just edit your VPC Security Group and allow all IPs. It helped in my case.",2022-07-05T04:28:51Z
UgytOftJjcmojcyBLT94AaABAg.9d5-t9oqQI69dQvXRGxKvE,@stratascratch,UgytOftJjcmojcyBLT94AaABAg,2,wqBFgaMgFQA,0,0,2022-07-13T16:45:21Z,Wonderful suggestion!,2022-07-13T16:45:21Z
UgytOftJjcmojcyBLT94AaABAg.9d5-t9oqQI69dr6EJ_4UOx,@prome4985,UgytOftJjcmojcyBLT94AaABAg,2,wqBFgaMgFQA,0,0,2022-07-24T06:07:05Z,Yea. Please can you help? How do I edit it? I&#39;ve been trying since yesterday. Thanks,2022-07-24T06:07:05Z
UgxnXCEW5oM58hYCIXN4AaABAg,@mickyelmb,,1,wqBFgaMgFQA,0,2,2022-06-29T03:09:49Z,Excellent tutorial. I was able to follow along in Jupyter. with no issues. Thank you for posting it.,2022-06-29T03:09:49Z
UgxTzO4V0ItborUHAYJ4AaABAg,@yassw1995,,1,wqBFgaMgFQA,0,1,2022-06-11T04:20:39Z,"Hi Nate, this video is amazing and concise, thank you so much. However, I&#39;m trying to learn this using as few advanced python libraries as possible for a class, so I can&#39;t use pandas or the like. I&#39;m limiting myself to mySQL.connector and csv libraries. For this reason, MySQLCursor does not accept the &quot;cursor.copy_expert&quot; syntax. What would be a good alternative method to insert the contents of the csv file we saved during that step into the sql database?",2022-06-11T04:20:39Z
Ugwm8U0j7FyHl6pl29J4AaABAg,@gambu4810,,1,wqBFgaMgFQA,0,1,2022-06-11T02:00:43Z,"Hi Nate. Your work is so good, I can&#39;t believe my eyes. Can you please let me know how to make a connection to a SQL management studio instead? This can really help if I knew how to do it in SQL server management studio instead of postgres.",2022-06-11T02:00:43Z
UgygYxsaBhC6HEGt6254AaABAg,@gregNFL,,1,wqBFgaMgFQA,0,0,2022-05-31T05:29:14Z,"10/10 Clear, concise.",2022-05-31T05:29:14Z
UgyGatmLBnHbTIQtvyN4AaABAg,@radhakrishnanaik5656,,1,wqBFgaMgFQA,0,0,2022-05-21T00:11:23Z,"Hey Nate, your way of explanation is really very good .I would like to request you to prepare a video to fetch database table in csv file in python code in the form of pipeline",2022-05-21T00:11:23Z
UgwAIphSRPNNzCuniIR4AaABAg,@BelowIce,,1,wqBFgaMgFQA,1,0,2022-05-18T07:20:13Z,"Love this video. Can you share why you copy the dataframe to a csv, and then import the csv to the database? Is it possible to directly reference the values in the dataframe and use those to populate the insert statements?",2022-05-18T07:20:13Z
UgwAIphSRPNNzCuniIR4AaABAg.9b9iLEcuWiz9bBwiNMRIOh,@stratascratch,UgwAIphSRPNNzCuniIR4AaABAg,2,wqBFgaMgFQA,0,1,2022-05-19T04:04:19Z,You can definitely just upload the df to the database. I used a function that required a csv to import to a db. But there are other approaches like you mentioned that would likely work as well.,2022-05-19T04:04:19Z
UgxforsP1D4w9IFdNWx4AaABAg,@Davidkiania,,1,wqBFgaMgFQA,0,0,2022-04-19T15:02:40Z,This is really good thanks. You‚Äôve saved me lots of manual hours.,2022-04-19T15:02:40Z
Ugx5bClHAc9LCXVnr-p4AaABAg,@austincelestine1925,,1,wqBFgaMgFQA,1,1,2022-04-14T14:34:14Z,"Holy cow, I wish I came across this earlier. This helped alot to importing these csv files into mysql. The 2nd video was great to except my formatting was different for my job. Great work!!! Absolutely love your content it really helps!",2022-04-14T14:34:14Z
Ugx5bClHAc9LCXVnr-p4AaABAg.9_nwzOyUpoY9_o0IxFVxu1,@stratascratch,Ugx5bClHAc9LCXVnr-p4AaABAg,2,wqBFgaMgFQA,0,1,2022-04-14T15:12:00Z,Thank you and glad you found this vid useful!,2022-04-14T15:12:00Z
UgxOTjePhvsuL6fKl7p4AaABAg,@thelatecomer4245,,1,wqBFgaMgFQA,0,0,2022-04-12T18:36:57Z,"Hey, Can you give a solution on exporting table from sql server into local as multi character delimited file?",2022-04-12T18:36:57Z
UgwnfG9Wlw2WYXeOxSt4AaABAg,@supriya1316,,1,wqBFgaMgFQA,0,0,2022-02-18T08:41:19Z,"Hello Nate, Could you please help me to resolve the below error <br><br>---------------------------------------------------------------------------<br>InFailedSqlTransaction                    Traceback (most recent call last)<br>~\AppData\Local\Temp/ipykernel_5072/<a href=""http://3532242763.py/"">3532242763.py</a> in &lt;module&gt;<br>----&gt; 1 cursor.copy_expert(sql=SQL_STATEMENT, file=my_file)<br>      2 print(&#39;file copied to db &#39;)<br><br>InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block    <br><br>for the code :<br>cursor.copy_expert(sql=SQL_STATEMENT, file=my_file)<br>print(&#39;file copied to db &#39;)",2022-02-18T08:41:49Z
UgwUwo5IW-cNeybszqF4AaABAg,@zynkers401,,1,wqBFgaMgFQA,0,0,2021-11-06T15:39:23Z,These series are the best! Thanks Nate,2021-11-06T15:39:23Z
UgzqqBtYmaCWggPBk8l4AaABAg,@huseyindansman7415,,1,wqBFgaMgFQA,2,0,2021-11-06T14:37:07Z,"its really so slow to insert a  lot big data into ms sql server db using pyodbc library, any idea you have might be really useful for me to do it faster.",2021-11-06T14:37:07Z
UgzqqBtYmaCWggPBk8l4AaABAg.9UPXvKpmZ0a9UQ23tLwYEJ,@stratascratch,UgzqqBtYmaCWggPBk8l4AaABAg,2,wqBFgaMgFQA,0,0,2021-11-06T19:18:02Z,"I feel like it&#39;s always a slow process no matter what db engine. There are engines that allow you to multi-thread the processes but otherwise, batch processing is the only thing that comes to mind. There&#39;s usually a parameter setting for batch loading in whatever python library you&#39;re using to import your data",2021-11-06T19:18:02Z
UgzqqBtYmaCWggPBk8l4AaABAg.9UPXvKpmZ0a9UQ4hE5-dMX,@huseyindansman7415,UgzqqBtYmaCWggPBk8l4AaABAg,2,wqBFgaMgFQA,0,0,2021-11-06T19:41:01Z,@@stratascratch thank you so much for your reply but I found out a way to get it faster using a different parameter being able to insert 16 thousand rows withing 20 or 30 seconds,2021-11-06T19:41:01Z
Ugz70iOFK9Mmvl5Hs0N4AaABAg,@fealgu100,,1,wqBFgaMgFQA,1,0,2021-10-21T09:18:39Z,"Great video. (<a href=""https://www.youtube.com/watch?v=wqBFgaMgFQA&amp;t=11m35s"">11:35</a>),  If you don&#39;t want to use &#39;underscores&#39;, and have your column names read like this:<br> customername	startdate	enddate	contractamountm	invoicesent	paid<br>you can use this line of code:<br><br>df.columns= df.columns.str.replace(r&#39;([^A-Za-z0-9,&#39;&#39;])&#39;,&#39;&#39;).str.lower()",2021-10-21T09:18:39Z
Ugz70iOFK9Mmvl5Hs0N4AaABAg.9TklksESupl9TltXeTqWr3,@stratascratch,Ugz70iOFK9Mmvl5Hs0N4AaABAg,2,wqBFgaMgFQA,0,0,2021-10-21T19:45:51Z,Thank you for the tip!,2021-10-21T19:45:51Z
UgwmGmgWHYjLQn73rKB4AaABAg,@martithon,,1,wqBFgaMgFQA,0,0,2021-10-04T07:35:20Z,"[SOLVED] at @16.10 I&#39;m getting a &quot; &#39;, &#39; KeyError &quot; and have no idea how to solve it. Any tips?<br><br>edit: misplaced the closing bracket of the join function",2021-10-04T07:39:58Z
UgzhPgoQWUBRI-ZqAsh4AaABAg,@sasongkosan8832,,1,wqBFgaMgFQA,1,0,2021-09-28T05:32:09Z,"Very Good ,, :) Nice tutorial ...",2021-09-28T05:32:09Z
UgzhPgoQWUBRI-ZqAsh4AaABAg.9Sp8YpYDbyh9SqDOih4AqG,@stratascratch,UgzhPgoQWUBRI-ZqAsh4AaABAg,2,wqBFgaMgFQA,0,0,2021-09-28T15:33:42Z,Thanks for watching!,2021-09-28T15:33:42Z
Ugx384jhK3An-Xt9VEJ4AaABAg,@limichelle6895,,1,wqBFgaMgFQA,4,0,2021-08-27T09:02:21Z,"Hi,  I have a question. How to only upload &quot;newest csv data&quot; in a folder? Thanks",2021-08-27T09:02:21Z
Ugx384jhK3An-Xt9VEJ4AaABAg.9RY7AJg8ewA9RZ36AnOoZN,@stratascratch,Ugx384jhK3An-Xt9VEJ4AaABAg,2,wqBFgaMgFQA,0,0,2021-08-27T17:46:04Z,"Sorry, not sure what you mean?",2021-08-27T17:46:04Z
Ugx384jhK3An-Xt9VEJ4AaABAg.9RY7AJg8ewA9RZ3VaqAHMW,@limichelle6895,Ugx384jhK3An-Xt9VEJ4AaABAg,2,wqBFgaMgFQA,0,0,2021-08-27T17:49:32Z,"@@stratascratch hi , thanks for getting back to me. So I‚Äôve like a folder with a new csv data every single day. I would like to only update the newest data to MySQL. Can help me ? Thanks",2021-08-27T17:49:32Z
Ugx384jhK3An-Xt9VEJ4AaABAg.9RY7AJg8ewA9RZ3zP6HzqV,@stratascratch,Ugx384jhK3An-Xt9VEJ4AaABAg,2,wqBFgaMgFQA,0,1,2021-08-27T17:53:45Z,"@@limichelle6895 You can just upload it to colabs ever day. That&#39;s the manual way. Otherwise, you could automate it by specifying the name of the CSV file in your code. If there&#39;s a new csv file everyday, I would make sure that csv file has the current date at the end of the file name. Then have your code find the file name with the current date. The script will find the file and import it.",2021-08-27T17:53:45Z
Ugx384jhK3An-Xt9VEJ4AaABAg.9RY7AJg8ewA9RZ41dP6fUx,@limichelle6895,Ugx384jhK3An-Xt9VEJ4AaABAg,2,wqBFgaMgFQA,0,0,2021-08-27T17:54:11Z,"And my csv data come with a new date everyday. For example 280721, 290721 and so on",2021-08-27T17:54:11Z
Ugwh09-uaCb5f61RqDl4AaABAg,@prateek2159,,1,wqBFgaMgFQA,0,9,2021-07-24T16:31:37Z,"Hey Nate, your videos are just too good. I love how your channel is so dedicated towards real word data science. By the way I noticed that you started a video series, &quot;For your Data Science Project&quot; and I really want you to continue making videos for this particular series because there&#39;s literally no one on YouTube with such guidance on DS projects and I have been looking for one since a very long time because I have my placements just after 12 months and I really want to make a full stack data science project. Thank you.",2021-07-24T16:31:37Z
UgzzoAn2zkLS8HPLY054AaABAg,@smsyrandomacts1725,,1,wqBFgaMgFQA,0,0,2021-07-08T00:34:35Z,Simply amazing.. knowledge transfer series. Great job Nat üëç,2021-07-08T00:34:35Z
Ugz1nEbuJzrIZaAkDsF4AaABAg,@sahej97,,1,wqBFgaMgFQA,1,6,2021-07-05T08:03:30Z,Love this series of real world on the job tasks,2021-07-05T08:03:30Z
Ugz1nEbuJzrIZaAkDsF4AaABAg.9PPYIsl_S749PRlPWohEr7,@stratascratch,Ugz1nEbuJzrIZaAkDsF4AaABAg,2,wqBFgaMgFQA,0,1,2021-07-06T04:45:13Z,Thank you!,2021-07-06T04:45:13Z
UgzUWuBaaQJV52v2EpJ4AaABAg,@its_me7363,,1,wqBFgaMgFQA,3,0,2021-04-25T12:12:59Z,"Thanks for these amazing helpful videos, something different from other youtubers. Can we use vscode to connect and open AWS database  and write queries? I don&#39;t want to download another software for database.",2021-04-25T12:12:59Z
UgzUWuBaaQJV52v2EpJ4AaABAg.9MZAQ1ogaEb9M_0rHy1BjB,@stratascratch,UgzUWuBaaQJV52v2EpJ4AaABAg,2,wqBFgaMgFQA,0,1,2021-04-25T20:08:41Z,"I think you can use vscode to connect to a db but I&#39;ve never used that platform before so I don&#39;t know how to. But seeing as how vscode is for developers, it&#39;s likely you can use it.",2021-04-25T20:08:41Z
UgzUWuBaaQJV52v2EpJ4AaABAg.9MZAQ1ogaEb9M_1dP8Hqmd,@its_me7363,UgzUWuBaaQJV52v2EpJ4AaABAg,2,wqBFgaMgFQA,0,0,2021-04-25T20:15:32Z,@@stratascratch I am trying to use an extension in vscode named SQLtools but it is not able to connect to AWS RDS for postgresql database i created as you showed.,2021-04-25T20:15:32Z
UgzUWuBaaQJV52v2EpJ4AaABAg.9MZAQ1ogaEb9Mb-HRsNcGE,@stratascratch,UgzUWuBaaQJV52v2EpJ4AaABAg,2,wqBFgaMgFQA,0,1,2021-04-26T14:33:24Z,@@its_me7363 You might need to find another platform that allows you to connect to an aws rds. Another reason could be that the database you created isn&#39;t configured right. There could be a firewall in place or ports closed. Try to see if you can connect to the rds through another way.,2021-04-26T14:33:24Z
Ugxbhb-X_vqFku2PoWZ4AaABAg,@MichelUNITED,,1,wqBFgaMgFQA,5,1,2021-04-06T21:41:17Z,"hello i ran this code::                                                                                                                                                        col_str = &quot;, &quot;.join(&quot;{} {}&quot; .format(n, d) for (n, d) in zip(dta.columns, dta.dtypes.replace(replacements)))<br>col_str              the error received was that i cannot compare types ndarray(dtype = objects) and str. please help",2021-04-06T21:41:17Z
Ugxbhb-X_vqFku2PoWZ4AaABAg.9LoGLmUWunG9LopM3w0UJG,@stratascratch,Ugxbhb-X_vqFku2PoWZ4AaABAg,2,wqBFgaMgFQA,0,0,2021-04-07T02:55:53Z,"Check out the notebook here: <a href=""https://github.com/Strata-Scratch/csv_to_db_automation"">https://github.com/Strata-Scratch/csv_to_db_automation</a> Hopefully that helps you troubleshoot.",2021-04-07T02:55:53Z
Ugxbhb-X_vqFku2PoWZ4AaABAg.9LoGLmUWunG9LopWJBiYz4,@stratascratch,Ugxbhb-X_vqFku2PoWZ4AaABAg,2,wqBFgaMgFQA,0,1,2021-04-07T02:57:17Z,But the error sounds like you have an object dtype which is expected but you&#39;re trying to compare that to a string. It leads me to believe that your &quot;col_str&quot; might be wrong? Or your zip() code? Hard to say without really seeing your code in its entirety.,2021-04-07T02:57:17Z
Ugxbhb-X_vqFku2PoWZ4AaABAg.9LoGLmUWunG9LpiU_Pl6yV,@MichelUNITED,Ugxbhb-X_vqFku2PoWZ4AaABAg,2,wqBFgaMgFQA,0,0,2021-04-07T11:15:07Z,"@@stratascratch Hello thank you for the reply. i did exactly what did. i followed your line one after the other. <br><br>datype_str = &quot;, &quot;.join(&quot;{} {}&quot; .format(n, d) for (n, d) in zip(dta.columns ,dta.dtypes.replace(replacements)))<br>this is the line. sorry but i don&#39;t see any kind of comparing going on.",2021-04-07T11:15:07Z
Ugxbhb-X_vqFku2PoWZ4AaABAg.9LoGLmUWunG9LqUdE4ESBE,@stratascratch,Ugxbhb-X_vqFku2PoWZ4AaABAg,2,wqBFgaMgFQA,0,0,2021-04-07T18:24:37Z,"@@MichelUNITED I believe there might be something wrong with the dta values or column headers. You should also check your replacements array too to make sure you&#39;re getting the same output as I did. See here for info <a href=""https://stackoverflow.com/questions/53478932/cannot-compare-types-ndarraydtype-int64-and-str"">https://stackoverflow.com/questions/53478932/cannot-compare-types-ndarraydtype-int64-and-str</a>",2021-04-07T18:24:37Z
Ugxbhb-X_vqFku2PoWZ4AaABAg.9LoGLmUWunG9MFLuiOwbnH,@MichelUNITED,Ugxbhb-X_vqFku2PoWZ4AaABAg,2,wqBFgaMgFQA,0,0,2021-04-17T19:28:37Z,@@stratascratch can i send you my code please. help me please. can i send you my code to have  a look at. i hope you see this. i am working on something related to your video.,2021-04-17T19:28:37Z
Ugwuj0Zc7VQepjH--0F4AaABAg,@TheSwil325,,1,wqBFgaMgFQA,1,0,2021-03-19T00:15:31Z,I really appreciate the quality of your videos! üíØ,2021-03-19T00:15:31Z
Ugwuj0Zc7VQepjH--0F4AaABAg.9L2btojTraw9L4Kt2Fnr71,@stratascratch,Ugwuj0Zc7VQepjH--0F4AaABAg,2,wqBFgaMgFQA,0,0,2021-03-19T16:16:36Z,Thanks so much! Will keep doing more!,2021-03-19T16:16:36Z
Ugxpa26m2N6cIwgQAit4AaABAg,@javierjdaza,,1,wqBFgaMgFQA,1,0,2021-02-10T19:09:52Z,"How The fuck this video only has +1000 views? you deserve more visibility man, u re a fucking Genius and great explaining the things. Go ahead, u got a new subs",2021-02-10T19:09:52Z
Ugxpa26m2N6cIwgQAit4AaABAg.9JaNIhlPMea9JarYLMQx8s,@stratascratch,Ugxpa26m2N6cIwgQAit4AaABAg,2,wqBFgaMgFQA,0,0,2021-02-10T23:42:53Z,"I guess not a lot of people like to see me code =) But, honestly, thanks for the kind words. I&#39;ll keep trying to code and provide thoughtful explanations to my solutions and approaches. This python series took a really long time and I had probably like 20 pages of notes that I covered across the 3 videos.",2021-02-10T23:42:53Z
Ugxs0z-rMVVAnajGKJd4AaABAg,@RedShipsofSpainAgain,,1,wqBFgaMgFQA,3,1,2020-12-20T13:27:54Z,"Great video Nate.  Few suggestions:  <br><a href=""https://www.youtube.com/watch?v=wqBFgaMgFQA&amp;t=6m58s"">6:58</a>  It is more clear to use a variable name like &quot;file_name&quot; rather than the more ambiguous variable name &quot;file&quot;<br><a href=""https://www.youtube.com/watch?v=wqBFgaMgFQA&amp;t=8m31s"">8:31</a>  To improve readability, consider putting each .replace() call on a separate line followed by the escape character.  And perhaps consider consolidating all these replace statements into a single replace() call, using, say, a regex.",2020-12-20T13:30:20Z
Ugxs0z-rMVVAnajGKJd4AaABAg.9HUrpXuT9-D9HVFCaRo89p,@stratascratch,Ugxs0z-rMVVAnajGKJd4AaABAg,2,wqBFgaMgFQA,0,1,2020-12-20T17:00:54Z,"Thanks so much for your input. I&#39;ll keep those in mind in my next python video. Keep the feedback coming. Also, if you have any requests on topics, I&#39;d be happy to try to cover them on this channel.",2020-12-20T17:00:54Z
Ugxs0z-rMVVAnajGKJd4AaABAg.9HUrpXuT9-D9HVUGovOdW7,@RedShipsofSpainAgain,Ugxs0z-rMVVAnajGKJd4AaABAg,2,wqBFgaMgFQA,0,1,2020-12-20T19:12:33Z,"@@stratascratch happy to help and offer constructive feedback!  Admittedly my advice is opinionated, but I&#39;ve found adapting certain details like these really help to prevent headaches for debugging and for future development of your code.  Sort of like a set of &quot;best practices&quot;.  Love your videos, btw. Keep up the great work Nate.",2020-12-20T19:12:33Z
Ugxs0z-rMVVAnajGKJd4AaABAg.9HUrpXuT9-D9HVi7zNX2Jk,@stratascratch,Ugxs0z-rMVVAnajGKJd4AaABAg,2,wqBFgaMgFQA,0,1,2020-12-20T21:22:25Z,@@RedShipsofSpainAgain Thanks so much for the kind words. I&#39;m hoping to get into more python code but it takes sooo long to record compared to SQL stuff. Hope to have another python series in Jan or Feb. Your &quot;best practices&quot; are great advice and I&#39;ll keep these in mind as I build my solutions out.,2020-12-20T21:22:25Z
UgxZFQqvm7CqmnOfca54AaABAg,@sureshkumarramachandran6993,,1,wqBFgaMgFQA,1,1,2020-12-14T23:52:42Z,Great job!,2020-12-14T23:52:42Z
UgxZFQqvm7CqmnOfca54AaABAg.9HGXZWvD5Yi9HGwiVw5bAn,@stratascratch,UgxZFQqvm7CqmnOfca54AaABAg,2,wqBFgaMgFQA,0,0,2020-12-15T03:41:16Z,Thank you. I&#39;m glad you liked it!,2020-12-15T03:41:16Z
UgzDao4EVCa4b9hyKch4AaABAg,@CawlCawl1,,1,wqBFgaMgFQA,1,2,2020-12-11T15:39:44Z,"Amazing video - thank you! I am using MS SQL SERVER  Management Studio, thus have to use PYODBC and get an error &quot;&#39;pyodbc.Cursor&#39; object has no attribute &#39;copy_expert&#39;&quot; in the line of code  &quot;cursor.copy_expert(SQL_STATEMENT, file=my_file)&quot;. Is there any alternative code I can use since Im using PYODBC?",2020-12-11T15:39:44Z
UgzDao4EVCa4b9hyKch4AaABAg.9H7vlI-23D19HAR6dSQYK1,@stratascratch,UgzDao4EVCa4b9hyKch4AaABAg,2,wqBFgaMgFQA,0,1,2020-12-12T15:00:53Z,"If you&#39;re going to use PYODBC, you&#39;ll probably need to INSERT your values row by row rather than using something like copy_expert() which takes the entire file and just does it INSERT all at once. Here&#39;s the documentation on INSERTs for PYODBC (<a href=""https://docs.microsoft.com/en-us/sql/connect/python/pyodbc/step-3-proof-of-concept-connecting-to-sql-using-pyodbc?view=sql-server-ver15)"">https://docs.microsoft.com/en-us/sql/connect/python/pyodbc/step-3-proof-of-concept-connecting-to-sql-using-pyodbc?view=sql-server-ver15)</a>. Every database is different and requires a different python library. Hope this helps.",2020-12-12T15:00:53Z
UgxdTnDkLkj2-zjf4BV4AaABAg,@exili,,1,wqBFgaMgFQA,1,1,2020-11-28T03:37:25Z,"Extremely helpful video, and a very useful tool , glad I found this on Reddit.",2020-11-28T03:37:25Z
UgyUpE6AHsUBwFAz94V4AaABAg,@gopalswami999,,1,WS0fM1agxTk,0,0,2023-10-01T19:26:33Z,Sir i am from india and wanted to subscribe yearly plan but platform isnt accepting visa card and there is also no option to pay through upi,2023-10-01T19:26:33Z
UgyFxNR0Mrh6DCAAJmV4AaABAg,@kadourkadouri3505,,1,WS0fM1agxTk,1,0,2023-09-25T21:14:33Z,"Great work as always! However, is the background music necessary?",2023-09-25T21:14:33Z
UgyFxNR0Mrh6DCAAJmV4AaABAg.9v5nJAa3emT9v62ROR0cTJ,@stratascratch,UgyFxNR0Mrh6DCAAJmV4AaABAg,2,WS0fM1agxTk,0,0,2023-09-25T23:35:28Z,It&#39;s not necessary but it seems good to keep the viewer&#39;s attention.,2023-09-25T23:35:28Z
UgyVmItSwakZSrT3vm94AaABAg,@Marytheanalyst,,1,WS0fM1agxTk,0,0,2023-09-24T07:50:06Z,Subqueries need Time,2023-09-24T07:50:13Z
UgwykwY0jTZUQMzXRBR4AaABAg,@yuthpatirathi2719,,1,rbRYsAgwPBA,0,0,2023-05-27T02:42:21Z,Thanks,2023-05-27T02:42:21Z
Ugx-vVR6AfOC33j-OjR4AaABAg,@ashhere7610,,1,rbRYsAgwPBA,0,1,2023-05-16T16:37:10Z,The question is not very clear.,2023-05-16T16:37:10Z
Ugyl8En-xRwxLHrJLBt4AaABAg,@redwannabil8031,,1,rbRYsAgwPBA,1,1,2023-04-24T01:19:45Z,Was it necessary to use the ifnull again in the outer query?,2023-04-24T01:26:47Z
Ugyl8En-xRwxLHrJLBt4AaABAg.9os79T1tRld9pmPkiG5S81,@ashhere7610,Ugyl8En-xRwxLHrJLBt4AaABAg,2,rbRYsAgwPBA,0,1,2023-05-16T16:38:12Z,"No, it was already made 0 in the second CTE",2023-05-16T16:38:12Z
Ugwbi12mRoHp76KUZfV4AaABAg,@uddipangoswami8427,,1,BgN5hpl3WKc,1,0,2023-02-17T13:45:04Z,for mysql can u share the functions,2023-02-17T13:45:04Z
Ugwbi12mRoHp76KUZfV4AaABAg.9mEVzaKVgjb9mI1zxtg8uJ,@stratascratch,Ugwbi12mRoHp76KUZfV4AaABAg,2,BgN5hpl3WKc,0,0,2023-02-18T22:39:56Z,basically all the functions can be used in mysql. There are some changes like ilike doesn&#39;t work in mysql. You&#39;ll need to use LIKE to do the same operation. So just google functions for mysql to figure it out.,2023-02-18T22:39:56Z
UgyEZqOfFbtFKwo_PH54AaABAg,@diaconescutiberiu7535,,1,BgN5hpl3WKc,1,2,2023-02-16T16:45:54Z,"I think this is exactly what i need to perform a task which I&#39;m struggling with: I have a view (which has 170 columns, some come from tables, some from other views and some come from calculated syntax &lt;complex CASE WHENs&gt;). I want to create a 2 column table: column_name, which will hold the actual column names (pulled from information.schema) and the definition responsible with creating that column. My approach was, pull all the text between SELECT and FROM (i refactored the original sql code so that it won&#39;t have any subqueries within the SELECT clause)(i moved them in the JOINs part). All columns have aliases, which means i do have the delimiter &quot;, (double quotes followed by comma) at the &quot;end&quot; of each column statement. For some reason, which can&#39;t figure out, instead of having 170 rows (with column&#39;s definitions) I get more. First issue i tracked down was that my code was splitting each calculated column (one row for CASE, many rows for each WHEN, another row for ELSE). I fixed that, still have issues. If i were to try regexp_split_to_table what do i need to type in after contents...so it splits text based on double quotes followed by comma? Would this work &#39; &quot;, &#39; ... do i need any escape characters?",2023-02-16T16:45:54Z
UgyEZqOfFbtFKwo_PH54AaABAg.9mCFtDDk1kC9mCayGGNN7g,@stratascratch,UgyEZqOfFbtFKwo_PH54AaABAg,2,BgN5hpl3WKc,0,1,2023-02-16T19:58:49Z,"I really can&#39;t answer this question without seeing the code. I also think it will be much more valuable if you struggle through this technical issue rather than having someone else debug your work. But all in all, your approach looks good to me. You&#39;ll need to study up on regexps and keep trying until you get to the right number. Try posting you question + code to StackOverflow. That community has been great for troubleshooting.",2023-02-16T19:58:49Z
UgzTs0gZjjFdJhMLsSV4AaABAg,@tanmaythaker2905,,1,-KXluZNAYrA,0,0,2023-08-04T18:12:59Z,"My solution<br>with first_last_calls as (<br>select *,<br>first_value(recipient_id) over (partition by  caller_id,date_called::DATE order by date_called asc) as first_call_for_that_day,<br>first_value(recipient_id) over (partition by caller_id, date_called::DATE order by date_called desc range between unbounded preceding and unbounded following) as last_call_for_that_day<br>from caller_history)<br><br>select distinct caller_id, recipient_id,  date_called::DATE<br>from first_last_calls<br>where first_call_for_that_day = last_call_for_that_day;",2023-08-04T18:12:59Z
UgyJFMsrET8Wt8xNuf94AaABAg,@mehmetkaya4330,,1,-KXluZNAYrA,1,0,2023-07-21T15:03:31Z,Great! Thank you so much!,2023-07-21T15:03:31Z
UgyJFMsrET8Wt8xNuf94AaABAg.9sRBNzuKBup9s_rR1I6pC9,@stratascratch,UgyJFMsrET8Wt8xNuf94AaABAg,2,-KXluZNAYrA,0,0,2023-07-25T09:12:50Z,Glad it was helpful!,2023-07-25T09:12:50Z
UgwlOq2DkJxWEuQrEZh4AaABAg,@chillvibe4745,,1,-KXluZNAYrA,0,0,2023-03-04T17:25:58Z,Great question! <br>In my opinion it&#39;s not right to include the call made on 2022-07-06 because there is only one call made on that day.<br>The way the question is asked it seems like we should filter out that day.<br>In my solution I filtered it out by getting only the days where there is more than one call.,2023-03-04T17:27:54Z
UgzIqH0APPNJxXuNVuR4AaABAg,@Moflia2,,1,-KXluZNAYrA,1,0,2023-02-18T20:12:33Z,"I love y&#39;alls website! It has been a lot of fun to get on, practice, and see how other people answer the same question in many different ways. Thank you",2023-02-18T20:12:33Z
UgzIqH0APPNJxXuNVuR4AaABAg.9mHm7W9RiEC9mI242xztfx,@stratascratch,UgzIqH0APPNJxXuNVuR4AaABAg,2,-KXluZNAYrA,0,0,2023-02-18T22:40:37Z,"Thank you! Appreciate the kind words. If you have any feedback or feature requests, you can always reach out to me here or at team@<a href=""http://stratascratch.com/"">stratascratch.com</a>.",2023-02-18T22:40:37Z
UgyqAkErQEUrtNt7-pJ4AaABAg,@redwannabil8031,,1,-KXluZNAYrA,0,0,2023-02-18T18:03:01Z,"i couldnt figure out the assumptions but this is what i came up with...<br><br>select x.* from<br>(select *,<br>case when datepart(day,date_called)=lead((datepart(day,date_called))) over(partition by caller_id order by date_called)<br>and recipient_id =lead(recipient_id) over (partition by caller_id order by date_called)<br>then 1 else 0 end as flag<br>from caller_history) x<br>where x.flag=1",2023-02-18T18:03:01Z
UgzIRFY6kiMoVNiFknp4AaABAg,@dannyd9333,,1,-KXluZNAYrA,0,4,2023-02-09T17:31:47Z,This channel is a gold mine of info!,2023-02-09T17:31:47Z
UgxRQ0xEIm9wajkYG-d4AaABAg,@abhimanyutiwari100,,1,-KXluZNAYrA,1,0,2023-02-09T01:12:44Z,Can we write case statement in first_value() function?,2023-02-09T01:12:44Z
UgxRQ0xEIm9wajkYG-d4AaABAg.9ltZXNPllJu9m04TT0n2AO,@stratascratch,UgxRQ0xEIm9wajkYG-d4AaABAg,2,-KXluZNAYrA,0,0,2023-02-11T23:15:14Z,I think so. What would it look like? Try it in the coding editor. There&#39;s a link in the description.,2023-02-11T23:15:14Z
Ugw8n_bfp99_1FnOGih4AaABAg,@architectaq813,,1,vdmCxvaRdLU,0,0,2023-08-30T09:17:39Z,"my solution:<br><br>with cte1 as<br>(select region_1 as region, variety, price <br>from winemag_p1<br>union all<br>select region_2, variety, price <br>from winemag_p1)<br><br>select a.region, a.cheapest_variety, b.expensive_variety from<br>(select distinct region, variety as cheapest_variety, price as cheapest_price<br>from cte1<br>where (region, price) in (select region, min(price) from cte1 group by region)) a<br>join<br>(select distinct region, variety as expensive_variety, price as expensive_price<br>from cte1<br>where (region, price) in (select region, max(price) from cte1 group by region)) b<br>using(region);",2023-08-30T09:17:39Z
UgwdzQqepVPrxOBF0zV4AaABAg,@VloggingMemories,,1,vdmCxvaRdLU,0,0,2023-08-08T07:46:18Z,"My Solution:<br>with final_view as<br>(select region_1 region, price, variety<br>from winemag_p1<br>where region_1 is not null<br>union<br>select region_2 region, price, variety<br>from winemag_p1<br>where region_2 is not null)<br>select t1.region,<br>    t2.variety expesive_variety,<br>    t3.variety cheapest_variety<br>from<br>(select region, <br>    max(price) expensive,<br>    min(price) cheapest<br>from final_view<br>group by 1)t1<br>join final_view t2 on t1.region=t2.region and t1.expensive=t2.price<br>join final_view t3 on t1.region=t3.region and t1.cheapest=t3.price<br>order by 1;",2023-08-08T07:46:18Z
UgwLaGdRmLimNwcvqT14AaABAg,@ashhere7610,,1,vdmCxvaRdLU,0,0,2023-05-16T19:11:39Z,"Hi StrataScratch, why did you perform a Union ALL in the initial CTE? Iff any duplicates, don&#39;t we want to exclude them?",2023-05-16T19:11:39Z
Ugxq9yeniYC6RagNSC54AaABAg,@petertran98,,1,vdmCxvaRdLU,6,0,2023-01-12T18:26:44Z,How would this work if there is two wines with the same price. Rank() would mark them both as the same ranking and u will lose this once u apply the max function?? Big edge case missing,2023-01-12T18:26:44Z
Ugxq9yeniYC6RagNSC54AaABAg.9knJbAZ7SWW9knVtpkofFP,@stratascratch,Ugxq9yeniYC6RagNSC54AaABAg,2,vdmCxvaRdLU,0,0,2023-01-12T20:14:09Z,Do you lose tied rankings with max() if there is a case statement that categorizes the rankings? It&#39;s worth testing out on the platform. The datasets typically include those edge cases. Max() is often used to isolate ties in rankings and should identify all the ties.,2023-01-12T20:14:09Z
Ugxq9yeniYC6RagNSC54AaABAg.9knJbAZ7SWW9kna2NQmooK,@petertran98,Ugxq9yeniYC6RagNSC54AaABAg,2,vdmCxvaRdLU,0,1,2023-01-12T20:59:08Z,For example if u have two rows in the same group with ranking of 1. Max will remove one of the two.,2023-01-12T20:59:08Z
Ugxq9yeniYC6RagNSC54AaABAg.9knJbAZ7SWW9kneK6sWRxe,@stratascratch,Ugxq9yeniYC6RagNSC54AaABAg,2,vdmCxvaRdLU,0,2,2023-01-12T21:36:31Z,@@petertran98 Ah that&#39;s true. The max() should be in the WHERE clause to return multiple values. Great catch! Thank you,2023-01-12T21:36:31Z
Ugxq9yeniYC6RagNSC54AaABAg.9knJbAZ7SWW9knePlHk5MA,@stratascratch,Ugxq9yeniYC6RagNSC54AaABAg,2,vdmCxvaRdLU,0,2,2023-01-12T21:37:17Z,we&#39;ll make the change!,2023-01-12T21:37:17Z
Ugxq9yeniYC6RagNSC54AaABAg.9knJbAZ7SWW9knmF20XR2b,@kimpijl3230,Ugxq9yeniYC6RagNSC54AaABAg,2,vdmCxvaRdLU,0,0,2023-01-12T22:45:43Z,Could you please upload the correct last part of the query? ThanksüòÉ,2023-01-12T22:45:43Z
UgwCAK0saSANSbZgVod4AaABAg,@pamelaalvarez9848,,1,93quPoReV1M,0,0,2022-12-05T06:10:31Z,"Or you know, we could just: <br><br>SELECT us.trackname, count(us.date)<br>FROM spotify_daily_rankings_2017_us us<br>INNER JOIN spotify_worldwide_daily_song_ranking ww <br>ON us.trackname = ww.trackname AND <a href=""http://us.date/"">us.date</a> = <a href=""http://ww.date/"">ww.date</a> AND ww.position = 1<br>GROUP BY trackname<br>ORDER BY us.trackname<br><br>üò£",2022-12-05T06:10:49Z
Ugx_rz2PU5zMg3Tl_nJ4AaABAg,@PersonalBurrito,,1,93quPoReV1M,3,4,2022-12-01T17:22:35Z,"Great video! Very well explained, but I still have a doubt as to why at <a href=""https://www.youtube.com/watch?v=93quPoReV1M&amp;t=07m50s"">07:50</a> you filtered us.position = 1, instead of world.position = 1, considering you already know all of the tracks are position 1 in us table but not necessarily in worlds.<br>Would you care to explain it to me? Thanks! üòÅ",2022-12-01T19:01:27Z
Ugx_rz2PU5zMg3Tl_nJ4AaABAg.9j62sZgEO4M9j6DfO5QC48,@dagmaraprzymus2606,Ugx_rz2PU5zMg3Tl_nJ4AaABAg,2,93quPoReV1M,0,5,2022-12-01T18:56:54Z,"You could even argue that if this statement had been put in the where clause, the use of the window function wouldn&#39;t have been needed, the whole code could have been much more intermediate level friendly ü§î",2022-12-01T18:56:54Z
Ugx_rz2PU5zMg3Tl_nJ4AaABAg.9j62sZgEO4M9j6OQ2MOz3I,@eldarnasyrov7588,Ugx_rz2PU5zMg3Tl_nJ4AaABAg,2,93quPoReV1M,0,5,2022-12-01T20:30:48Z,This is not the first time that a query suggested by this channel is incorrect or excessive.,2022-12-01T20:30:48Z
Ugx_rz2PU5zMg3Tl_nJ4AaABAg.9j62sZgEO4M9jYfJGszTuo,@joeymonachino6003,Ugx_rz2PU5zMg3Tl_nJ4AaABAg,2,93quPoReV1M,0,2,2022-12-12T20:05:54Z,"As stated above, I was thinking why this code is so complex. Most of it isn&#39;t needed at all.",2022-12-12T20:05:54Z
UgyksHzyrEbkdQXJu1N4AaABAg,@VloggingMemories,,1,M-dT_0m4qhI,0,0,2023-08-08T08:29:39Z,"My Solution:<br>with user_viewer as<br>(select distinct user_id from (select *,<br>row_number() over (partition by user_id order by session_start) rank_num<br>from twitch_sessions)x<br>where rank_num=1 and session_type=&#39;viewer&#39;)<br>select user_id, count(distinct session_id)<br>from twitch_sessions<br>where user_id in (select user_id from user_viewer) and session_type=&#39;streamer&#39;<br>group by 1<br>order by 2 desc, 1;",2023-08-08T08:29:39Z
Ugz04CKAOQKBUrQt9D54AaABAg,@ashhere7610,,1,M-dT_0m4qhI,0,0,2023-05-16T20:00:44Z,"Can&#39;t we use FIRST_VALUE() in order to find users with first &#39;viewer&#39; session?<br>With first_session_cte as<br>(<br>Select *,<br>FIRST_VALUE(session_type) over (Partition by user order by Session_start) as First_session<br>from <br>twitch_sessions<br>)",2023-05-16T20:00:44Z
UgzM1YLwOQp1sRJB5hl4AaABAg,@swethathiruppathy9973,,1,M-dT_0m4qhI,0,1,2022-12-29T18:18:14Z,"select user_id,count(session_start) over (partition by user_id order by user_id)<br>from<br>twitch_sessions<br>where <br>session_type LIKE &#39;streamer&#39;<br>and<br>user_id IN (<br>select  user_id from<br>(select *,ROW_NUMBER() OVER ( PARTITION BY User_id  ORDER BY session_start asc )ROW_NUM <br>from twitch_sessions)base <br>WHERE ROW_NUM = 1 AND session_type LIKE &#39;viewer&#39;)<br>order by count desc,user_id;",2022-12-29T18:18:14Z
UgyRUs9j9vL047oZxEV4AaABAg,@sridevim4302,,1,M-dT_0m4qhI,0,1,2022-12-22T15:11:29Z,"with cte as(<br>select * from(<br>select user_id,session_type,row_number() over(partition by user_id order by session_start) as rn from twitch_sessions)x where session_type=&#39;viewer&#39; <br>and rn=1)<br>select t.user_id,count(*) as cnt from twitch_sessions t inner join cte on t.user_id=cte.user_id where t.session_type=&#39;streamer&#39; group by t.user_id order by user_id",2022-12-22T15:11:29Z
UgwYuDv79LG5jmaS6Sd4AaABAg,@anxanhdekhoe,,1,M-dT_0m4qhI,1,2,2022-12-15T16:16:25Z,"My solution: <br>WITH first_ses AS (SELECT user_id, Min(session_start) as session_start<br>from twitch_sessions<br>GROUP BY user_id)<br>SELECT COUNT(t2.session_type), first_ses.user_id<br>FROM first_ses JOIN twitch_sessions t1 ON<br>first_ses.user_id = t1.user_id<br>AND t1.session_start = first_ses.session_start JOIN twitch_sessions t2 ON<br>first_ses.user_id = t2.user_id<br>WHERE t1.session_type = &#39;viewer&#39; AND t2.session_type = &#39;streamer&#39;<br>GROUP BY first_ses.user_id<br>ORDER BY COUNT(t2.session_type) DESC, first_ses.user_id",2022-12-15T16:16:25Z
UgwYuDv79LG5jmaS6Sd4AaABAg.9jezR7u5_J-9jk1ug3edwO,@stratascratch,UgwYuDv79LG5jmaS6Sd4AaABAg,2,M-dT_0m4qhI,0,0,2022-12-17T15:23:00Z,Thank you for sharing!,2022-12-17T15:23:00Z
UgxfSHXGzulWla6CH114AaABAg,@botiran,,1,M-dT_0m4qhI,0,0,2022-10-21T08:08:06Z,xqcL,2022-10-21T08:08:06Z
UgzT7aDyWpDqRSVFXh54AaABAg,@VloggingMemories,,1,Q-3wvYInCeI,0,0,2023-08-08T10:17:18Z,"My Solution:<br>with final_view as<br>(select t1.session_id, <br>    t1.post_id,<br>    ((t1.perc_viewed/100) * (t2.session_endtime-t2.session_starttime)) as time_spent<br>from post_views t1<br>join user_sessions t2 on t1.session_id=t2.session_id)<br>select post_id, sum(time_spent) viewing_time<br>from final_view<br>group by 1<br>order by 1;",2023-08-08T10:17:18Z
UgwnnW88CAUCT6rnVyN4AaABAg,@namangoyal8477,,1,Q-3wvYInCeI,0,0,2022-10-04T16:56:08Z,"select p.post_id, ((p.perc_viewed * (u.session_endtime-u.session_starttime))/100) from post_views p natural join user_sessions u;",2022-10-04T16:56:08Z
UgxSytjqmPCj3rhyA8J4AaABAg,@bocanegradev,,1,Q-3wvYInCeI,0,0,2022-09-30T19:23:15Z,"This is sooo great, it would be really awesome watch more of this ü•∫",2022-09-30T19:23:15Z
UgzWN7g4q7bT8zRJiMJ4AaABAg,@rajarajeswari8614,,1,Q-3wvYInCeI,6,0,2022-09-30T04:30:31Z,How to solve it in mySQL,2022-09-30T04:30:31Z
UgzWN7g4q7bT8zRJiMJ4AaABAg.9ga1ESzHKMl9gas6IAPE2N,@faeezaroos5908,UgzWN7g4q7bT8zRJiMJ4AaABAg,2,Q-3wvYInCeI,0,0,2022-09-30T12:21:15Z,"I tried replacing the EXTRACT(EPOCH from session_endtime) - EXTRACT(EPOCH from session_starttime) with TIMEDIFF(session_endtime,session_starttime) for mysql. Does it work on your end?",2022-09-30T12:21:15Z
UgzWN7g4q7bT8zRJiMJ4AaABAg.9ga1ESzHKMl9gax-L_xdbG,@rajarajeswari8614,UgzWN7g4q7bT8zRJiMJ4AaABAg,2,Q-3wvYInCeI,0,0,2022-09-30T13:04:00Z,"@@faeezaroos5908 TIMEDIFF is working but they have converted into float right that one when I used cast i am not getting. I am writing my query please help me to correct where it is wrong.<br>Select post_id, sum(duration * CAST((perc_viewed/100) as decimal) as total_viewtime)<br>from post_views p <br>(select session_id, TIMEDIFF(session_endtime,session_starttime) as duration <br>FROM user_sessions)us<br>on p.session_id = us.session_id<br>group by post_id;",2022-09-30T13:04:00Z
UgzWN7g4q7bT8zRJiMJ4AaABAg.9ga1ESzHKMl9gbx4f-l18n,@stratascratch,UgzWN7g4q7bT8zRJiMJ4AaABAg,2,Q-3wvYInCeI,0,0,2022-09-30T22:23:58Z,I believe we have the MySQL solution on the StrataScratch platform. A link is in the description. Just choose the appropriate SQL engine,2022-09-30T22:23:58Z
UgzWN7g4q7bT8zRJiMJ4AaABAg.9ga1ESzHKMl9gcc4Xwz2Cw,@rajarajeswari8614,UgzWN7g4q7bT8zRJiMJ4AaABAg,2,Q-3wvYInCeI,0,0,2022-10-01T04:39:41Z,@@stratascratch No it is not present,2022-10-01T04:39:41Z
UgzWN7g4q7bT8zRJiMJ4AaABAg.9ga1ESzHKMl9gcc_Z2vBny,@rajarajeswari8614,UgzWN7g4q7bT8zRJiMJ4AaABAg,2,Q-3wvYInCeI,0,0,2022-10-01T04:44:03Z,"It is a premium feature, but I am having free account",2022-10-01T04:44:03Z
Ugw6PS5Q9HBVD5bY7Xd4AaABAg,@scottgrenestedt9983,,1,Q-3wvYInCeI,1,0,2022-09-29T18:37:13Z,"Nice video, I&#39;m glad to say that I got the same answer on my own! By the way, what&#39;s the most common way/best practice when it comes to improving the runtime of a query? Thanks!",2022-09-29T18:37:13Z
Ugw6PS5Q9HBVD5bY7Xd4AaABAg.9gZyKzNXESN9g_0MjkwkLA,@stratascratch,Ugw6PS5Q9HBVD5bY7Xd4AaABAg,2,Q-3wvYInCeI,0,1,2022-09-29T19:03:40Z,"I think the best way to practice improving runtime would be to take a look at other people&#39;s code and compare it to yours. You&#39;ll be able to figure out how other people think and solve the problem. Just by looking at the code, you will be able to sense how fast it would run based on the number of joins, complex logical statements, etc in the code. StrataScratch has a feature where you can look at anyone&#39;s solutions to compare to yours. We will soon have a runtime feature to compare your runtime against other users. Other platforms have these features as well so it&#39;s good to explore and take a look around to see what way you like to learn the best.",2022-09-29T19:03:40Z
UgwHbC6x0nHqIjJ3TeJ4AaABAg,@VloggingMemories,,1,5Lpbw71xR3o,0,0,2023-08-08T11:41:56Z,"My Solution:<br>with duration as<br>(select location, <br>avg(signup_stop_date-signup_start_date) as avg_duration<br>from signups<br>group by 1),<br>transaction as<br>(select location,<br>avg(amt) as avg_amount<br>from transactions<br>join signups on signups.signup_id=transactions.signup_id<br>group by 1<br>)<br>select t1.location, <br>t1.avg_duration, <br>t2.avg_amount, <br>(t2.avg_amount/ t1.avg_duration)::float ratio<br>from duration t1, transaction t2<br>where t1.location=t2.location<br>order by (t2.avg_amount/ t1.avg_duration)::float desc;",2023-08-08T11:41:56Z
UgxjB-sKu3GODnG1Eed4AaABAg,@sridevim4302,,1,5Lpbw71xR3o,1,4,2022-12-23T14:38:33Z,"select x.location,x.avg_dur,y.avg_trans_amt,convert (x.avg_dur/y.avg_trans_amt<br>, float) as ratio<br>from(<br><br>select location,signup_id,avg(datediff(signup_stop_date,signup_start_date)) as avg_dur from signups<br>group by location)x<br><br>join<br><br>(select location,avg(amt) as avg_trans_amt from signups s join transactions t on t.signup_id=s.signup_id group by location)y<br><br>on x.location=y.location order by ratio desc",2022-12-23T14:38:33Z
UgxjB-sKu3GODnG1Eed4AaABAg.9jzPaOjE8Kl9k8wred0vPV,@stratascratch,UgxjB-sKu3GODnG1Eed4AaABAg,2,5Lpbw71xR3o,0,1,2022-12-27T16:50:24Z,Thanks for sharing!,2022-12-27T16:50:24Z
UgxQp9SC9hu3Hbw816t4AaABAg,@sridevim4302,,1,DmUR2QSNUq8,1,0,2022-12-24T14:05:08Z,Did anyone try it in mysql ? If you got the output snd it,2022-12-24T14:05:08Z
UgxQp9SC9hu3Hbw816t4AaABAg.9k0vZSVFHXz9k1MTw49AqG,@stratascratch,UgxQp9SC9hu3Hbw816t4AaABAg,2,DmUR2QSNUq8,0,0,2022-12-24T18:09:02Z,Try going on the platform and checking out the MySQL option. Users will post their solutions.,2022-12-24T18:09:02Z
UgzMYbZEbfoqiNRefed4AaABAg,@shobhamourya8396,,1,DmUR2QSNUq8,0,0,2022-09-08T15:21:18Z,"Here&#39;s my step-by-step solution using CTE:<br>with <br>cte1<br>as<br>(<br>-- get order date in YYYY-MM format<br>select delivery_id id, <br>       to_char(order_placed_time::date,&#39;YYYY-MM&#39;) order_date,<br>       predicted_delivery_time predicted,<br>       actual_delivery_time actual<br>from delivery_orders<br>),<br>cte2<br>as<br>(<br>-- calculate time interval between the actual and predicted delivery time<br>select order_date, (actual - predicted) delay_interval<br>from cte1<br>order by order_date<br>),<br>cte3<br>as<br>(<br>-- convert the time difference interval into minutes<br>select order_date, extract(minute from delay_interval) delay_minutes<br>from cte2<br>),<br>cte4<br>as<br>(<br>-- find the total orders and extremely late orders for each month<br>select order_date, <br>       count(*) monthly_orders,<br>       sum(case when delay_minutes &gt; 20 then 1 else 0 end) extreme_delay<br>from cte3<br>group by order_date<br>order by order_date<br>)<br>-- Find the proportion of extremely late orders to total orders for each month<br>select *,  round((extreme_delay*1.0/monthly_orders*1.0)*100,2) proportion<br>from cte4",2022-09-08T15:29:17Z
UgxV6xTeq2oGpaxkFXB4AaABAg,@eldarnasyrov7588,,1,DmUR2QSNUq8,1,2,2022-09-08T06:10:11Z,"But the EXTRACT function doesn&#39;t work the way you need. It literately extracts minutes. Imagine you&#39;re trying to calculate the difference between &quot;2022-09-08 <a href=""https://www.youtube.com/watch?v=DmUR2QSNUq8&amp;t=21h30m00s"">21:30:00</a>&quot; and &quot;2022-09-08 <a href=""https://www.youtube.com/watch?v=DmUR2QSNUq8&amp;t=09h00m00s"">09:00:00</a>&quot;. In this case the output will be 30 minutes while in fact it&#39;s 750 minutes (via the &quot;datediff&quot; function).",2022-09-08T06:10:11Z
UgxV6xTeq2oGpaxkFXB4AaABAg.9fhZ9DrCHyN9h-w39BtnWv,@KME1983,UgxV6xTeq2oGpaxkFXB4AaABAg,2,DmUR2QSNUq8,0,0,2022-10-10T15:16:02Z,"Yep, better to do DATEDIFF(&#39;minute&#39;, predicted_delivery_time, actual_delivery_time)",2022-10-10T15:16:02Z
UgykVIvVuP2HrJYXUWJ4AaABAg,@djjiang3718,,1,DmUR2QSNUq8,0,1,2022-09-08T02:35:18Z,Thank you! Good stuff!,2022-09-08T02:35:18Z
UgwS-vR0du9hf28HTK14AaABAg,@gabrielfinco8768,,1,DmUR2QSNUq8,0,1,2022-09-08T01:05:34Z,I missed this v√≠deos so much!! Very helpfull <br>Stratascratch should put up a SQL data analysis  course!,2022-09-08T01:05:34Z
UgyGZsFwK-VSSyM9A9N4AaABAg,@dwaipayansaha4443,,1,DmUR2QSNUq8,0,0,2022-09-07T19:12:19Z,"My Solution:-<br>with t1 as(select date_format(order_placed_time,&#39;%Y-%m&#39;) year_month1,<br>sum(case when date_add(predicted_delivery_time, interval 20 minute)&lt;actual_delivery_time then 1 else 0 end) ex_late_orders<br>from delivery_orders<br>group by year_month1<br>order by year_month1),<br>t2 as(select date_format(order_placed_time,&#39;%Y-%m&#39;) year_month1, count(distinct delivery_id) total_orders from delivery_orders<br>group by year_month1)<br>select t2.year_month1,round(ex_late_orders*100.0/total_orders,3) pct_late from t2<br>left join t1 on t2.year_month1=t1.year_month1<br>Happy codingüòÉüòÉ!!",2022-09-07T19:13:07Z
UgzokIZj_PzeP3U7GFt4AaABAg,@VloggingMemories,,1,o8cFJmI50Dw,0,0,2023-08-12T07:38:14Z,"My Solution:<br><br>with last_page(last_page_number) as<br>(select max(page_number)  last_page_number<br>from cookbook_titles),<br>generate_series as<br>(select generate_series page_number<br>from last_page, generate_series(0,last_page.last_page_number::int)<br>where generate_series%2=0)<br>select t1.page_number, <br>    t2.title left_title,<br>    t3.title right_title<br>    from generate_series t1<br>left join cookbook_titles t2 on t1.page_number=t2.page_number<br>left join cookbook_titles t3 on t1.page_number+1=t3.page_number<br>order by 1;",2023-08-12T07:38:14Z
UgxJcBJHjiYy7A3_deN4AaABAg,@redwannabil8031,,1,o8cFJmI50Dw,0,0,2023-02-02T18:13:15Z,"with cte as<br>(select 0 as num<br>union all<br>select cte.num+1 as num<br>from cte <br>where num &lt;15),<br>cte2 as<br>(select cte.num as page_number,c.title as title from cookbook_titles c<br>right join cte on c.page_number=cte.num),<br>cte3 as<br>(select convert(decimal,(x.page_number/2)*2) as left_page_number,<br>case when convert(decimal,(x.page_number)%2)=0 then x.title end as left_title,<br>case when convert(decimal,(x.page_number)%2)!=0 then x.title end as right_title<br>from cte2 as x)<br><br>select cte3.left_page_number,max(cte3.left_title) as left_title,max(cte3.right_title) as right_title from cte3<br>group by cte3.left_page_number",2023-02-02T18:13:15Z
Ugz387pnxi5AwrlfQHV4AaABAg,@jerrygeorge4478,,1,o8cFJmI50Dw,1,1,2022-09-07T21:03:04Z,"just tried it without generate_series() because we don&#39;t have that in MySQL  :/<br><br><br>WITH mega AS (SELECT row_number() over (ORDER by a.page_number)-1 rr FROM cookbook_titles a CROSS JOIN cookbook_titles b),<br>seq AS (SELECT * FROM mega WHERE rr&lt;=(SELECT MAX(page_number) FROM cookbook_titles))<br>SELECT seq.rr AS left_page,cook1.title,cook2.title FROM seq<br>LEFT JOIN cookbook_titles cook1<br>ON seq.rr=cook1.page_number<br>LEFT JOIN cookbook_titles AS cook2<br>ON seq.rr=cook2.page_number-1<br>WHERE seq.rr%2=0<br><br><br>probably not the most efficient but oh well",2022-09-07T21:03:26Z
Ugz387pnxi5AwrlfQHV4AaABAg.9fg_Y2A315a9fhlSxAtTwb,@stratascratch,Ugz387pnxi5AwrlfQHV4AaABAg,2,o8cFJmI50Dw,0,0,2022-09-08T08:06:28Z,"Wonderful, thank you for sharing!",2022-09-08T08:06:28Z
UgwpjCSe5pMp2K1cJN54AaABAg,@pankaj3146,,1,o8cFJmI50Dw,1,1,2022-08-30T18:38:24Z,"Hi @StrataScratch,<br><br>I solved the above problem without using string_agg() and case statement. Below is the query for the same. My logic is much simpler than one shown above as it uses rows between clause. I have tested the query and the final result is same as the one shown above.<br><br>/* query starts here */<br><br>with cookbook_pagenums as ( select generate_series as page_number from generate_series(0,(select max(page_number) from cookbook_titles)) ),<br>cookbook_v2 as ( select t1.page_number,t2.title from cookbook_pagenums t1<br>left join cookbook_titles t2<br>on t1.page_number = t2.page_number ),<br>cookbook_final as ( select page_number,title as left_title,max(title) over (rows between 1 following and 1 following) as right_title from cookbook_v2 )<br>select * from cookbook_final where ( page_number % 2 ) = 0",2022-08-30T18:38:24Z
UgwpjCSe5pMp2K1cJN54AaABAg.9fMicRk5F-G9fMukXfJNHt,@stratascratch,UgwpjCSe5pMp2K1cJN54AaABAg,2,o8cFJmI50Dw,0,1,2022-08-30T20:24:21Z,Ah wonderful!  Thank you for sharing.,2022-08-30T20:24:21Z
UgyxP5FSEId2ERxggOd4AaABAg,@nishanthkumar4743,,1,o8cFJmI50Dw,1,0,2022-08-15T06:35:07Z,"with<br>no_series as (select generate_series as page_number from generate_series(0, (select max(page_number) from cookbook_titles))),<br>all_data as (<br>		select<br>		gs.page_number left_page_number,<br>		ct.title left_title,<br>		case when<br>			lead(ct.page_number,1) over(order by gs.page_number) is not null then lead(ct.title,1) over(order by gs.page_number)<br>			else null end right_title<br>		from no_series gs left join cookbook_titles ct on gs.page_number=ct.page_number)<br>select * from all_data where (left_page_number)%2 = 0",2022-08-15T06:35:07Z
UgyxP5FSEId2ERxggOd4AaABAg.9ejnvrcTP509fBfHSk_4Lt,@harshalpatel3012,UgyxP5FSEId2ERxggOd4AaABAg,2,o8cFJmI50Dw,0,0,2022-08-26T11:37:32Z,Very nice and simple explanation,2022-08-26T11:37:32Z
Ugz8J5Ddb-o-MH9f7jF4AaABAg,@brettpeace9858,,1,o8cFJmI50Dw,0,0,2022-08-11T19:24:05Z,Maybe if i had a JUMBO-TRON I could see the f*ckng 2pt font that you use!! Combine that with the monotone voice ... i lost 3 minutes of my life ... thanks,2022-08-11T19:24:05Z
UgwUpC9zOhHHDe8IZEx4AaABAg,@VloggingMemories,,1,8zeLdtkY2CQ,0,0,2023-08-12T08:33:23Z,"My Solution:<br>with user_lastest as<br>(select account_id,<br>    user_id,<br>    to_char(max(date), &#39;YYYY-MM&#39;) as latest_month_date<br>from sf_events<br>group by 1, 2),<br>dec_2020 as<br>(select account_id,<br>    sum(retention_status)/count(user_id)::float dec_rt<br>from<br>(select t1.account_id,<br>    t1.user_id,<br>    t1.year_month,<br>    t2.latest_month_date,<br>    case <br>        when latest_month_date&gt;year_month then 1 else 0<br>        end as retention_status<br>from<br>(select distinct account_id,<br>    user_id,<br>    to_char(date, &#39;YYYY-MM&#39;) as year_month<br>from sf_events<br>where to_char(date, &#39;YYYY-MM&#39;)=&#39;2020-12&#39;)t1<br>join user_lastest t2 on t1.account_id=t2.account_id and t1.user_id=t2.user_id)t3<br>group by 1),<br>jan_2021 as<br>(select account_id,<br>    sum(retention_status)/count(user_id)::float jan_rt<br>from<br>(select t1.account_id,<br>    t1.user_id,<br>    t1.year_month,<br>    t2.latest_month_date,<br>    case <br>        when latest_month_date&gt;year_month then 1 else 0<br>        end as retention_status<br>from<br>(select distinct account_id,<br>    user_id,<br>    to_char(date, &#39;YYYY-MM&#39;) as year_month<br>from sf_events<br>where to_char(date, &#39;YYYY-MM&#39;)=&#39;2021-01&#39;)t1<br>join user_lastest t2 on t1.account_id=t2.account_id and t1.user_id=t2.user_id)t3<br>group by 1)<br>select t1.account_id,<br>jan_rt/dec_rt::float<br>from jan_2021 t1, dec_2020 t2<br>where t1.account_id=t2.account_id;",2023-08-12T08:33:23Z
Ugx9dIqSfBUBOX-JddJ4AaABAg,@huanchenli4137,,1,8zeLdtkY2CQ,0,0,2023-04-28T00:35:12Z,nice video! but the only thing that guy said is yea....lol,2023-04-28T00:35:12Z
UgxpkTf3xhanww-bb4N4AaABAg,@stanislavdidenko8436,,1,8zeLdtkY2CQ,0,0,2022-10-01T17:11:13Z,"with aum_order as<br>( <br>    select <br>        distinct account_id, user_id, date_trunc(&#39;months&#39;, date) as month<br>    from sf_events<br>    order by account_id, user_id, month desc<br>)<br>, aum_lead as<br>( <br>    select <br>        *,<br>        lead(month, 1) over (partition by account_id, user_id order by month) lead_month<br>    from aum_order<br>)<br>, acc_rr as<br>(<br>    select <br>        account_id, month,<br>        100*sum(case when lead_month is not null then 1 else 0 end )/count(*)*1.0 rr<br>    from aum_lead<br>    where month&lt;&#39;2021-02-01&#39;<br>    group by account_id, month<br>)<br>, rr_next as<br>(<br>select<br>    account_id, month, rr,  lag(rr, 1) over (partition by account_id order by month) rr_next<br>    from acc_rr<br>    order by account_id, month desc<br>)<br>select *,  rr/rr_next<br>from rr_next<br>where rr_next is not null<br>;",2022-10-01T17:11:13Z
UgyV4HHU2nuww82n99V4AaABAg,@challenger42,,1,8zeLdtkY2CQ,1,0,2022-08-09T08:16:22Z,"Hi, thank you for amazing video. During the interview, should we use SQL only, or can we go with pandas as well?",2022-08-09T08:16:22Z
UgyV4HHU2nuww82n99V4AaABAg.9eVXk8kRVvA9eWZG8Wkh3z,@stratascratch,UgyV4HHU2nuww82n99V4AaABAg,2,8zeLdtkY2CQ,0,1,2022-08-09T17:48:51Z,"I would say it will depend on where you are more comfortable and which you think will help you better come up with a solution.  But for this interview video, I would use SQL.",2022-08-09T17:48:51Z
Ugz8bmYiCS9sL9-DZah4AaABAg,@sweetie_py,,1,8zeLdtkY2CQ,1,0,2022-08-06T11:34:50Z,"damn... like i do know about assuming, breaking down the problems and identifying the edge cases but idk how to get good at each of those. Could you make a video on how to think in SQL?",2022-08-06T11:34:50Z
Ugz8bmYiCS9sL9-DZah4AaABAg.9eOA4AGKPQQ9eP03Gh0ncC,@stratascratch,Ugz8bmYiCS9sL9-DZah4AaABAg,2,8zeLdtkY2CQ,0,0,2022-08-06T19:26:34Z,That sounds like a plan.,2022-08-06T19:26:34Z
UgyiYKhrkfL9Ort0kTp4AaABAg,@AnkitPatel-jg1fw,,1,8zeLdtkY2CQ,1,0,2022-07-24T19:20:09Z,Great video. Amazing content. I come from a chemical engineering background. I have an admit for MS in CS in US. Will that help me get into data science? Will not having prior experience cause hindrance in getting full times. I would be grateful if you could answer,2022-07-24T19:20:09Z
UgyiYKhrkfL9Ort0kTp4AaABAg.9dsWzrpKSSR9dwDP-6ALuK,@stratascratch,UgyiYKhrkfL9Ort0kTp4AaABAg,2,8zeLdtkY2CQ,0,0,2022-07-26T05:45:55Z,"Thank you, glad you like it.  There is no set of requirements for Data Science, but an engineering background can help you. You will definitely need to sharpen relevant skills in order to clinch that offer.",2022-07-26T05:45:55Z
UgwKhfO4TE_0vg4C_g54AaABAg,@phuongdinh3769,,1,8zeLdtkY2CQ,1,0,2022-06-29T16:27:35Z,I&#39;ve learned so much from this! Thank you!,2022-06-29T16:27:35Z
UgwKhfO4TE_0vg4C_g54AaABAg.9crqN6G9kgQ9dmmbCMqLEb,@stratascratch,UgwKhfO4TE_0vg4C_g54AaABAg,2,8zeLdtkY2CQ,0,0,2022-07-22T13:49:53Z,Wonderful!  You are welcome.,2022-07-22T13:49:53Z
UgzmKZL6t7JtqSTDlcd4AaABAg,@kaitlynkhanhthidong5452,,1,8zeLdtkY2CQ,1,2,2022-06-29T03:47:12Z,I took your class 2 years ago during the pandemic at USF and I&#39;m so glad your channel is doing well! It really sparked my interest in problem-solving and data analytics for me even though my major is Finance. Found myself here as I need to solve for a SQL problem :) Keep up the good work prof!,2022-06-29T03:47:12Z
UgzmKZL6t7JtqSTDlcd4AaABAg.9cqULrms80G9cvO1kO7Uad,@stratascratch,UgzmKZL6t7JtqSTDlcd4AaABAg,2,8zeLdtkY2CQ,0,1,2022-07-01T01:28:14Z,Oh very cool! Small world that you found me here! And glad you are continuing with SQL and data analytics! Good luck!,2022-07-01T01:28:14Z
UgziCnvONcOpwoR-Qld4AaABAg,@eduarlara3424,,1,8zeLdtkY2CQ,1,0,2022-06-25T03:37:58Z,"i have a college degree in mechanical engineering and naval engineering, will this help me get a job as a data scientist?",2022-06-25T03:37:58Z
UgziCnvONcOpwoR-Qld4AaABAg.9cgA6iG8Y8k9dbNXxsMCBc,@stratascratch,UgziCnvONcOpwoR-Qld4AaABAg,2,8zeLdtkY2CQ,0,0,2022-07-18T03:30:28Z,An engineering background can definitely help you become a Data Scientist.,2022-07-18T03:30:28Z
UgwIjx26t4vK61vbpoh4AaABAg,@rustinshamloo3413,,1,8zeLdtkY2CQ,0,0,2022-06-23T05:56:15Z,Keep up the good work guys!,2022-06-23T05:56:15Z
UgydWC3GfkyfVdCAy9J4AaABAg,@YUVRAJSINGH-iz9gt,,1,8zeLdtkY2CQ,1,0,2022-06-23T04:25:25Z,"Hi team, I am going to buy a lifetime subscription of strata could someone please provide me a discount coupon for the same",2022-06-23T04:25:25Z
UgydWC3GfkyfVdCAy9J4AaABAg.9cb5xXzBATt9dmmmkCqqC-,@stratascratch,UgydWC3GfkyfVdCAy9J4AaABAg,2,8zeLdtkY2CQ,0,0,2022-07-22T13:51:28Z,"Kindly email nate@<a href=""http://stratascratch.com/"">stratascratch.com</a>.  We do have a discount for students.",2022-07-22T13:51:28Z
Ugw8yb3_XIMf9dERtzV4AaABAg,@artivd6429,,1,8zeLdtkY2CQ,0,0,2022-06-22T04:45:28Z,"Please watch for python interview questions with explanations <a href=""https://youtu.be/92_YNkA90Nc?view_as=subscriber?sub_confirmation=1"">https://youtu.be/92_YNkA90Nc?view_as=subscriber?sub_confirmation=1</a>",2022-06-22T04:45:28Z
UgznsTszlkYd0dRtbad4AaABAg,@ethigpen8,,1,8zeLdtkY2CQ,0,1,2022-06-19T17:15:12Z,stratascratch and your video have been huge for me throughout my interview process,2022-06-19T17:15:12Z
UgxVhLHpizEwKyBS9NJ4AaABAg,@joel9616,,1,8zeLdtkY2CQ,0,0,2022-06-19T14:33:00Z,Thank You for this,2022-06-19T14:33:00Z
UgwgCfwOF-d5Jc1n35l4AaABAg,@yuthpatirathi2719,,1,8zeLdtkY2CQ,0,0,2022-06-18T00:36:39Z,Nate this is an amazing video. Thank you,2022-06-18T00:36:39Z
UgyjGTzk8OMeQTZNP2R4AaABAg,@jennymaalouf9928,,1,8zeLdtkY2CQ,0,0,2022-06-17T10:21:39Z,Thank you these videos are so helpful!,2022-06-17T10:21:39Z
Ugw39jF5i92g3rmijnh4AaABAg,@VloggingMemories,,1,OzyHfddLj9g,0,0,2023-08-12T08:43:18Z,My Solution:<br>select first_name<br>from employee<br>where id in<br>(select manager_id<br>from employee<br>group by 1<br>having count(distinct first_name)&gt;=7);,2023-08-12T08:43:18Z
UgwJNfYMa7JRfywj2y94AaABAg,@no-nonsense-here,,1,OzyHfddLj9g,1,1,2022-10-17T06:53:28Z,"Thank you for the video!<br><br>My PostgreSQL solution: Using SELF JOIN<br><br>SELECT e2.first_name<br>FROM employee e1<br>JOIN employee e2 ON e1.manager_id = <a href=""http://e2.id/"">e2.id</a><br>GROUP BY e2.first_name<br>HAVING COUNT(*) &gt;= 7;",2022-10-17T07:00:39Z
UgwJNfYMa7JRfywj2y94AaABAg.9hH36PUgM9E9hHrfQv7phA,@stratascratch,UgwJNfYMa7JRfywj2y94AaABAg,2,OzyHfddLj9g,0,0,2022-10-17T14:24:02Z,Thank you for sharing!,2022-10-17T14:24:02Z
UgyxVCdLeKSXCbzhSmB4AaABAg,@yuthpatirathi2719,,1,OzyHfddLj9g,0,0,2022-06-11T17:12:16Z,amazing video. Thanks,2022-06-11T17:12:16Z
UgzlSvevRz3fiUHtjuJ4AaABAg,@xylyx_,,1,OzyHfddLj9g,0,2,2022-05-15T12:05:20Z,"with cte as <br>(<br>select manager_id<br>from employee<br>group by manager_id<br>having count(manager_id) &gt;= 7<br>)<br>select first_name<br>from employee <br>join cte<br>on <a href=""http://employee.id/"">employee.id</a> = cte.manager_id",2022-05-15T12:05:20Z
UgyR7vFztbWJ_zBUl1p4AaABAg,@brwler1,,1,OzyHfddLj9g,0,0,2022-05-03T15:06:42Z,please fix audio. great video otherwise,2022-05-03T15:06:42Z
UgzqhcQftqSNuI3SzW94AaABAg,@rohitprajapati2303,,1,OzyHfddLj9g,0,0,2022-04-24T15:47:15Z,"You can shorten the query by using below query<br>select first_name<br>from employee a<br>inner join (select manager_id ,count(distinct id) from employee<br>group by 1<br>having count(distinct id) &gt;=7 ) b<br>on <a href=""http://a.id/"">a.id</a> =b.manager_id",2022-04-24T15:47:15Z
UgwwBacAdlZX6xNYfWt4AaABAg,@simardeepkaur969,,1,OzyHfddLj9g,1,2,2022-04-13T23:53:31Z,Thanks for such great content. I really appreciate you making all these videos with such great explanantion. Can you please solve - Apple Product Counts from Stratascratch. Thanks in advance :),2022-04-13T23:53:31Z
UgwwBacAdlZX6xNYfWt4AaABAg.9_mNBoD177v9_o0LEeqZPH,@stratascratch,UgwwBacAdlZX6xNYfWt4AaABAg,2,OzyHfddLj9g,0,0,2022-04-14T15:12:18Z,We&#39;ll add that one to our list!,2022-04-14T15:12:18Z
UgzxrPA9z32A7nT6fCt4AaABAg,@vdyb745,,1,OzyHfddLj9g,2,2,2022-04-13T16:09:33Z,Why is the voice so different ? Some places not able to follow what is being said.,2022-04-13T16:09:33Z
UgzxrPA9z32A7nT6fCt4AaABAg.9_lY5eEcNhI9_lZDCvUTeI,@stratascratch,UgzxrPA9z32A7nT6fCt4AaABAg,2,OzyHfddLj9g,0,1,2022-04-13T16:19:19Z,Sorry about that. Some mic issues =(,2022-04-13T16:19:19Z
UgzxrPA9z32A7nT6fCt4AaABAg.9_lY5eEcNhI9aARdCx0eDO,@minnuss,UgzxrPA9z32A7nT6fCt4AaABAg,2,OzyHfddLj9g,0,0,2022-04-23T17:33:21Z,"@@stratascratch Everything in video is great, except microphone production. Keep up the good work. üëç",2022-04-23T17:33:21Z
UgwUY1ulioxjlpNCjaV4AaABAg,@AbhishekKumar-bh3lx,,1,OzyHfddLj9g,0,0,2022-04-13T05:04:06Z,Thank you for sharing. <br><br>One other condition I believe we should have put is - where manage_id != id,2022-04-13T05:04:06Z
UgxIv0dmmeMxFrZhwqp4AaABAg,@VloggingMemories,,1,Av92CaPTRr8,0,0,2023-08-12T10:16:41Z,"My Solution:<br>select id_guest,<br>    sum(n_messages),<br>    dense_rank() over (order by sum(n_messages) desc)<br>from airbnb_contacts<br>group by 1<br>order by 2 desc;",2023-08-12T10:16:41Z
UgzXYAuOMYkocvqz5N94AaABAg,@VloggingMemories,,1,Av92CaPTRr8,0,0,2023-08-12T10:10:29Z,"My Solution:<br>select city, <br>    count(distinct business_id) busi_cnt,<br>    dense_rank() over (order by count(distinct business_id) desc) rank<br>from yelp_business<br>where stars=5<br>group by 1<br>order by 2 desc;",2023-08-12T10:10:29Z
Ugze-aRbz3ADLZAGjPB4AaABAg,@yuthpatirathi2719,,1,Av92CaPTRr8,0,0,2022-06-12T00:42:34Z,"Thank you for the second I  used this solution <br><br>with tt as  (select sum(n_messages) as s , id_guest from airbnb_contacts<br>group by 2<br>order by 1 desc) <br>select dense_rank() over ( order by s desc) , id_guest, s from tt ;",2022-06-12T00:42:34Z
UgyII-0IYrv9AxB8oLN4AaABAg,@gamergoosling,,1,Av92CaPTRr8,0,0,2022-04-08T05:29:13Z,super helpful. i‚Äôd greatly appreciate more videos that cover the use cases of any and all SQL functions that are commonly used in industry. thanks!,2022-04-08T05:29:13Z
UgxYzrvDjM6KNLIGyhh4AaABAg,@florincopaci6821,,1,Av92CaPTRr8,0,1,2022-03-08T16:46:09Z,Thank you for all the videos from which i learned i lot.Thank you!<br>It is possible to do in the future a video about the question -2153. The Number of Passengers in Each Bus II from LeetCode? please.,2022-03-08T16:46:09Z
UgztsbCWhnfTNSjYXFt4AaABAg,@sharathgowdayr5509,,1,Av92CaPTRr8,0,2,2022-03-06T07:42:49Z,Great thought process while solving SQL interview questions.,2022-03-06T07:42:49Z
UgxLSTY2y3t_1uo-cXd4AaABAg,@rakibraihan1572,,1,Av92CaPTRr8,1,1,2022-03-04T16:28:45Z,Carry on your good work.,2022-03-04T16:28:45Z
UgxLSTY2y3t_1uo-cXd4AaABAg.9Z9_VGQOUfH9ZAPhtDk0IX,@stratascratch,UgxLSTY2y3t_1uo-cXd4AaABAg,2,Av92CaPTRr8,0,0,2022-03-05T00:13:44Z,Glad you keep up with my channel!,2022-03-05T00:13:44Z
Ugyl8r4jdNpCqxFvEwR4AaABAg,@e555t66,,1,Av92CaPTRr8,0,0,2022-03-02T22:01:29Z,Use your edu email get stratascratch lifetime for 100 -&gt; Amazon BIE/BA -&gt; data science or engg..,2022-03-02T22:01:29Z
UgyJpLtLCVYu89TS5S54AaABAg,@caiyu538,,1,Av92CaPTRr8,0,0,2022-03-02T14:51:18Z,Great series,2022-03-02T14:51:18Z
UgxVWCv1kr10aLT9Gul4AaABAg,@VloggingMemories,,1,VEjxlKBkZGM,0,1,2022-11-12T12:22:14Z,"with consecutive_win_data as<br>(select player_id,<br>match_date,<br>match_result,<br>row_number() over (partition by player_id order by match_date) <br>as row_num_1,<br>row_number() over (partition by player_id , match_result order by match_date) <br>as row_num_2<br>from players_results)<br>select player_id,<br>cnt,<br>rank() over (order by cnt desc)<br>from<br>(select player_id,<br>count(*) as cnt,<br>rank() over (partition by player_id order by count(*) desc) as rank<br>from<br>(select player_id, match_date, match_result , (row_num_1-row_num_2) as diff from consecutive_win_data<br>where match_result=&#39;W&#39;)x<br>group by player_id, diff)y<br>where rank=1<br>order by cnt desc<br>limit 2;",2022-11-12T12:22:14Z
UgxAPG3Uzl3xOGO27s54AaABAg,@SP-db6sh,,1,VEjxlKBkZGM,0,0,2022-10-30T16:44:28Z,"Easier options there in pandas to solve this problem , can&#39;t we take this into Pandas dataframe ?",2022-10-30T16:44:28Z
UgzOaM_rq7RQCHhAB8x4AaABAg,@nikhilp2351,,1,VEjxlKBkZGM,1,0,2022-09-11T07:32:23Z,"The first solution wouldn&#39;t always work. For example, if the streak is WLWWW, it wouldn&#39;t work.",2022-09-11T07:32:23Z
UgzOaM_rq7RQCHhAB8x4AaABAg.9fpQwv0UMIC9jzRk31ckK5,@duckpond2672,UgzOaM_rq7RQCHhAB8x4AaABAg,2,VEjxlKBkZGM,0,0,2022-12-23T14:57:21Z,WWLLLWL wouldn&#39;t work neither... That method is often (like in this video) misapplied without discussing its use case.,2022-12-23T15:06:00Z
UgxakLwgY-ORLHIT3iV4AaABAg,@shima1960ify,,1,VEjxlKBkZGM,1,1,2022-08-24T11:31:02Z,The second solution is really difficult.<br>The first solution is really cleaver one.<br>Do you think there is another way to solve the question?,2022-08-24T11:31:02Z
UgxakLwgY-ORLHIT3iV4AaABAg.9f6Vx61uvJ69f7l6BDOv_g,@stratascratch,UgxakLwgY-ORLHIT3iV4AaABAg,2,VEjxlKBkZGM,0,0,2022-08-24T23:11:28Z,There can be a number of different ways to solve a question.  You might find a solution that can work for you.,2022-08-24T23:11:28Z
UgzTuQA32cKSjUqJMj54AaABAg,@touchsoumya,,1,VEjxlKBkZGM,2,0,2022-07-17T18:43:17Z,"@StrataScratch I think we would miss out displaying the player ids who has not won any matches. But as per the actual question, we should still be able to display all player ids and the streak length should be 0 for the player who has not won any matches. Could you please clarify?",2022-07-17T18:43:17Z
UgzTuQA32cKSjUqJMj54AaABAg.9daRCj8n8Tj9daSFDjVjz7,@touchsoumya,UgzTuQA32cKSjUqJMj54AaABAg,2,VEjxlKBkZGM,0,0,2022-07-17T18:52:22Z,"select a.player_id,coalesce(longest_streak,0) longest_streak from (<br> (select distinct player_id  from Matches ) a<br> left join (<br>select distinct player_id,<br>cnt as longest_streak from (<br>select player_id,streaks,count(*) as cnt,<br>row_number() over(partition by player_id order by count(*) desc) as rn from (<br>select player_id , match_day  , result,<br>row_number() over (partition by player_id order by match_day ) as rn1,<br>row_number() over (partition by player_id,result order by match_day ) as rn2,<br>(row_number() over (partition by player_id order by match_day )) - <br>(row_number() over (partition by player_id,result order by match_day )) streaks<br>from Matches<br>    )a <br>    where result = &#39;win&#39;<br>    group by player_id,streaks<br>    )b<br>    where rn=1<br>    )c <br>    on a.player_id = c.player_id<br>    )<br> <br>--This works in SQL Server",2022-07-17T18:52:22Z
UgzTuQA32cKSjUqJMj54AaABAg.9daRCj8n8Tj9g-UCZz_VzR,@stratascratch,UgzTuQA32cKSjUqJMj54AaABAg,2,VEjxlKBkZGM,0,0,2022-09-15T14:32:31Z,"Since we are asked only to get the win streak and the longest one, the players that haven&#39;t won any match don&#39;t influence the solution.",2022-09-15T14:32:31Z
UgzCyERPD5KToN11imx4AaABAg,@yuthpatirathi2719,,1,VEjxlKBkZGM,3,0,2022-06-12T21:27:58Z,@nate or anyone from the stratascratch team show us how this question can be solved using lag and lead . I got a same question in a grubhub interview and they mentioned can you please use lag and lead rather than this method. Would really appreciate the guidance? Thank you nate,2022-06-12T21:27:58Z
UgzCyERPD5KToN11imx4AaABAg.9cBbEJvYfXy9cG-Rb16bbl,@stratascratch,UgzCyERPD5KToN11imx4AaABAg,2,VEjxlKBkZGM,0,0,2022-06-14T14:25:12Z,"Hi yuthpati rathi,<br><br>Sorry, but I can&#39;t imagine a good and robust solution with lag or lead functions. If streak is == 5, then you would need to have 4 lagged rows along with current row to determine streak is 5. But if it&#39;s 6 then you&#39;d need 5 lagged rows along with the current row. Here is another solution that is maybe more clear:<br><br>-- CTE: calculate streak_end variable that is 1 when loss happens<br>-- CTE2: use window function sum of streak_end variable to determine streak_id<br>-- CTE_STREAK: COUNT player, streak id combinations<br>-- OUTER QUERY: output players with longest streak<br>WITH cte AS<br>  (SELECT player_id,<br>          match_date,<br>          match_result,<br>          CASE<br>              WHEN match_result = &#39;L&#39; THEN 1<br>              ELSE 0<br>          END AS streak_end<br>   FROM players_results<br>   ORDER BY 1,<br>            2),<br>     cte2 AS<br>  (SELECT *,<br>          SUM(streak_end) OVER(PARTITION BY player_id<br>                               ORDER BY match_date) AS streak_id<br>   FROM cte),<br>     cte_streak AS<br>  (SELECT player_id,<br>          count(*) AS streak<br>   FROM cte2<br>   WHERE match_result = &#39;W&#39;<br>   GROUP BY player_id,<br>            streak_id)<br>SELECT player_id,<br>       streak<br>FROM cte_streak<br>WHERE streak =<br>    (SELECT max(streak)<br>     FROM cte_streak)<br><br>Lag and lead functions can only work if you already know the longest streak and you just need to determine the player/entity with that streak.",2022-06-14T14:25:12Z
UgzCyERPD5KToN11imx4AaABAg.9cBbEJvYfXy9e0NqXFpsyV,@emiliogarza6446,UgzCyERPD5KToN11imx4AaABAg,2,VEjxlKBkZGM,0,0,2022-07-28T05:53:24Z,"@@stratascratch I think I might&#39;ve come up with a solution with lead and lag that works without previously knowing the longest streak, I&#39;d really appreciate it if I could get feedback/criticism on it! It&#39;s like 25 lines without comments. Really loving the premium questions, btw. Much better than leetcode :)<br><br><br>--Creates a table that replaces dates with an ordered match number <br>WITH match_number AS ( <br>    SELECT player_id, match_result, ROW_NUMBER() OVER (PARTITION BY player_id ORDER BY match_date) AS match_number<br>    FROM players_results<br>),<br><br>-- Adds a column that integrates results of upcoming matches into the respective records<br>next_match AS ( <br>    SELECT *, LEAD(match_result) OVER(PARTITION BY player_id) AS next_match_result<br>    FROM match_number<br>    <br>),<br><br>-- Adds a boolean value to determine if the next or previous rows are consecutive<br>-- by using lead() and lag() to check neighboring match numbers. Becomes<br>-- meaningful in the next CTE when checking for consecutive wins.<br>next_consecutive_bool AS( <br>    SELECT *, LEAD(match_number) OVER (PARTITION BY player_id) - match_number = 1 OR<br>    LAG(match_number) OVER (PARTITION BY player_id) - match_number = -1 AS next_conse<br>    FROM next_match<br>    WHERE match_result = next_match_result<br>),<br><br>-- Filters the results to exclude losses. Using a ROW_NUMBER() function, this CTE adds<br>-- a column that serves to identify consecutive wins by using the boolean value &quot;next_conse&quot;<br>-- from the previous CTE.<br>--   <br>-- ¬ª If &quot;next_conse&quot; column = FALSE, this indicates a pair of consecutive wins only<br>-- ¬ª If next_conse is TRUE, this indicates a presence of 3 or more consecutive wins, with the<br>--   total number of wins being determined by the maximum row_num of that group. <br>--   e.g. id 402 has a MAX row_num of 4, indicating 5 consecutive wins (row_num + 1)<br>final_result_table AS ( <br>    SELECT *, ROW_NUMBER() OVER (PARTITION BY player_id, next_conse ORDER BY player_id, match_number, next_conse DESC) AS row_num<br>    FROM next_consecutive_bool<br>    WHERE match_result = &#39;W&#39;<br>)<br><br>-- Retrieves the player(s) who had the maximum amount of consecutive wins, determined<br>-- by &quot;MAX(row_num) from final_results_table&quot;. Adds +1 to the row_num to properly display<br>-- the result. This +1 exists because our next_consecutive_bool table filters out the first<br>-- match as to not get any non-consecutive pairs.<br>SELECT player_id, row_num + 1 AS win_streak<br>FROM final_result_table<br>WHERE row_num = ((SELECT MAX(row_num) FROM final_result_table))",2022-07-28T05:56:04Z
UgzCyERPD5KToN11imx4AaABAg.9cBbEJvYfXy9enFuMcO09h,@oscararmandocisnerosruvalc8503,UgzCyERPD5KToN11imx4AaABAg,2,VEjxlKBkZGM,0,0,2022-08-16T14:46:02Z,"I tried everything but common patterns can be repeated, hence, it¬¥s really difficult.<br>Even if you are labeling your data and you use comparison through those windows functions it¬¥s really hard",2022-08-16T14:46:02Z
UgzpbL9An9mJF1ajtE54AaABAg,@Isaiah094,,1,VEjxlKBkZGM,2,0,2022-03-24T01:19:53Z,"The second code example was hell. All in all a really tough question to follow, I‚Äôll just have to practice myself",2022-03-24T01:19:53Z
UgzpbL9An9mJF1ajtE54AaABAg.9ZwSNxbgtgD9ZxxpagzBzn,@stratascratch,UgzpbL9An9mJF1ajtE54AaABAg,2,VEjxlKBkZGM,0,0,2022-03-24T15:22:39Z,Sorry to hear. It&#39;s a tough question overall. We&#39;ll try to break it down with more detail next time!,2022-03-24T15:22:39Z
UgzpbL9An9mJF1ajtE54AaABAg.9ZwSNxbgtgD9ZyMsQL5z8h,@Isaiah094,UgzpbL9An9mJF1ajtE54AaABAg,2,VEjxlKBkZGM,0,0,2022-03-24T19:10:13Z,@@stratascratch you guys did well! It was just hell for me. Like I‚Äôm saying it‚Äôs a tough question overall not that you guys are bad! Haha,2022-03-24T19:10:13Z
Ugxr8-HKhzmxqXhZhSZ4AaABAg,@namrata9908,,1,VEjxlKBkZGM,0,0,2022-03-18T21:58:58Z,Very detailed explanation. Thanks!,2022-03-18T21:58:58Z
Ugwi4xunA7xBj3Z2AEZ4AaABAg,@supriya1316,,1,VEjxlKBkZGM,0,0,2022-02-16T13:35:04Z,Thanks for your video üëç,2022-02-16T13:35:04Z
UgzZQBcOStYfEJPUNe54AaABAg,@danchensun9686,,1,VEjxlKBkZGM,1,0,2022-02-15T19:10:14Z,thank you for the clear explanation !,2022-02-15T19:10:14Z
UgzZQBcOStYfEJPUNe54AaABAg.9YU5TA4QCRh9kV-HY2e0Fy,@stratascratch,UgzZQBcOStYfEJPUNe54AaABAg,2,VEjxlKBkZGM,0,0,2023-01-05T06:23:35Z,Glad it was helpful!,2023-01-05T06:23:35Z
UgzVDixF4OBTV3zIhAt4AaABAg,@johnwig285,,1,VEjxlKBkZGM,1,1,2022-02-12T19:16:34Z,"I don&#39;t understand why they would ask this instead of logistics related questions like number of deliveries, shortest distance etc which would be more practical",2022-02-12T19:16:34Z
UgzVDixF4OBTV3zIhAt4AaABAg.9YMNnr3cOVY9YMbt3Q3P8W,@stratascratch,UgzVDixF4OBTV3zIhAt4AaABAg,2,VEjxlKBkZGM,0,0,2022-02-12T21:28:21Z,"They probably do ask those questions too. I&#39;ve experienced simple, practice questions and longer, more complex questions later in the interview series.",2022-02-12T21:28:21Z
UgyZkaUmrIIAGHWMtut4AaABAg,@TheRaju991,,1,VEjxlKBkZGM,0,1,2022-02-11T05:55:37Z,This was a pretty tough question!! Enjoyed it greatly.,2022-02-11T05:55:37Z
UgzRGH9ZrJy8JMUrRlF4AaABAg,@siddhantkumar6145,,1,VEjxlKBkZGM,0,0,2022-02-10T19:06:31Z,"I have some questions of SQL, how can i get the answers of those. can you make videos on that questions. and how can i send you those questions.",2022-02-10T19:06:31Z
Ugxn-z3BNBmGTJTPSB54AaABAg,@alfatmiuzma,,1,VEjxlKBkZGM,2,2,2022-02-08T15:25:22Z,Very good explanation üëåüëå..love to see more such videos..maybe some medium level questions üòä,2022-02-08T15:25:22Z
Ugxn-z3BNBmGTJTPSB54AaABAg.9YBfA06-zf79YBnwf32WPP,@stratascratch,Ugxn-z3BNBmGTJTPSB54AaABAg,2,VEjxlKBkZGM,0,1,2022-02-08T16:42:03Z,More coming in the medium to hard level!,2022-02-08T16:42:03Z
Ugxn-z3BNBmGTJTPSB54AaABAg.9YBfA06-zf79YBrO46tcjD,@alfatmiuzma,Ugxn-z3BNBmGTJTPSB54AaABAg,2,VEjxlKBkZGM,0,0,2022-02-08T17:12:08Z,@@stratascratch Thanks for considering my request.,2022-02-08T17:12:08Z
Ugz3jil8GkgJUGLmSz14AaABAg,@guinhaa,,1,VEjxlKBkZGM,1,0,2022-02-08T05:40:54Z,Superb video! I would love to see this same format of video but focusing on a Interview for a Data Analyst position,2022-02-08T05:40:54Z
Ugz3jil8GkgJUGLmSz14AaABAg.9YAcHIqpVFC9YBnyODCkyw,@stratascratch,Ugz3jil8GkgJUGLmSz14AaABAg,2,VEjxlKBkZGM,0,1,2022-02-08T16:42:17Z,We&#39;ll have a few more coming out soon. Glad you like our new format.,2022-02-08T16:42:17Z
UgwO6YeKcvAi2Nca1654AaABAg,@shanaebrown1563,,1,VEjxlKBkZGM,0,7,2022-02-08T05:12:08Z,Is it weird that i want to build my skills and work for strata scratch I love the website . Keep it up guys I‚Äôm coming to work for you guys in the future lol,2022-02-08T05:12:30Z
UgxgZY6F1HsvW8O0hu14AaABAg,@ClaudioBOsorio,,1,VEjxlKBkZGM,0,0,2022-02-08T03:36:53Z,Great! Thanks for the video,2022-02-08T03:36:53Z
Ugx2tzKoZxrLCNKUDvR4AaABAg,@VloggingMemories,,1,hMUf7DqG1nQ,1,1,2022-11-16T20:31:42Z,"My Solution:<br><br>select country.country_name,<br>city.city_name,<br>count(distinct cust.id) cust_cnt<br>from linkedin_customers cust<br>inner join linkedin_city city on cust.city_id=<a href=""http://city.id/"">city.id</a><br>inner join linkedin_country country on city.country_id=<a href=""http://country.id/"">country.id</a><br>group by 1, 2;",2022-11-16T20:31:42Z
Ugx2tzKoZxrLCNKUDvR4AaABAg.9iVlaV52ams9iXg4xCPuqx,@stratascratch,Ugx2tzKoZxrLCNKUDvR4AaABAg,2,hMUf7DqG1nQ,0,0,2022-11-17T14:22:02Z,Wonderful!  Thanks for sharing.,2022-11-17T14:22:02Z
UgzzH67469YpOahP7PJ4AaABAg,@Isaiah094,,1,hMUf7DqG1nQ,0,0,2022-03-24T00:10:55Z,Why don‚Äôt you join all tables to the first table? Or does you joining Table A to Table B then table B to table C technically create a join from table A to C?,2022-03-24T00:10:55Z
Ugws9ucTToMdw679OUh4AaABAg,@nidhityagi5390,,1,hMUf7DqG1nQ,0,1,2022-01-28T21:34:42Z,Can I have the exact solution of the whole query,2022-01-28T21:34:42Z
UgzCdPpbVX5L1n_T5hd4AaABAg,@Vintagetube310,,1,hMUf7DqG1nQ,1,1,2022-01-27T08:37:17Z,So is this channel still belonging to Nate? Im confused. Great video by the way,2022-01-27T08:37:17Z
UgzCdPpbVX5L1n_T5hd4AaABAg.9Xh1vc1-l-79Xhvi6uZ9lG,@stratascratch,UgzCdPpbVX5L1n_T5hd4AaABAg,2,hMUf7DqG1nQ,0,3,2022-01-27T16:53:31Z,"Yeah it&#39;s still me! But I&#39;ve been trying to pump out more content and having my team help out. I hope to still produce videos (with my face) once a month. But my team will also be making videos and releasing 1-2x a month so that we get to about 2-3x videos a month. But don&#39;t worry, I&#39;m still heavily involved in the writing and production of the videos!",2022-01-27T16:53:31Z
UgyhPkVIXPWoYZPXtu14AaABAg,@muni7561,,1,hMUf7DqG1nQ,0,0,2022-01-27T08:34:31Z,This kind of video is so helpful thank you!,2022-01-27T08:34:31Z
Ugzn2C6Knx3YJOetWxR4AaABAg,@bhavyarajdev676,,1,hMUf7DqG1nQ,0,1,2022-01-27T06:35:37Z,Hello sir. I just want to say thank you for your video on &quot;The only data science project you need..&#39;&#39;. That was the best direction showing video for me. I&#39;m currently working on the project and it&#39;s half over now. Thank you üòä,2022-01-27T06:35:37Z
UgzNaUNHqakO3LRtLxl4AaABAg,@waskjohnson2033,,1,30hS-MjpU6E,1,0,2023-08-21T02:38:40Z,"Thank you as this helped me think differently around optimization, and that it&#39;s more intuitive than big and scary.<br><br>At work I review a lot of other&#39;s queries (internal &amp; client) and code readability is <b>always</b> an issue. Drives me insane as I always have to rewrite/reformat them before I even bother trying to understand them.",2023-08-21T02:38:40Z
UgzNaUNHqakO3LRtLxl4AaABAg.9tefm_q6SX69thUO5IfuKT,@stratascratch,UgzNaUNHqakO3LRtLxl4AaABAg,2,30hS-MjpU6E,0,0,2023-08-22T04:48:02Z,Great to hear!,2023-08-22T04:48:02Z
Ugw7f15p-c4B6GTGhs54AaABAg,@luckychitundu1070,,1,30hS-MjpU6E,0,0,2023-05-07T23:11:43Z,Hi @ Nate!<br>Does StrataScratch have a shortcut for formatting your SQL code?,2023-05-07T23:11:43Z
UgzjSm22gv93ApVdjRx4AaABAg,@VloggingMemories,,1,30hS-MjpU6E,0,1,2022-11-13T19:05:34Z,"My Solution:<br><br>with user_table as(<br>select t1.user_id, t2.paying_customer<br>from ms_user_dimension t1<br>inner join ms_acc_dimension t2 on t1.acc_id=t2.acc_id<br>),<br>final_res as(<br>select distinct <a href=""http://t1.date/"">t1.date</a>, <br>    t2.paying_customer,<br>    sum(downloads) over (partition by date, paying_customer order by date) sum_downloads<br>from ms_download_facts t1<br>inner join user_table t2 on t1.user_id=t2.user_id)<br>select <a href=""http://t1.date/"">t1.date</a>,<br>t1.sum_downloads as &quot;non-paying downloads&quot;,<br>t2.sum_downloads as &quot;paying downloads&quot;<br>from (select * from final_res where paying_customer=&#39;no&#39;) t1, (select * from final_res where paying_customer=&#39;yes&#39;) t2 <br>where t1.date=<a href=""http://t2.date/"">t2.date</a> and t1.sum_downloads&gt;t2.sum_downloads <br>order by 1 desc;",2022-11-13T19:05:34Z
UgwsZ1btReBbm8oKRJ54AaABAg,@agnespitka3703,,1,30hS-MjpU6E,0,2,2022-10-21T07:55:36Z,Thanks. I learned a lot. I&#39;m pretty new to the industry. I am working with SQL every day so videos that start with &quot;what is SQL&quot; and &quot;this is a SELECT statement&quot; are really not what I need but I am not in the level to write good/optimal code. The video was exactly what I needed. And not too long and not too short. Everything structured well.,2022-10-21T07:55:36Z
Ugzg_9qvhpLyJ074Bpt4AaABAg,@elleandish,,1,30hS-MjpU6E,1,0,2022-07-15T00:48:44Z,"nate!!! i was looking up sql videos and thought i saw a familiar face from high school üòÜ nice channel, very helpful!  i‚Äôm not doing data science by title but wanted to get better at sql. üíñ janelle",2022-07-15T00:48:44Z
Ugzg_9qvhpLyJ074Bpt4AaABAg.9dUMdgypB0E9dUjOXc7d0a,@stratascratch,Ugzg_9qvhpLyJ074Bpt4AaABAg,2,30hS-MjpU6E,0,0,2022-07-15T04:16:14Z,"Wow what a blast from the past. You got a great channel yourself. Way better than mine! If you want to learn SQL, check out my platform =) We&#39;ll start resuming the SQL videos next month so I hope you check back. Also, LeetCode is another great platform with a SQL component.",2022-07-15T04:16:14Z
UgxVlPJKJeCjpLaqWEF4AaABAg,@ManishaParmar,,1,30hS-MjpU6E,0,0,2022-05-26T03:28:00Z,"Really great optimizing techniques! These are some of the key things that I keep in mind while writing my solutions as well, and I like how you explain every step.",2022-05-26T03:28:00Z
UgxhBk2zzp4EdIxS89t4AaABAg,@CruiserPup,,1,30hS-MjpU6E,0,0,2022-05-08T06:28:59Z,I&#39;ve learned so much from your videos and my stratascratch premium subscription in the past year.  Thank you thank you thank you! :),2022-05-08T06:28:59Z
UgywCuaAD8lYsqGFC_N4AaABAg,@insider-training1439,,1,30hS-MjpU6E,0,0,2022-04-13T20:57:10Z,"Very helpful video Nate, thanks for doing this.",2022-04-13T20:57:10Z
Ugww41Nqu9-WWKuNMKV4AaABAg,@austinocampo2410,,1,30hS-MjpU6E,1,0,2022-03-16T15:45:24Z,Cleaner if you use an IF statement instead of CASE!,2022-03-16T15:45:24Z
Ugww41Nqu9-WWKuNMKV4AaABAg.9ZdP4EEBaDq9ZdZbwqPKL2,@stratascratch,Ugww41Nqu9-WWKuNMKV4AaABAg,2,30hS-MjpU6E,0,0,2022-03-16T17:17:31Z,IF statement in SQL?,2022-03-16T17:17:31Z
UgyJPBUrmxDjdlLxnv54AaABAg,@bocanegradev,,1,30hS-MjpU6E,0,1,2022-02-15T16:41:34Z,amazing! always it&#39;s better to understand best practices with examples like this... It would be awesome to see more like this,2022-02-15T16:41:34Z
UgzoItLqI60E05oIGOd4AaABAg,@dhanushph8170,,1,30hS-MjpU6E,1,0,2022-02-07T21:38:25Z,"Can anyone help me , which is the best site to learn the syntax for window functions",2022-02-07T21:38:25Z
UgzoItLqI60E05oIGOd4AaABAg.9Y9l3QUXVz59Y9nQ0qoMAN,@stratascratch,UgzoItLqI60E05oIGOd4AaABAg,2,30hS-MjpU6E,0,1,2022-02-07T21:58:59Z,Try mode analytics SQL tutorial for a free tutorial. That&#39;s how I learned how to write window functions. Then try StrataScratch to practice and get better. Good luck!,2022-02-07T21:58:59Z
UgydSL4c8uWgGwbdlst4AaABAg,@nandiniguntur4509,,1,30hS-MjpU6E,1,0,2022-01-31T20:23:38Z,"Hey Nate! Kudos to the great series. You are a good tutor. I have a request, can you please make a playlist for machine learning &amp; statistics with python? It would be extremely helpful.",2022-01-31T21:19:42Z
UgydSL4c8uWgGwbdlst4AaABAg.9Xsawa0qX699XscpCFGHdC,@stratascratch,UgydSL4c8uWgGwbdlst4AaABAg,2,30hS-MjpU6E,0,0,2022-01-31T20:40:06Z,We are definitely doing this soon!,2022-01-31T20:40:06Z
Ugwx-4R9uSi5gZXZOw94AaABAg,@diaconescutiberiu7535,,1,30hS-MjpU6E,3,0,2022-01-30T11:04:25Z,"Quick question (I&#39;ve just started with sql). From what i learned so far for case when...then ... statement, after the &quot;then&quot;  comes labeling (strings; all examples were using case when for binning). In your code i saw something else (dowlnds.downloads) ...what did you do here? (Is it also a labeling but with a downlds reference, or...?)",2022-01-30T11:04:25Z
Ugwx-4R9uSi5gZXZOw94AaABAg.9Xp18rfcwU19Xq0MnjAwxr,@stratascratch,Ugwx-4R9uSi5gZXZOw94AaABAg,2,30hS-MjpU6E,0,1,2022-01-30T20:16:49Z,"I&#39;m just referencing the ms_download_facts table. I&#39;m calling it downlds as you can see from the JOIN statement. So when I am referencing columns, SQL knows what table the column is in.",2022-01-30T20:16:49Z
Ugwx-4R9uSi5gZXZOw94AaABAg.9Xp18rfcwU19XrlsQz0n6Q,@diaconescutiberiu7535,Ugwx-4R9uSi5gZXZOw94AaABAg,2,30hS-MjpU6E,0,0,2022-01-31T12:39:57Z,"@@stratascratch Got it. I was able to replicate this on some other exercise. You are referencing the downloads (from the downlds) column so as SUM knows what to ... sum.<br>Can you please tell me, if we were to have 2 columns (something to sum and something to count) how would the case when look like? Would you just write 4 case when(s)? (2 with sums for paying &amp; non_paying and 2 with counts for paying &amp; non_paying)... or is it possible to optimize further into 2 case when(s)",2022-01-31T12:39:57Z
Ugwx-4R9uSi5gZXZOw94AaABAg.9Xp18rfcwU19XsEL0dIFo2,@stratascratch,Ugwx-4R9uSi5gZXZOw94AaABAg,2,30hS-MjpU6E,0,1,2022-01-31T16:57:23Z,@@diaconescutiberiu7535 It would be 4 cases just as you mentioned. You can&#39;t combine sum and counts when writing the cases. And you still need to split paying and non-paying so it&#39;s 4 cases in the end.,2022-01-31T16:57:23Z
Ugz-UKH-93k82JVZB6N4AaABAg,@jennajia9522,,1,30hS-MjpU6E,1,2,2022-01-29T03:54:58Z,"Hi Nate,  I cannot explain how much appreciation I have to you. Your video is extremely helpful and I have almost watch every single SQL one. All of the questions you explain in a clear way and the platform you built out is also the best. You are my best SQL teacher I want to say. Thank you so much. Keep up making these, we love it.",2022-01-29T03:54:58Z
Ugz-UKH-93k82JVZB6N4AaABAg.9XlgCacDx8S9XmtrPiCi2m,@stratascratch,Ugz-UKH-93k82JVZB6N4AaABAg,2,30hS-MjpU6E,0,1,2022-01-29T15:13:30Z,Thanks so much for the kind words! I&#39;ll keep making more content! You&#39;ll also see much more from my team. We&#39;re hoping to release a lot more videos this year!,2022-01-29T15:13:30Z
Ugy9q9aJyfJ_CvqJN-B4AaABAg,@alfatmiuzma,,1,30hS-MjpU6E,0,1,2022-01-11T18:12:21Z,"Loved this video‚ù§Ô∏è. Everything is on point! I really liked the optimization techniques using WITH, CASE statements! Looking forward to more videos on SQL. Thanks a tonüòä",2022-01-11T18:12:21Z
Ugyzcs8XP9kewlQQ8B54AaABAg,@dmitriyp3702,,1,30hS-MjpU6E,1,0,2022-01-09T20:36:23Z,"Hello. I prefer to use USING, instead of ON in INNER JOIN string. Do think it&#39;s a good idea?",2022-01-09T20:36:23Z
Ugyzcs8XP9kewlQQ8B54AaABAg.9WzyulBmZcE9X-AyQqHiag,@stratascratch,Ugyzcs8XP9kewlQQ8B54AaABAg,2,30hS-MjpU6E,0,1,2022-01-09T22:30:29Z,"Not many people I know use USING instead of ON so I can&#39;t really comment on if it&#39;s a good idea or not. Out of everyone I&#39;ve ever interviewed for a job, I don&#39;t think anyone has ever used USING.",2022-01-09T22:30:29Z
UgwChT8bglNiG5alyKl4AaABAg,@rakibraihan1572,,1,30hS-MjpU6E,0,0,2022-01-09T16:07:01Z,great,2022-01-09T16:07:01Z
UgzInLkPchk7BlYQ_-x4AaABAg,@StanleySI,,1,30hS-MjpU6E,2,2,2022-01-08T14:35:41Z,"Hi Nate, are you sure we can use where clause after group by clause ?  [HAVING vs WHERE: (<a href=""https://www.youtube.com/watch?v=30hS-MjpU6E&amp;t=12m48s"">12:48</a>)]",2022-01-08T14:35:41Z
UgzInLkPchk7BlYQ_-x4AaABAg.9Wwkq-ptgDw9Wx7FpUYoaI,@stratascratch,UgzInLkPchk7BlYQ_-x4AaABAg,2,30hS-MjpU6E,0,0,2022-01-08T18:00:19Z,Nope! I made a mistake! I should have ran the code to double check to see if it worked. All you have to do is place the WHERE clause before the GROUP BY and it&#39;s fixed.,2022-01-08T18:00:19Z
UgzInLkPchk7BlYQ_-x4AaABAg.9Wwkq-ptgDw9WxcObd5pMu,@StanleySI,UgzInLkPchk7BlYQ_-x4AaABAg,2,30hS-MjpU6E,0,0,2022-01-08T22:41:08Z,"@@stratascratch The question asked &#39;Include only records where non-paying customers have more downloads than paying customers&quot; - this may imply we need to use WHERE clause first to filter individual records, then aggregate results with GROUP BY date.",2022-01-08T22:41:08Z
Ugz9_5_yu_VAMTUIkMB4AaABAg,@tran3490,,1,30hS-MjpU6E,0,0,2022-01-08T04:46:54Z,Super helpful! Thank you!,2022-01-08T04:46:54Z
Ugy-OcGjTrA3d0ceZkt4AaABAg,@zukeplastic,,1,30hS-MjpU6E,0,0,2022-01-04T19:21:18Z,great content!,2022-01-04T19:21:18Z
UgxRu6_xsIBWzwKMLuB4AaABAg,@totleariss,,1,30hS-MjpU6E,1,0,2022-01-01T06:39:01Z,"Love this video! I‚Äôm at the point where I can get through a lot of the problems, but the code is a mess and way longer than the optimized solution. It‚Äôs been a challenge to ‚Äúthink in SQL‚Äù to write a better code to begin with.",2022-01-01T06:39:01Z
UgxRu6_xsIBWzwKMLuB4AaABAg.9Wdsigs7BsD9WfL4MgHDLA,@stratascratch,UgxRu6_xsIBWzwKMLuB4AaABAg,2,30hS-MjpU6E,0,1,2022-01-01T20:14:46Z,That&#39;ll come with time and experience. The initial code is always a mess. Don&#39;t worry too much about it. Just optimize at a later time!,2022-01-01T20:14:46Z
UgxYBfcFF__JN3YQ0T14AaABAg,@bien.papachin,,1,30hS-MjpU6E,1,3,2022-01-01T02:08:46Z,"great video but as a a rule of thumb run the SQL to see if it works, for example the &#39;where&#39; clause at the last select was left after the &#39;group by&#39; , nonetheless the explanation is great! thanks for the video... cheers",2022-01-01T02:08:46Z
UgxYBfcFF__JN3YQ0T14AaABAg.9WdOnLum3D39WfKzXr44G3,@stratascratch,UgxYBfcFF__JN3YQ0T14AaABAg,2,30hS-MjpU6E,0,2,2022-01-01T20:13:58Z,Yea you&#39;re right. I totally forgot to run the SQL query at the end to see if it worked. =),2022-01-01T20:13:58Z
UgxoX3XUuFTE1YigHrJ4AaABAg,@sagarmgandhi,,1,30hS-MjpU6E,0,1,2021-12-29T06:02:51Z,I think I could apply all this when I write my queries. Good session,2021-12-29T06:02:51Z
Ugz3flZjGJ1qpaC9Sdh4AaABAg,@ahyoungkim8256,,1,30hS-MjpU6E,0,0,2021-12-27T07:20:10Z,"i love this contents, this is so useful to look back our codes! thank you so much Nate !",2021-12-27T07:20:10Z
Ugx1A3Z_RGbMnFQ2X4t4AaABAg,@israelgonzalez677,,1,30hS-MjpU6E,0,0,2021-12-25T07:09:00Z,In love with your videos. I am enjoying them a lot. Greetings from MX! üòÄ,2021-12-25T07:09:00Z
UgzdQOL2lRZidkIivjF4AaABAg,@user-dl3qr5hm3t,,1,30hS-MjpU6E,2,4,2021-12-25T04:56:49Z,"Do we really need ‚Äúgroup by date, n_nonpaying, n_paying‚Äù in the last query? cte returns data already grouped by date: one row - one date. <br>PS. Surprised that it‚Äôs on Hard level. Seems pretty basic.",2021-12-25T04:56:49Z
UgzdQOL2lRZidkIivjF4AaABAg.9WMfT3IgZSE9WNugDDLwuX,@stratascratch,UgzdQOL2lRZidkIivjF4AaABAg,2,30hS-MjpU6E,0,3,2021-12-25T16:29:04Z,"The groupby only facilitates any de-duping needed. I wouldn&#39;t expect any duplicates so it&#39;s not needed. The groupby might just be an artifact of some other code that was refactored. Agree with you that it&#39;s a basic question that would be asked on an interview. Probably a medium level question tbh. But with all the CTEs with joins and case statement, we graded it hard.",2021-12-25T16:29:04Z
UgzdQOL2lRZidkIivjF4AaABAg.9WMfT3IgZSE9WO-SqsyXUm,@user-dl3qr5hm3t,UgzdQOL2lRZidkIivjF4AaABAg,2,30hS-MjpU6E,0,0,2021-12-25T17:19:32Z,"@@stratascratch Thanks for prompt response, Nate. Highly appreciate what you do on your channel.",2021-12-25T17:19:32Z
Ugw3HCkN19Mfx5r4l1B4AaABAg,@bandhammanikanta6302,,1,30hS-MjpU6E,0,1,2021-12-24T16:12:03Z,"I really appreciate your efforts, Nate.<br><br>but please do improve the title of the video according to the question,.",2021-12-24T16:12:03Z
UgwPikH5WH1EgdzW72R4AaABAg,@henrold6228,,1,30hS-MjpU6E,0,0,2021-12-23T17:13:02Z,nate you are my best friend,2021-12-23T17:13:02Z
UgymlxxMhGyZpZUzX3R4AaABAg,@dianadennis7225,,1,30hS-MjpU6E,0,0,2021-12-23T05:49:43Z,Such a great video!,2021-12-23T05:49:43Z
UgwJJzC-kZ7s1V1nF1J4AaABAg,@parantikaghosh4396,,1,30hS-MjpU6E,0,0,2021-12-23T00:02:27Z,"This is an amazing video, it is sooooooooooo helpful!! :)",2021-12-23T00:02:27Z
UgzYZIyEUD2-2A7HcZl4AaABAg,@abaji434,,1,30hS-MjpU6E,0,0,2021-12-22T13:16:41Z,great!,2021-12-22T13:16:41Z
Ugzu0C6pyoUOFIPMrX14AaABAg,@Sanatos98,,1,30hS-MjpU6E,0,1,2021-12-22T11:00:13Z,I love your videos! Hope you get the recognition you deserve brother,2021-12-22T11:00:13Z
UgyvkImgyMARiDU6MdB4AaABAg,@weiyang8650,,1,30hS-MjpU6E,2,7,2021-12-22T07:57:07Z,"thank you for your video, but WHERE should be put before the GROUP BY clause, right?",2021-12-22T07:57:07Z
UgyvkImgyMARiDU6MdB4AaABAg.9WFGhxhDAVr9WFbb38l_bC,@Sanatos98,UgyvkImgyMARiDU6MdB4AaABAg,2,30hS-MjpU6E,0,3,2021-12-22T11:08:25Z,Yes!,2021-12-22T11:08:25Z
UgyvkImgyMARiDU6MdB4AaABAg.9WFGhxhDAVr9WGCZjZYMxg,@stratascratch,UgyvkImgyMARiDU6MdB4AaABAg,2,30hS-MjpU6E,0,4,2021-12-22T16:40:09Z,Whoops. Yes! haha sorry about that.,2021-12-22T16:40:09Z
UgzqhTHlXUZ2pfDYfFl4AaABAg,@BBBBBBAAAl,,1,30hS-MjpU6E,0,0,2021-12-22T07:21:08Z,perfect,2021-12-22T07:21:08Z
UgzBic3dV-2aAjA6BMZ4AaABAg,@rishikeshbharti5064,,1,30hS-MjpU6E,0,0,2021-12-22T06:49:20Z,GREAT VIDEOüî•üî•,2021-12-22T06:49:20Z
UgyvjYuYzTneZ934OHV4AaABAg,@raxmatillomaribjonov3787,,1,30hS-MjpU6E,0,0,2021-12-22T03:35:00Z,thanksüëç,2021-12-22T03:35:00Z
Ugzru1He8I4kmScFCsp4AaABAg,@sunnygawande5283,,1,GeJUvdkJKEc,1,0,2022-12-28T19:10:12Z,"what if i come with last solution only in an interview, then what can i say about optimization?",2022-12-28T19:10:12Z
Ugzru1He8I4kmScFCsp4AaABAg.9kBlePNkadF9kEQTksxOP9,@stratascratch,Ugzru1He8I4kmScFCsp4AaABAg,2,GeJUvdkJKEc,0,1,2022-12-29T19:54:06Z,If you don&#39;t think it can be optimized then say so. Maybe talk about other ways of solving the question if the interviewers are interested. The goal is to communicate your knowledge on how SQL works.,2022-12-29T19:54:06Z
Ugx2L9TYOzHs4tnIStZ4AaABAg,@VloggingMemories,,1,GeJUvdkJKEc,1,1,2022-11-16T20:30:42Z,"My Solution:<br><br>with fin_table as<br>(SELECT to_char(created_at, &#39;YYYY-MM&#39;) as year_month,<br>sum(purchase_amt) as sum_purchase_amt,<br>lag(sum(purchase_amt)) over (order by to_char(created_at, &#39;YYYY-MM&#39;)) as prev_month,<br>lag(sum(purchase_amt), 2) over (order by to_char(created_at, &#39;YYYY-MM&#39;)) as prev_to_prev_month<br>FROM amazon_purchases<br>where purchase_amt&gt;0<br>group by 1<br>order by to_char(created_at, &#39;YYYY-MM&#39;))<br>select year_month, <br>round((sum_purchase_amt+prev_month+prev_to_prev_month)/3, 2) as rolling_average <br>from fin_table<br>where prev_month is not null<br>and prev_to_prev_month is not null<br>order by 1;",2022-11-16T20:30:42Z
Ugx2L9TYOzHs4tnIStZ4AaABAg.9iVlUDK3REu9iXgKooqYgC,@stratascratch,Ugx2L9TYOzHs4tnIStZ4AaABAg,2,GeJUvdkJKEc,0,0,2022-11-17T14:24:12Z,Thank you for sharing!,2022-11-17T14:24:12Z
UgzMcCr67adgSS_bjgR4AaABAg,@papilpatil,,1,GeJUvdkJKEc,0,0,2022-09-17T17:02:50Z,LAG would have been easier?,2022-09-17T17:02:50Z
UgxfwOd6671_PzGfzD54AaABAg,@sanaayakurup5453,,1,GeJUvdkJKEc,1,1,2022-09-02T10:12:55Z,"a little long but I used this!-select date,sum(total+second_lag+first_lag)/3 as rolling_avg from(<br>select *, lag(total,2) over (order by date) as second_lag,<br>lag(total) over (order by date) as first_lag<br>from (select date_format(created_at,&quot;%Y-%m&quot;) as date,sum(purchase_amt) as total from amazon_purchases where purchase_amt&gt;=0 group by date) as a) as b<br>group by date",2022-09-02T10:12:55Z
UgxfwOd6671_PzGfzD54AaABAg.9fTY9oz0kAX9fTjWWmXT8u,@stratascratch,UgxfwOd6671_PzGfzD54AaABAg,2,GeJUvdkJKEc,0,0,2022-09-02T12:00:52Z,That is awesome!  Thanks for sharing.,2022-09-02T12:00:52Z
UgzKiUB4fx_6J_mA9mZ4AaABAg,@tamalchandra3427,,1,GeJUvdkJKEc,1,0,2022-07-27T12:15:37Z,Nice collection of problems .. good stuff.,2022-07-27T12:15:37Z
UgzKiUB4fx_6J_mA9mZ4AaABAg.9dzUn3kTlbL9e1PHSpatvA,@stratascratch,UgzKiUB4fx_6J_mA9mZ4AaABAg,2,GeJUvdkJKEc,0,0,2022-07-28T15:25:11Z,Glad you enjoyed.  Thanks.,2022-07-28T15:25:11Z
UgwbB-F6cYYa-J1i0pd4AaABAg,@bodwiser100,,1,GeJUvdkJKEc,1,0,2022-07-11T05:43:41Z,"For solution #1, what if you used left join instead of the default (inner) join? Would that solve the problem of missing January and February in your output?",2022-07-11T05:43:41Z
UgwbB-F6cYYa-J1i0pd4AaABAg.9dKaDTaPyh99efRau_KyhZ,@stratascratch,UgwbB-F6cYYa-J1i0pd4AaABAg,2,GeJUvdkJKEc,0,0,2022-08-13T13:54:19Z,"Hey Rajat!  Only partially. While using the LEFT JOIN would get you January and February in the column, the average for these months will be NULL. This could be accepted as a solution. However, the average for January and February has to be shown for the solution to be accepted on the platform. For January, it will equal the January revenue. For February, it will be an average of January and February revenues.",2022-08-13T13:54:19Z
Ugy-bIXZznpFG4O_m-x4AaABAg,@purnimasharma71,,1,GeJUvdkJKEc,0,0,2022-06-23T11:54:17Z,"My approach with only one join in MSSQL server : <br><br>with CTE as (select left(created_at, 7)year_mon, sum(purchase_amt) su from amazon_purchases<br>where purchase_amt&gt;=0<br>group by left(created_at, 7))<br><br>select C1.year_mon,AVG(C2.su) from CTE C1<br>left join CTE C2<br>on  right(C1.year_mon,2)-right(C2.year_mon,2)&lt;=2<br>where right(C1.year_mon,2)&gt;=right(C2.year_mon,2)<br>group by 1<br>order by 1",2022-06-23T11:54:17Z
UgwJ9HzO-1gBG9GAhi14AaABAg,@purnimasharma71,,1,GeJUvdkJKEc,0,0,2022-06-23T11:41:52Z,Why not you use left join in the first approach  to get month 1 and 2 ?,2022-06-23T11:41:52Z
UgxeCPKk-btUuoeMvOt4AaABAg,@friendsplain,,1,GeJUvdkJKEc,1,0,2022-05-21T16:47:33Z,"Hello Nate, huge fan of your content!<br><br>Just wondering, from your monthly revenue CTE, wouldn&#39;t applying LAG function to generate a last_month_revenue and 2mos_ago_revenue also give the 3 months of revenue needed to generate a 3 month rolling average?   This would help avoid the situation of 2 self-joins to generate those columns",2022-05-21T16:47:33Z
UgxeCPKk-btUuoeMvOt4AaABAg.9bISeB1s39K9bQZLMDN90m,@stratascratch,UgxeCPKk-btUuoeMvOt4AaABAg,2,GeJUvdkJKEc,0,1,2022-05-24T20:19:56Z,"I think you&#39;re thinking about using the lag() function like this:<br><br>WITH revenues AS (<br>SELECT to_date(to_char(created_at::date, &#39;YYYY-MM&#39;),&#39;YYY-MM-01&#39;) AS month_year,<br>       SUM(purchase_amt) AS revenue_month<br>FROM amazon_purchases<br>WHERE purchase_amt&gt;=0<br>GROUP BY month_year<br>ORDER BY month_year)<br>SELECT *,<br>       (revenue_month + LAG(revenue_month, 1) OVER (ORDER BY month_year)+ LAG(revenue_month, 2) OVER (ORDER BY month_year))/3 AS rolling_avg<br>FROM revenues<br><br>The output is slightly different. It won&#39;t produce an average for the first two months. But it&#39;s not like it&#39;s a wrong solution either...it&#39;s just a minor nuance in the output.",2022-05-24T20:19:56Z
UgzuxuX57cJB6adPzlp4AaABAg,@caiyu538,,1,GeJUvdkJKEc,0,0,2022-03-11T15:01:29Z,Great series.,2022-03-11T15:01:29Z
UgzWrE28ZJgNWG6cJ0Z4AaABAg,@TheRaju991,,1,GeJUvdkJKEc,0,6,2022-01-18T07:41:00Z,Why didn&#39;t I find this channel sooner!<br>I always love a good SQL challenge.,2022-01-18T07:41:00Z
UgyjDnu1VLVtbYfcWCZ4AaABAg,@jerrywang1550,,1,GeJUvdkJKEc,0,0,2022-01-16T23:41:10Z,good content,2022-01-16T23:41:10Z
UgzS_8H8y1_ZEKVi4_V4AaABAg,@EverythingData98,,1,GeJUvdkJKEc,1,1,2021-11-08T16:57:16Z,"Thank you, watching these videos has helped me so much!",2021-11-08T16:57:16Z
UgzS_8H8y1_ZEKVi4_V4AaABAg.9UUwYZmLgb-9UUxueJrK7d,@stratascratch,UgzS_8H8y1_ZEKVi4_V4AaABAg,2,GeJUvdkJKEc,0,0,2021-11-08T17:09:09Z,Thanks for watching! Glad I could be of help.,2021-11-08T17:09:09Z
Ugx7iI0qAMWvC7Fe2-F4AaABAg,@advisorita,,1,GeJUvdkJKEc,3,3,2021-10-31T22:23:31Z,I think this window function below can do the magic too:<br>AVG(monthly_revenue) OVER (ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)<br>Let me know your thoughts.,2021-10-31T22:23:31Z
Ugx7iI0qAMWvC7Fe2-F4AaABAg.9UAvX6DDDic9UBRVXQchY6,@stratascratch,Ugx7iI0qAMWvC7Fe2-F4AaABAg,2,GeJUvdkJKEc,0,0,2021-11-01T03:11:39Z,"Yup! That works! I think I had this solution towards the end too, no?",2021-11-01T03:11:39Z
Ugx7iI0qAMWvC7Fe2-F4AaABAg.9UAvX6DDDic9UBXEm-rcwu,@advisorita,Ugx7iI0qAMWvC7Fe2-F4AaABAg,2,GeJUvdkJKEc,0,1,2021-11-01T04:01:48Z,@@stratascratch  HAHA You are right. You got it!  I literally commented half way in üòÖ,2021-11-01T04:01:48Z
Ugx7iI0qAMWvC7Fe2-F4AaABAg.9UAvX6DDDic9WPtzKtOR4n,@hemantsharma7986,Ugx7iI0qAMWvC7Fe2-F4AaABAg,2,GeJUvdkJKEc,0,0,2021-12-26T11:01:25Z,@@advisorita why would somebody do that... Lol,2021-12-26T11:01:25Z
UgwRaZV51_-FMHZZwNB4AaABAg,@barsyuksel9106,,1,GeJUvdkJKEc,4,2,2021-10-29T19:43:36Z,"Hi Nate! <br><br>Another brilliant and informing content as always.<br>Before watching your approach to the problem, I tried to solve it by myself.<br>I had followed the similar approach as your second one, however only difference was that I created two new columns representing the revenues of previous month  and that of two months before respectively by using lag function twice.<br>After watching your solution, I have been made aware of that the fact I could have done with only one line of code.<br><br>I was wondering does it lead to a delay matter in term of optimization if I got solution by applying two lag operation instead of solving it in your way. <br><br><br>Thanks<br>Below you can see my solution. <br><br>with revenues as (<br>select <br>to_char(created_at::date, &#39;YYYY-MM&#39;) as year_month,<br>sum(purchase_amt) as revenue<br>from amazon_purchases<br>where purchase_amt &gt;= 0<br>group by year_month<br>order by year_month)<br><br><br>select <br>year_month,<br>coalesce((revenue+previous_month_avg+two_month_ago_avg)/3,(revenue+previous_month_avg)/2,revenue,0) as rolling_revenue<br>from<br>(select *,<br>lag(revenue, 1) over (order by year_month) as previous_month_avg,<br>lag(revenue, 2) over (order by year_month) as two_month_ago_avg<br>from revenues ) b",2021-10-29T19:51:21Z
UgwRaZV51_-FMHZZwNB4AaABAg.9U5Ud86GzaS9U5a1kI6Y7n,@stratascratch,UgwRaZV51_-FMHZZwNB4AaABAg,2,GeJUvdkJKEc,0,1,2021-10-29T20:39:32Z,Great solution. I&#39;m assuming it validated on the platform? I&#39;m glad you watched the video and were able to figure it out!,2021-10-29T20:39:32Z
UgwRaZV51_-FMHZZwNB4AaABAg.9U5Ud86GzaS9U5b3EakcKX,@barsyuksel9106,UgwRaZV51_-FMHZZwNB4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-29T20:48:28Z,"@@stratascratch it worked smoothly.<br><br>I am assuming my solution would take longer as I apply coalesce, wouldn‚Äôt it?",2021-10-29T20:48:28Z
UgwRaZV51_-FMHZZwNB4AaABAg.9U5Ud86GzaS9U5bOF-UYdM,@stratascratch,UgwRaZV51_-FMHZZwNB4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-29T20:51:20Z,@@barsyuksel9106 Like a millisecond longer =). It&#39;s nothing that would need to be optimized.,2021-10-29T20:51:20Z
UgwRaZV51_-FMHZZwNB4AaABAg.9U5Ud86GzaS9U5b_OaImwe,@barsyuksel9106,UgwRaZV51_-FMHZZwNB4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-29T20:53:00Z,@@stratascratch perfect then. Thanks for the clarification :)<br>Looking forward to your new videos :),2021-10-29T20:53:00Z
Ugzykcw0K_kHigfoIyN4AaABAg,@shobhamourya8396,,1,GeJUvdkJKEc,1,2,2021-10-20T07:44:57Z,"Hi Nate.<br>Thanks for showing how to use aggregate window function without having to partition based on a column but rather by specifying the rows - it&#39;s super cool!<br>I solved using the lag window function and also using avg window function the way you have applied.<br><br>Here&#39;s my solution using the lag function:<br>with cte1<br>as<br>(<br>-- Extract the year_month from datetime  <br>    select *,<br>       to_char(cast(created_at as date), &#39;YYYY-MM&#39;) as year_month<br>    from amazon_purchases<br>    where purchase_amt &gt; 0<br>),<br>cte2<br>as<br>(<br>-- Calcualte the total revenue for each month<br>    select year_month,<br>       sum(purchase_amt) monthly_revenue<br>       from cte1<br>       group by year_month<br>       order by year_month<br>),<br>cte3<br>as<br>(<br>-- Find the prev_month and prev_to_prev_month revenue for each month<br>    select year_month,monthly_revenue curr_month_revenue,<br>       lag(monthly_revenue, 1, 0.0)  over(order by year_month) prev_month_revenue,<br>       lag(monthly_revenue, 2, 0.0) over(order by year_month)  prev_to_prev_month_revenue<br>    from cte2<br>)<br>-- Finally, calculate the three months rolling average<br>-- Note for first month previous two months revenue is not available<br>-- Note for second month only previous month revenue is available<br>select year_month, <br>    case when prev_to_prev_month_revenue &gt; 0.0 then<br>         (curr_month_revenue + prev_month_revenue + prev_to_prev_month_revenue)/3 <br>         when prev_month_revenue &gt; 0.0 then <br>         (curr_month_revenue + prev_month_revenue)/2 <br>         else curr_month_revenue <br>    end as three_month_rolling_avg<br>from cte3",2021-10-20T08:09:56Z
Ugzykcw0K_kHigfoIyN4AaABAg.9Ti1EkTjmPA9Ti4FRouGrY,@shobhamourya8396,Ugzykcw0K_kHigfoIyN4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-20T08:11:16Z,"Here&#39;s the solution using avg window function with rows specified:<br><br>with cte1<br>as<br>(<br>-- Extract the year_month from datetime  <br>    select *,<br>       to_char(cast(created_at as date), &#39;YYYY-MM&#39;) as year_month<br>    from amazon_purchases<br>    where purchase_amt &gt; 0<br>),<br>cte2<br>as<br>(<br>-- Calcualte the total revenue for each month<br>    select year_month,<br>       sum(purchase_amt) monthly_revenue<br>       from cte1<br>       group by year_month<br>       order by year_month<br>)<br>-- Calculate the avg by specifying the rows to consider <br>-- Note: Here we don&#39;t specify the column to partion by but rows to consider<br>select year_month, monthly_revenue, <br>      avg(monthly_revenue) <br>      over(order by year_month rows between 2 preceding  and current row) <br>      as three_month_rolling_avg<br>    from cte2",2021-10-20T08:11:16Z
UgzqTZZF41bay4UZK7V4AaABAg,@joaopedroreissilva7075,,1,GeJUvdkJKEc,2,1,2021-10-19T17:47:55Z,"Really good content and also at least for me, really tough question.",2021-10-19T17:47:55Z
UgzqTZZF41bay4UZK7V4AaABAg.9TgXS6pxW7q9TglVJRoFBh,@stratascratch,UgzqTZZF41bay4UZK7V4AaABAg,2,GeJUvdkJKEc,0,1,2021-10-19T19:59:26Z,Thanks for watching! And it actually is a tough question for anyone but especially to have this question on an interview.,2021-10-19T19:59:26Z
UgzqTZZF41bay4UZK7V4AaABAg.9TgXS6pxW7q9Tgud4kbvbg,@joaopedroreissilva7075,UgzqTZZF41bay4UZK7V4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-19T21:19:16Z,@@stratascratch haha really good point!,2021-10-19T21:19:16Z
Ugx4uqmleZfZ_Y48t_V4AaABAg,@Gabri3lsGCSalles,,1,GeJUvdkJKEc,1,0,2021-10-18T03:47:34Z,loving this,2021-10-18T03:47:34Z
Ugx4uqmleZfZ_Y48t_V4AaABAg.9TcSUI-XUkc9TdeT_r0a4I,@stratascratch,Ugx4uqmleZfZ_Y48t_V4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-18T15:00:18Z,Thanks for watching!,2021-10-18T15:00:18Z
Ugy9wckdQePtq0X5Ond4AaABAg,@user-ix7vs6vr7m,,1,GeJUvdkJKEc,1,2,2021-10-16T23:43:49Z,Hi Nate. I was wondering if you were aware of an issue with the website where SQL questions won&#39;t fully load and when you try to run commands the only output is &quot;&#39;tuple&#39; object has no attribute &#39;is_unique&#39;&quot;. Not sure if it&#39;s on my end or sitewide,2021-10-16T23:43:49Z
Ugy9wckdQePtq0X5Ond4AaABAg.9T_RnChbEvu9Tbd2oJAL0t,@stratascratch,Ugy9wckdQePtq0X5Ond4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-17T20:09:26Z,It&#39;s fixed. Thanks for letting me know!,2021-10-17T20:09:26Z
UgwH4053aDYRutd3RJd4AaABAg,@VidyaBhandary,,1,GeJUvdkJKEc,0,1,2021-10-14T15:09:55Z,Awesome !,2021-10-14T15:09:55Z
Ugx5dlorSXfbsh0bH-l4AaABAg,@YUVRAJSINGH-iz9gt,,1,GeJUvdkJKEc,1,1,2021-10-12T03:58:37Z,"Hello sir, I want to buy Lifetime plan. Can you please provide me Discount code",2021-10-12T03:58:37Z
Ugx5dlorSXfbsh0bH-l4AaABAg.9TO0z-pY2OZ9TO5SaNMhCd,@stratascratch,Ugx5dlorSXfbsh0bH-l4AaABAg,2,GeJUvdkJKEc,0,4,2021-10-12T04:37:45Z,That&#39;s great! Glad you&#39;re interested and find it useful. You can use `ss15` for 15% off the plans. Just add the code at checkout.,2021-10-12T04:37:45Z
UgxsC6eDz2DZmF8ph8V4AaABAg,@jameshizon4861,,1,GeJUvdkJKEc,4,4,2021-10-11T21:22:39Z,"Keep it up! I have a possible PayPal contract DE role to prep for, so I will continue to look to your channel for SQL accountability/motivation! :)",2021-10-11T21:22:39Z
UgxsC6eDz2DZmF8ph8V4AaABAg.9TNJendpE2d9TNKI1lGlCX,@stratascratch,UgxsC6eDz2DZmF8ph8V4AaABAg,2,GeJUvdkJKEc,0,1,2021-10-11T21:28:08Z,Good luck on the interview! Hope this channel helps.,2021-10-11T21:28:08Z
UgxsC6eDz2DZmF8ph8V4AaABAg.9TNJendpE2d9TNR9vtoWvd,@jameshizon4861,UgxsC6eDz2DZmF8ph8V4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-11T22:28:12Z,"‚Äã@@stratascratch Just additional commenting to help myself summarize info...Window function approach is also better in regards to readability. Thus, it is key for me personally to think about reducing amount of joins and instead using a window function in regards to performance and readability to easily catch errors when/if I need to debug.",2021-10-11T22:28:12Z
UgxsC6eDz2DZmF8ph8V4AaABAg.9TNJendpE2d9TzuIoRRUyA,@jayaa9730,UgxsC6eDz2DZmF8ph8V4AaABAg,2,GeJUvdkJKEc,0,1,2021-10-27T06:21:56Z,@james How was your interview with PayPal? I am also preparing for the interview with PayPal which requires sql skills,2021-10-27T06:22:23Z
UgxsC6eDz2DZmF8ph8V4AaABAg.9TNJendpE2d9U0W_U3CaDg,@jameshizon4861,UgxsC6eDz2DZmF8ph8V4AaABAg,2,GeJUvdkJKEc,0,0,2021-10-27T21:24:22Z,@@jayaa9730 Got ghosted :(,2021-10-27T21:24:22Z
UgyNHRtrpuZTRLYc3bF4AaABAg,@johnchan544,,1,GeJUvdkJKEc,0,2,2021-10-11T14:56:37Z,great content,2021-10-11T14:56:37Z
UgyIoen2I-V0mJUiEIR4AaABAg,@adibzainal3991,,1,GeJUvdkJKEc,0,2,2021-10-11T13:57:45Z,love this series,2021-10-11T13:57:45Z
UgxcrChAFOlrUSDP2YF4AaABAg,@pushkarraj1663,,1,GeJUvdkJKEc,1,5,2021-10-11T09:20:54Z,"Great content Nate as always, but please keep em coming more frequently. Would love to contribute if that helps in releasing the videos faster",2021-10-11T09:20:54Z
UgxcrChAFOlrUSDP2YF4AaABAg.9TM13_1ywI69TMmvG9Xdz4,@stratascratch,UgxcrChAFOlrUSDP2YF4AaABAg,2,GeJUvdkJKEc,0,3,2021-10-11T16:27:48Z,Thanks! Been trying to add content more frequently but my schedule&#39;s been so hectic lately! Scripts are written but there&#39;s just not enough time to film it =/. More to come though!,2021-10-11T16:27:48Z
Ugx-48Cjgv6c5uo1DIJ4AaABAg,@VloggingMemories,,1,GGURenNfXI0,1,1,2022-11-22T18:24:28Z,"My Solution:<br><br>SELECT<br>    department AS department,<br>    first_name AS employee_name,<br>    salary<br>FROM employee<br>WHERE (department , salary) IN<br>        (SELECT <br>            department, <br>            MAX(salary)<br>        FROM employee         <br>        GROUP BY department<br>        );",2022-11-22T18:24:28Z
Ugx-48Cjgv6c5uo1DIJ4AaABAg.9ijznupNYkE9jmwnE0suTb,@stratascratch,Ugx-48Cjgv6c5uo1DIJ4AaABAg,2,GGURenNfXI0,0,0,2022-12-18T18:27:16Z,Thanks for sharing!,2022-12-18T18:27:16Z
UgzE24XaK1FkT3HRdZJ4AaABAg,@Luke_L,,1,GGURenNfXI0,0,0,2022-06-16T04:13:27Z,"In the last solution, why can&#39;t you use the WHERE clause in the subquery by itself:<br>SELECT department, first_name, salary,<br>RANK() OVER(PARTITION BY DEPARTMENT ORDER BY salary DESC) AS salary_rank<br>FROM employee<br>WHERE salary_rank = 1;<br>I know it gives an error, but I don&#39;t know why. Thanks!",2022-06-16T04:13:27Z
Ugxg1qodovQzuFoYJMp4AaABAg,@ayoajayi280,,1,GGURenNfXI0,0,0,2022-06-14T19:29:35Z,what if we want to find the employees with the lowest salary. will the windows function work?,2022-06-14T19:29:35Z
UgxK-q1xFmJqKy0_pup4AaABAg,@krishnaKumar-fx6eu,,1,GGURenNfXI0,2,0,2021-12-21T14:40:28Z,"Hi Nate, <br>I really enjoy your video and the depth of explanation that you provide for every video. I am currently looking to solve medium level of questions and I request you to please cover more questions of medium level.. !<br>Looking forward to your response.<br>Thank you so much...!!",2021-12-21T14:40:28Z
UgxK-q1xFmJqKy0_pup4AaABAg.9WDQ4KWtwNh9WDjSx6JFrK,@stratascratch,UgxK-q1xFmJqKy0_pup4AaABAg,2,GGURenNfXI0,0,0,2021-12-21T17:38:36Z,Will do! It&#39;s a 2022 goal of mine to cover more questions and make more videos.,2021-12-21T17:38:36Z
UgxK-q1xFmJqKy0_pup4AaABAg.9WDQ4KWtwNh9WQGiMSJzbr,@krishnaKumar-fx6eu,UgxK-q1xFmJqKy0_pup4AaABAg,2,GGURenNfXI0,0,0,2021-12-26T14:28:49Z,"@@stratascratch Yup, That sounds great..!!!!<br>Completed most of the videos.. waiting for more.",2021-12-26T14:28:49Z
Ugxn4IbmJS3ro3d2bU94AaABAg,@elfridhasman4181,,1,GGURenNfXI0,0,0,2021-11-30T15:53:07Z,"Thanks man, you Help Me alot",2021-11-30T15:53:07Z
UgyhzRNf8ZKJz7Bzj2V4AaABAg,@ruima721,,1,GGURenNfXI0,1,0,2021-11-12T06:11:43Z,"Hey Nate, thanks for your tutorial and it really helps me. Could I ask in the third solution, is it necessary to put an &quot;a&quot; in the subquery just after &quot;)&quot;. I tried the same sql without &quot;a&quot; and there comes an error.",2021-11-12T06:11:43Z
UgyhzRNf8ZKJz7Bzj2V4AaABAg.9Ud4quudWL09UeGgFlCujL,@stratascratch,UgyhzRNf8ZKJz7Bzj2V4AaABAg,2,GGURenNfXI0,0,1,2021-11-12T17:14:21Z,"Yes, typically you need to alias subqueries. That&#39;s probably why you&#39;re getting the error.",2021-11-12T17:14:21Z
Ugze8UnhUdYy9nv2xj14AaABAg,@TheFiratayrilik,,1,GGURenNfXI0,0,0,2021-10-09T14:33:41Z,Wonderful!,2021-10-09T14:33:41Z
UgwT7dz-vIhoVvAXMIR4AaABAg,@joaopedroreissilva7075,,1,GGURenNfXI0,1,0,2021-10-02T12:43:35Z,"Thank you so much, Nate! Your didatic is awesome!",2021-10-02T12:43:35Z
UgwT7dz-vIhoVvAXMIR4AaABAg.9T-D6CJPEA_9T-NBHh-fTi,@stratascratch,UgwT7dz-vIhoVvAXMIR4AaABAg,2,GGURenNfXI0,0,0,2021-10-02T14:11:39Z,Thanks for watching! Really appreciate it!,2021-10-02T14:11:39Z
UgycugCVc2rTrvSDXtp4AaABAg,@harshitsati,,1,GGURenNfXI0,2,0,2021-09-29T11:16:15Z,"Hey Nate, why doesn&#39;t stratascratch support mysql ? I don&#39;t really know postgresql at the moment so I was looking for mysql but could not find it as an option<br>Or do you recommend dropping mySql over postgresql?",2021-09-29T11:16:15Z
UgycugCVc2rTrvSDXtp4AaABAg.9SsKisLs6Ve9StNWdpwVr4,@stratascratch,UgycugCVc2rTrvSDXtp4AaABAg,2,GGURenNfXI0,0,1,2021-09-29T20:59:53Z,"MySQL will be out soon -- probably sometime in November/Dec. We&#39;re currently building this functionality. MySQL and Postgres are 90% the same so I just chose an engine I was more familiar with and one that is better for analytics which is postgres. I wouldn&#39;t drop mysql over postres, both are great to know. If you know MySQL more then stick with it. We&#39;ll release it on the platform soon!",2021-09-29T20:59:53Z
UgycugCVc2rTrvSDXtp4AaABAg.9SsKisLs6Ve9SuztyEAjv1,@harshitsati,UgycugCVc2rTrvSDXtp4AaABAg,2,GGURenNfXI0,0,0,2021-09-30T12:03:14Z,@@stratascratch thanks for the answer nate!,2021-09-30T12:03:14Z
UgzbGkluKPl879iLyyt4AaABAg,@flipcase,,1,GGURenNfXI0,1,0,2021-09-25T23:08:10Z,"Wow Nate didn&#39;t know you were from Cornell! Can see it on the wall behind! And as always, really awesome video. My favorite is rank!",2021-09-25T23:08:10Z
UgzbGkluKPl879iLyyt4AaABAg.9SjJ0edPN5B9Slh8dieSUE,@stratascratch,UgzbGkluKPl879iLyyt4AaABAg,2,GGURenNfXI0,0,1,2021-09-26T21:26:11Z,That&#39;s impressive! Didn&#39;t know you can see the diploma all the way back there! Rank is the best one for this question!,2021-09-26T21:26:11Z
UgzrygVPnh3FCt98YqJ4AaABAg,@cleversachin,,1,GGURenNfXI0,0,0,2021-09-21T05:37:15Z,Looks like solution 3 will have edge cases if there are matching salary within multiple depts. I wll also join with deptno,2021-09-21T05:37:15Z
UgywUdtW23jqpyT_Xa54AaABAg,@aloknag4698,,1,GGURenNfXI0,1,1,2021-09-21T03:04:05Z,Hi everyone. I am really glad to share that I have just accepted a analytics consultant position at a top consulting and analytics firm in India. A great share of credits for this goes to Stratascratch website and youtube channel. The channel helped me improve my SQL skills by providing me a great collections of interview questions and teaching me how to approach and present your solution to the interviewer.<br><br>Thank you Nate.,2021-09-21T03:04:05Z
UgywUdtW23jqpyT_Xa54AaABAg.9SXr2AsUJ9R9SZFU5755Iz,@stratascratch,UgywUdtW23jqpyT_Xa54AaABAg,2,GGURenNfXI0,0,2,2021-09-21T16:05:35Z,That&#39;s great! And congratulations on your new job! Very happy that the videos and the platform helped you prepare for your interviews and led to a new opportunity.,2021-09-21T16:05:35Z
UgzDmp5ciMRstQmu2rt4AaABAg,@maarif1869,,1,GGURenNfXI0,0,0,2021-09-19T07:10:19Z,Great!! &lt;3,2021-09-19T07:10:19Z
UgzfWcmqUzktWUASzgx4AaABAg,@user-mo2xn3ni2b,,1,GGURenNfXI0,1,0,2021-09-18T17:43:42Z,"I think you could also use row_number too, and another approach would be to use cross apply with order by salary desc in it, right ?",2021-09-18T17:43:42Z
UgzfWcmqUzktWUASzgx4AaABAg.9SRhK7GFYIR9SRvcn6K0U8,@stratascratch,UgzfWcmqUzktWUASzgx4AaABAg,2,GGURenNfXI0,0,1,2021-09-18T19:48:43Z,Yup that&#39;s right on both approaches! None of them are wrong to use on an interview or on your day job.,2021-09-18T19:48:43Z
Ugxug2wCBrEgN1JA71J4AaABAg,@sauravkumar9454,,1,GGURenNfXI0,0,0,2021-09-17T09:23:34Z,Loved all four solutions and the differences among them. My personal fav would be self-join.,2021-09-17T09:23:34Z
UgzRMwnzK3s2AOhdlL14AaABAg,@17_gouravgupta87,,1,GGURenNfXI0,0,0,2021-09-16T07:17:08Z,Please make a series on SQL tutorial s it will be great help,2021-09-16T07:17:08Z
UgyiPCDAjZ0MRYShVH54AaABAg,@dipanjan_chaudhury,,1,GGURenNfXI0,1,0,2021-09-14T04:17:23Z,Hello I am from India. Do you provide any discount coupon for premium subscription?,2021-09-14T04:17:23Z
UgyiPCDAjZ0MRYShVH54AaABAg.9SFxrzMCM6V9SHBT2u-mNT,@stratascratch,UgyiPCDAjZ0MRYShVH54AaABAg,2,GGURenNfXI0,0,0,2021-09-14T15:44:10Z,"Yes, we have a 15% off any plan if you use ss15 discount code at checkout!",2021-09-14T15:44:10Z
UgwmW9B85Es9nck27SF4AaABAg,@kendallquinones5225,,1,GGURenNfXI0,1,7,2021-09-13T23:32:41Z,"I think that in a way the RANK() version is a tiny bit better just because it scales well in case you&#39;re asked to &quot;implement a tie-breaker&quot; if you end up with two or more employees with the same highest salary.<br><br>You could tell the interviewer that if in the future they decided that they wanted the employee with highest salary and longest tenure from each department, you could just add the hiring date to the ORDER BY clause and you&#39;d resolve the ties (even though we don&#39;t have hiring date on this table, but you get my point!)<br><br>Awesome video! Thanks as always.",2021-09-13T23:33:28Z
UgwmW9B85Es9nck27SF4AaABAg.9SFSHhHmSwW9SFZbhNkTal,@stratascratch,UgwmW9B85Es9nck27SF4AaABAg,2,GGURenNfXI0,0,1,2021-09-14T00:36:43Z,I really like that explanation! Those small nuances add a lot of value to interviewers since it proves that you know what you&#39;re talking about. Thanks for your input!,2021-09-14T00:36:43Z
UgzOUqzXtZ86t7jWqs94AaABAg,@adibzainal3991,,1,GGURenNfXI0,0,0,2021-09-13T13:58:49Z,"This is very insightful, subscribed!",2021-09-13T13:58:49Z
Ugx2Y3Rg4XsrGX1v65R4AaABAg,@hiovanycubillosgomez5901,,1,GGURenNfXI0,0,0,2021-09-13T09:15:02Z,"Hey man that&#39;s great, thank you so much to share your knowledge.",2021-09-13T09:15:02Z
UgzVEtJ_u-EH53TkgcJ4AaABAg,@tejasphirke3436,,1,GGURenNfXI0,0,0,2021-09-13T08:34:44Z,Very clear explanation..,2021-09-13T08:34:44Z
UgzYKU8fvo8Z8_Q_jUl4AaABAg,@shikharsaxena8984,,1,GGURenNfXI0,0,0,2021-09-13T06:45:55Z,"Your explanation is very good sir, appreciate your work üëèüëè",2021-09-13T06:45:55Z
Ugy9Q1vkJHBVvMgxHpx4AaABAg,@ashutoshsingh5568,,1,GGURenNfXI0,1,1,2021-09-13T06:27:47Z,Your way of teaching is simple and easy to understand. I‚Äôm learning new things from you.<br><br>Thanks for the teachings.<br>Keep it up.,2021-09-13T06:27:47Z
Ugy9Q1vkJHBVvMgxHpx4AaABAg.9SDc-8SH9889SEqU6kEZRY,@stratascratch,Ugy9Q1vkJHBVvMgxHpx4AaABAg,2,GGURenNfXI0,0,0,2021-09-13T17:53:35Z,Thanks for watching! Really appreciate it.,2021-09-13T17:53:35Z
UgxOlpy5JlZxJiW6fEF4AaABAg,@oscararmandocisnerosruvalc8503,,1,PlpUo6bHsBQ,1,0,2022-08-12T19:56:23Z,"Bro, your channel is just gold  !!!! I¬¥ve been spending 3 hours a day watching every day because before watching your videos I was not able to solve a single question, but since I started watching your videos everything seems to be a piece of cake.",2022-08-12T19:56:23Z
UgxOlpy5JlZxJiW6fEF4AaABAg.9edWEolJu8y9efQsYPG2Qv,@stratascratch,UgxOlpy5JlZxJiW6fEF4AaABAg,2,PlpUo6bHsBQ,0,0,2022-08-13T13:47:59Z,"Wow, thanks!  Music to my ears.  I am happy the videos I prepared, together with my team have helped you.",2022-08-13T13:47:59Z
Ugy-c7rZdfZkpntbcft4AaABAg,@jaymo2024,,1,PlpUo6bHsBQ,4,1,2022-07-30T01:02:37Z,"I believe the online ide is not working properly, unless you put in the expected result. Can someone please explain, why it is throwing errors when I use user_id and country columns (it is saying column name is ambiguous). I had to comment it out in order for the above code to run.<br>SELECT <br><br>    --user_id, <br>    sum(number_of_comments) as comments_dec, <br>    --country,<br>    dense_rank() over(order by sum(number_of_comments)desc) as country_rank<br>FROM fb_active_users as a<br>LEFT JOIN fb_comments_count as b<br>    ON a.user_id = b.user_id <br>WHERE created_at BETWEEN DATE(&#39;2019-12-01&#39;) AND (&#39;2019-12-31&#39;) and country is not null<br>GROUP BY country;",2022-07-30T01:02:37Z
Ugy-c7rZdfZkpntbcft4AaABAg.9e509VG9Spf9ffCfsgl59Y,@stratascratch,Ugy-c7rZdfZkpntbcft4AaABAg,2,PlpUo6bHsBQ,0,1,2022-09-07T08:15:19Z,"It&#39;s throwing an error because there&#39;s column user_id in both tables. So, yes, these columns are ambiguous. What you&#39;re missing is an alias in front of the column user_id. Which alias you&#39;ll use depends on which table you want to fetch user_id from. But if you&#39;re following my solution, there&#39;s no user_id in the SELECT statement; only country, SUM(), and DENSE_RANK()",2022-09-07T08:15:19Z
Ugy-c7rZdfZkpntbcft4AaABAg.9e509VG9Spf9ffxCCJHTXR,@jaymo2024,Ugy-c7rZdfZkpntbcft4AaABAg,2,PlpUo6bHsBQ,0,1,2022-09-07T15:10:34Z,@@stratascratch I was a super noop when I wrote that comment but I know now. Thanks üôèüèæ for the tutorials,2022-09-07T15:10:34Z
Ugy-c7rZdfZkpntbcft4AaABAg.9e509VG9Spf9fhkIEl5yTx,@stratascratch,Ugy-c7rZdfZkpntbcft4AaABAg,2,PlpUo6bHsBQ,0,0,2022-09-08T07:56:16Z,@@jaymo2024 Everyone has to start somewhere.  You&#39;re doing great!,2022-09-08T07:56:16Z
Ugy-c7rZdfZkpntbcft4AaABAg.9e509VG9Spf9fiQt6UX32S,@jaymo2024,Ugy-c7rZdfZkpntbcft4AaABAg,2,PlpUo6bHsBQ,0,0,2022-09-08T14:17:11Z,@@stratascratch I don‚Äôt think I should have been working on window functions as a beginner. I hope you make a video guide of the sql concepts to learn first from A to Z,2022-09-08T15:08:56Z
UgwGZQ0LWKhg8GD1D_R4AaABAg,@jaymo2024,,1,PlpUo6bHsBQ,0,0,2022-07-29T22:33:32Z,"with dec_summary as(<br>select <br>    country, <br>    sum(number_of_comments) as comments_dec,<br>    dense_rank() over(order by sum(number_of_comments)desc) as country_rank<br>from fb_active_users as a<br>left join fb_comments_count as b <br>    on a.user_id = b.user_id <br>where created_at BETWEEN DATE(&#39;2019-12-01&#39;) and DATE(&#39;2019-12-31&#39;) and country is not null <br>GROUP BY country<br>),<br>jan_summary as (<br>select <br>    country, <br>    sum(number_of_comments) as comments_jan,<br>    dense_rank() over(order by sum(number_of_comments)desc) as country_rank<br>from fb_active_users as a<br>left join fb_comments_count as b <br>    on a.user_id = b.user_id <br>where created_at BETWEEN DATE(&#39;2020-01-01&#39;) and DATE(&#39;2020-01-31&#39;) and country is not null <br>GROUP BY country<br>)<br>select *<br>from jan_summary as j<br>left join dec_summary as d<br>    on <a href=""http://j.country/"">j.country</a> = <a href=""http://d.country/"">d.country</a><br>where (j.country_rank &lt; d.country_rank) or d.country_rank is null;<br><br>-- My out put is Mali and Denmark which is different than what you have in the result. Am I missing something or did the table data changed??",2022-07-29T22:33:32Z
UgyJn5i6hytmxiJNhsR4AaABAg,@ashokvishalsr5591,,1,PlpUo6bHsBQ,1,0,2022-07-26T14:56:13Z,That was awesome!üí•üî•üí° Thanks a lot.,2022-07-26T14:56:13Z
UgyJn5i6hytmxiJNhsR4AaABAg.9dxCNYuykE-9e9XCKsQ_FQ,@stratascratch,UgyJn5i6hytmxiJNhsR4AaABAg,2,PlpUo6bHsBQ,0,1,2022-07-31T19:08:19Z,Definitely welcome.,2022-07-31T19:08:19Z
Ugy91DmA4UwDDlmyEsp4AaABAg,@rajyalakshmikadiyala6002,,1,PlpUo6bHsBQ,0,0,2022-07-01T08:21:09Z,Can&#39;t we use Rank function here in  window function,2022-07-01T08:21:09Z
UgxQ3DPLBRLaHHRaP1l4AaABAg,@yuthpatirathi2719,,1,PlpUo6bHsBQ,0,0,2022-06-19T02:19:27Z,Thanks nate,2022-06-19T02:19:27Z
Ugy8FE63Bhj_my8UvDJ4AaABAg,@manphu2515,,1,PlpUo6bHsBQ,0,0,2022-05-10T01:00:10Z,"Great solution, learn  a lot from this. Thank you",2022-05-10T01:00:10Z
UgyxJdKIGUoD7Ty2_ct4AaABAg,@callravik,,1,PlpUo6bHsBQ,0,0,2022-04-18T21:39:10Z,"God, am I glad I found StrataScratch or what?",2022-04-18T21:39:10Z
Ugyyi3Io9STLF8wQ7nd4AaABAg,@y_yy_2844,,1,PlpUo6bHsBQ,2,0,2022-03-24T06:58:09Z,"Obtain country and count of comments by month. Restrict it to December 2019 and January 2020 comments. Subtract December 2019 from January 2020 value. See the country that had the maximum positive difference.<br><br>I could do this query with click and drag in about 3 seconds in Microsoft Access, but no, let&#39;s write out SQL code like it&#39;s 1976.",2022-03-24T06:58:09Z
Ugyyi3Io9STLF8wQ7nd4AaABAg.9Zx35WQEzAG9bEqnRy-kOa,@sshnuke140,Ugyyi3Io9STLF8wQ7nd4AaABAg,2,PlpUo6bHsBQ,0,0,2022-05-20T07:10:18Z,"Now try to do this with 1 billion rows , ok buddy?",2022-05-20T07:10:18Z
Ugyyi3Io9STLF8wQ7nd4AaABAg.9Zx35WQEzAG9bEtwfS_wgx,@y_yy_2844,Ugyyi3Io9STLF8wQ7nd4AaABAg,2,PlpUo6bHsBQ,0,0,2022-05-20T07:37:47Z,@@sshnuke140 ? You just make the query in MS Access so all t he SQL is written for you. Then copy it to whatever else you&#39;re using.,2022-05-20T07:37:47Z
UgzgoaSr_fQKj61Dd594AaABAg,@austinocampo2410,,1,PlpUo6bHsBQ,1,0,2022-03-16T18:02:51Z,no Coalesce?,2022-03-16T18:02:51Z
UgzgoaSr_fQKj61Dd594AaABAg.9Zddo0HiyX29ZditV22ykq,@stratascratch,UgzgoaSr_fQKj61Dd594AaABAg,2,PlpUo6bHsBQ,0,0,2022-03-16T18:47:18Z,"If you wanted to, I supposed you could. But it wasn&#39;t required for this question.",2022-03-16T18:47:18Z
Ugwlx7v-ZKCcMY8KLqx4AaABAg,@rick2591,,1,PlpUo6bHsBQ,0,2,2022-03-01T15:47:52Z,"I took a different approach:<br>with sums as (<br> SELECT country, <br>  sum(case when created_at between cast(&#39;2019-12-1&#39; as date) and cast(&#39;2019-12-31&#39; as date) then number else 0 end) dec,<br>  sum(case when created_at between cast(&#39;2020-1-1&#39; as date) and cast(&#39;2020-1-31&#39; as date) then number else 0 end) jan<br> FROM users a inner join counts b on a.user_id=b.user_id)<br> group by country),<br>ranks as (<br> select country, <br>  dense_rank() over (partition by country order by dec) decRank, <br>  dense_rank() over (partition by country order by jan) JanRank<br> from sums)<br>select country <br>from ranks<br>where janRank &lt; decRank",2022-03-01T15:48:15Z
UgwltH75p4CJuJPSjiJ4AaABAg,@bindidesai8564,,1,PlpUo6bHsBQ,1,0,2022-02-25T07:01:52Z,"An attendance log for every student in a school district attendance_events :<br>date | student_id | attendance<br>‚Ä¢ A summary table with demographics for each student in the district all_<br>students : student_id | school_id | grade_level | date_of_birth | hometown<br>Using this data, you could answer questions like the following:<br>‚Ä¢ What percent of students attend school on their birthday?<br>‚Ä¢ Which grade level had the largest drop in attendance between yesterday<br>and today?",2022-02-25T07:01:52Z
UgwltH75p4CJuJPSjiJ4AaABAg.9YrY37WHDDs9YrY5EtimUN,@bindidesai8564,UgwltH75p4CJuJPSjiJ4AaABAg,2,PlpUo6bHsBQ,0,0,2022-02-25T07:02:09Z,can you pls solve this?,2022-02-25T07:02:09Z
UgzR4FgHLoNNNE7RQFd4AaABAg,@joaojoca100,,1,PlpUo6bHsBQ,0,0,2022-02-01T21:07:52Z,this is a hard question....,2022-02-01T21:07:52Z
UgyQRl4WRFYCodBnjj94AaABAg,@priyapatil724,,1,PlpUo6bHsBQ,0,0,2022-01-22T02:19:21Z,I have doubt that in your output Australia is having same number of comments in both the months means there is no rise so how it should be in the output. Was is necessary to use ranking function?  Instead can we compare the comments?,2022-01-22T02:19:21Z
UgwYs4s737WQ32pu4jV4AaABAg,@tigerbear3038,,1,PlpUo6bHsBQ,1,0,2022-01-11T07:08:11Z,"Hey Nate, your &quot;created_at &lt;= &#39;2019-12-31&#39;&quot; negates your left join because created_at table is from fb_comments_count and if a record is not in this table, it is null.  But because of your created_at join, you are forcing it to be unable to have nulls",2022-01-11T07:08:11Z
UgwYs4s737WQ32pu4jV4AaABAg.9X2g0-p8a_a9Z1muMVwzBK,@rick2591,UgwYs4s737WQ32pu4jV4AaABAg,2,PlpUo6bHsBQ,0,0,2022-03-01T15:51:59Z,Not a huge deal as the optimizer will determine this and switch to an inner join.,2022-03-01T15:51:59Z
UgxNLHnjN40SJo5CIAd4AaABAg,@rameespudussery2700,,1,PlpUo6bHsBQ,2,0,2022-01-02T16:15:49Z,"Hi Nate. Thanks for your video. I tried the below code to get sum of number of comments for Dec and Jan. But I am getting a different number when compared with yours. For Eg : In USA for Dec and Jan  i am getting 11 and 9 respectively. <br>But for you both months are 11. Confused!!<br><br>select COUNTRY,<br>SUM(CASE WHEN MONTH(created_at)=&#39;12&#39; AND Year(created_at)=&#39;2019&#39; THEN number_of_comments END) as dec_2019,<br>SUM(CASE WHEN MONTH(created_at)=&#39;1&#39; AND Year(created_at)=&#39;2020&#39; THEN number_of_comments END) as Jan_2020<br>from fb_active_users a<br>LEFT JOIN fb_comments_count b<br>ON a.user_id=b.user_id<br>GROUP BY COUNTRY;",2022-01-02T16:15:49Z
UgxNLHnjN40SJo5CIAd4AaABAg.9WhUX8VlyP09WiG3BvB205,@stratascratch,UgxNLHnjN40SJo5CIAd4AaABAg,2,PlpUo6bHsBQ,0,1,2022-01-02T23:28:38Z,"Hey, thanks for giving it a try. Can you leave a comment in the discussion forum on the platform? Someone from my team will be able to help much more quickly with technical questions!",2022-01-02T23:28:38Z
UgxNLHnjN40SJo5CIAd4AaABAg.9WhUX8VlyP09Wini8JmRxo,@rameespudussery2700,UgxNLHnjN40SJo5CIAd4AaABAg,2,PlpUo6bHsBQ,0,0,2022-01-03T04:31:27Z,@@stratascratch Sure. Thanks for your reply,2022-01-03T04:31:27Z
UgxzG5AE0gWcxeecyDp4AaABAg,@kebincui,,1,PlpUo6bHsBQ,0,0,2022-01-01T10:35:18Z,Thanks Nateüëçüèªüëçüèªüëçüèª,2022-01-01T10:35:18Z
UgxtJ9h69iagO-bIz_l4AaABAg,@walter_ullon,,1,PlpUo6bHsBQ,1,0,2021-11-08T01:05:37Z,"Basically, I&#39;m screwed... :(",2021-11-08T01:05:37Z
UgxtJ9h69iagO-bIz_l4AaABAg.9UTEdXvJS849UUx0z9TVLD,@stratascratch,UgxtJ9h69iagO-bIz_l4AaABAg,2,PlpUo6bHsBQ,0,1,2021-11-08T17:01:25Z,Keep practicing! it gets much easier!,2021-11-08T17:01:25Z
UgyDzCx4qsG3z6GZc414AaABAg,@shobhamourya8396,,1,PlpUo6bHsBQ,0,0,2021-10-26T11:43:18Z,"Hi Nate, here&#39;s my solution without left join<br><br>with cte1<br>as<br>(<br>-- for users who have commented get the country details<br>-- extract the month and year from created_at <br>    select c.user_id, <a href=""http://u.country/"">u.country</a>,<br>          extract(YEAR from created_at) as year, <br>          extract(MONTH from created_at) as month,<br>          number_of_comments as comments<br>    from fb_comments_count c, <br>         fb_active_users u<br>    where c.user_id = u.user_id <br>),<br>cte2<br>as<br>(<br>-- for each country find all the comments in dec and jan<br>    select country,<br>    sum(case when month = 12 and year = &#39;2019&#39; then comments else 0 end ) as dec_comments,<br>    sum(case when month = 1 and year = &#39;2020&#39; then comments else 0 end ) as jan_comments<br>    from cte1<br>    group by country<br>    order by country<br>),<br>cte3<br>as<br>(<br>-- rank the each country based on dec comments and jan comments<br>    select country,dec_comments,<br>           dense_rank() over(order by dec_comments desc) dec_rank,<br>           jan_comments,<br>           dense_rank() over(order by jan_comments desc) jan_rank<br>    from cte2<br>    order by country<br>)<br>-- highest number of comments get first rank, second-highest second rank<br>-- lesser rank in Jan means risen in ranking<br>--select country, jan_comments,jan_rank, dec_comments, dec_rank <br>select country<br>from cte3<br>where jan_rank &lt; dec_rank<br>order by country",2021-10-26T11:43:18Z
Ugy8iYKuGZHOCjM_I054AaABAg,@sandeepreddy4178,,1,PlpUo6bHsBQ,4,6,2021-10-23T21:47:15Z,"This is much faster and efficient:<br><br>With cte as <br>(<br>select base.*, dense_rank() over(partition by year_month order by total_comments desc) as ranks from<br>(Select country, format(created_at, ‚ÄòYYYY-MM) year_month, sum(comments) total_comments<br>From fb_active_users u<br>inner join comments_per_user c<br>On c.user_id = u.user_id and<br>created_at between ‚Äò2019-12-01‚Äô and ‚Äò2020-01-30‚Äô<br>Group by country, year_month) base<br>)<br><br>select <a href=""http://cte.country/"">cte.country</a>, ranks, lead(ranks) over (partition by country order by year_month) as next_month_ranks from cte<br>where next_month_ranks  &lt; ranks;",2021-10-25T02:41:50Z
Ugy8iYKuGZHOCjM_I054AaABAg.9TrG0SohvAt9Ttq4J8Hc0n,@stratascratch,Ugy8iYKuGZHOCjM_I054AaABAg,2,PlpUo6bHsBQ,0,1,2021-10-24T21:49:34Z,Love it! Thanks for sharing,2021-10-24T21:49:34Z
Ugy8iYKuGZHOCjM_I054AaABAg.9TrG0SohvAt9Xf_RN57sTw,@harshalpatel555,Ugy8iYKuGZHOCjM_I054AaABAg,2,PlpUo6bHsBQ,0,0,2022-01-26T19:00:22Z,"I like your query but I think Sandeep there is flaw when u r calculating the rank based on partition of only year and month , as it is calculating the rank for Dec or Jan month by comparing with all the Dec or Jan dates of all countries. What if we put country also in that partition so it would give us the rank based on country and year_month. Please correct me if I am going wrong way",2022-01-26T19:00:22Z
Ugy8iYKuGZHOCjM_I054AaABAg.9TrG0SohvAt9Xfa4MZKkRl,@harshalpatel555,Ugy8iYKuGZHOCjM_I054AaABAg,2,PlpUo6bHsBQ,0,0,2022-01-26T19:05:58Z,We can discuss more on this Sandeep if you are okay.,2022-01-26T19:05:58Z
Ugy8iYKuGZHOCjM_I054AaABAg.9TrG0SohvAt9XfrpjzE3qi,@sandeepreddy4178,Ugy8iYKuGZHOCjM_I054AaABAg,2,PlpUo6bHsBQ,0,0,2022-01-26T21:41:07Z,"Could work! I honestly didn‚Äôt run this against real data, just thought using a two level ranking function could make the query smaller so put it in here quick and dirty! So whatever works best üòä",2022-01-26T21:41:07Z
UgzcXFBgj63kNHgePhp4AaABAg,@michaelb1082,,1,PlpUo6bHsBQ,1,0,2021-10-15T05:00:12Z,This explanation is so good. I wish I found this channel a few months earlier!,2021-10-15T05:00:12Z
UgzcXFBgj63kNHgePhp4AaABAg.9TVrPiMULPD9TW3Rlxf3nH,@stratascratch,UgzcXFBgj63kNHgePhp4AaABAg,2,PlpUo6bHsBQ,0,1,2021-10-15T06:54:05Z,"Thanks for watching, buddy!",2021-10-15T06:54:05Z
UgwTpraIpZ5NVTbUkKJ4AaABAg,@kumarshishir92,,1,PlpUo6bHsBQ,0,13,2021-10-07T07:45:30Z,"Thanks for the walkthrough bro! Key takeaways: Understand data, formulate approach and then write code. BTW you used 2 CTEs which means 2 different joins on the same set of tables. You could have just used the one join along with sum(case when ...) statements to sum up the Dec and Jan comments. Just some food for thought. Cheers!",2021-10-07T07:45:45Z
UgxmEv-OXYnB44bifGB4AaABAg,@sauravkumar9454,,1,PlpUo6bHsBQ,1,2,2021-10-05T12:49:20Z,Great Explanation Nate. Thanks. Btw I also implemented the solution using group by on &quot;Country&quot; and &quot;Month&quot; and pivoted the table on the month column then used dense_rank() over &quot;December&quot; and &quot;January&quot; to extract the country that improved their ranking. It sounds complex though.üòÅ,2021-10-05T12:49:20Z
UgxmEv-OXYnB44bifGB4AaABAg.9T6x8x1ctDo9cKmh0dOeK5,@BhaveshKumar-dz8hq,UgxmEv-OXYnB44bifGB4AaABAg,2,PlpUo6bHsBQ,0,0,2022-06-16T11:01:19Z,I thought the same,2022-06-16T11:01:19Z
UgxwaxogJuYVfXzQu0l4AaABAg,@parantikaghosh4396,,1,PlpUo6bHsBQ,1,1,2021-10-05T03:31:45Z,you&#39;re the best SQL teacher I ever got!! :O amazing explanations :),2021-10-05T03:31:45Z
UgxwaxogJuYVfXzQu0l4AaABAg.9T5xL6CemuN9T7CWV8BwS9,@stratascratch,UgxwaxogJuYVfXzQu0l4AaABAg,2,PlpUo6bHsBQ,0,0,2021-10-05T15:12:21Z,Thanks for watching! Really appreciate it üòé,2021-10-05T15:12:21Z
UgzzfcXvv0UnW9CFi-h4AaABAg,@joaopedroreissilva7075,,1,PlpUo6bHsBQ,0,0,2021-10-04T14:18:12Z,That was AMAZING!,2021-10-04T14:18:12Z
UgwPijoiw1dIanGYk8l4AaABAg,@shuozhang8730,,1,PlpUo6bHsBQ,1,0,2021-10-04T04:56:57Z,"Dude, you rock!",2021-10-04T04:56:57Z
UgwPijoiw1dIanGYk8l4AaABAg.9T3XIDcwHaT9T4e3iSadjQ,@stratascratch,UgwPijoiw1dIanGYk8l4AaABAg,2,PlpUo6bHsBQ,0,0,2021-10-04T15:24:07Z,"Thanks, dude! Really appreciate it!",2021-10-04T15:24:07Z
UgxiTOp-llIoc65rQhl4AaABAg,@playlist4371,,1,PlpUo6bHsBQ,2,0,2021-09-18T22:51:47Z,"Thanks for this Nate, really helpful. Quick question: is there a reason why you are summing in the window functions? I am assuming each country only has one row",2021-09-18T22:51:47Z
UgxiTOp-llIoc65rQhl4AaABAg.9SSF_Ylbize9SU481CNA_q,@stratascratch,UgxiTOp-llIoc65rQhl4AaABAg,2,PlpUo6bHsBQ,0,0,2021-09-19T15:50:15Z,The window function just helps with the ranking. I&#39;m not partitioning my data but just taking the ranking over the entire window (which is the entire dataset) so that I can get a ranking.,2021-09-19T15:50:15Z
UgxiTOp-llIoc65rQhl4AaABAg.9SSF_Ylbize9XA2ACeHuX5,@emanne8067,UgxiTOp-llIoc65rQhl4AaABAg,2,PlpUo6bHsBQ,0,0,2022-01-14T03:45:14Z,Hi @@stratascratch: Shouldn&#39;t the dense_rank be ordered by num_of_comments rather than sum(num_of_comments),2022-01-14T03:45:14Z
UgxeTyj7IsoXw_OAGLN4AaABAg,@SelmanAy,,1,PlpUo6bHsBQ,0,1,2021-09-09T08:46:13Z,"‚Äã @Nate at StrataScratch  Hi Nate, I love your videos and they are very helpful, would you also prefer similar videos for python? for those people who prepares data engineer interviews ( like me :) )  would be very useful as well. Thanks for creating amazing content.",2021-09-09T08:46:13Z
Ugwtti0t5Re-_WsT9Fx4AaABAg,@plttji2615,,1,PlpUo6bHsBQ,2,0,2021-08-30T22:22:45Z,"Hello, I&#39;ve been watching this youtube channel a lot and found it really helpful. If you don&#39;t mind me asking a simple question?. Although I saw your video related which kinds of interview questions do DS role has to take, is it quite common to solve normal algorithm coding test (the one that for SWE for usual) for a DS interview? Thank you.",2021-08-30T22:22:45Z
Ugwtti0t5Re-_WsT9Fx4AaABAg.9RgH9GkOcNd9RgJblO9d_r,@stratascratch,Ugwtti0t5Re-_WsT9Fx4AaABAg,2,PlpUo6bHsBQ,0,1,2021-08-30T22:44:15Z,Yea it&#39;s pretty common these days to be asked algo questions on a DS interview. It might not be very common for a data analyst or similar position but for a DS it&#39;s starting to trend. I&#39;m hoping to do a few videos on this once I add these types of questions to my platform. Thanks for watching!,2021-08-30T22:44:15Z
Ugwtti0t5Re-_WsT9Fx4AaABAg.9RgH9GkOcNd9RgKABg2IZ4,@plttji2615,Ugwtti0t5Re-_WsT9Fx4AaABAg,2,PlpUo6bHsBQ,0,2,2021-08-30T22:49:05Z,@@stratascratch Thank you so much for answering my question. I got a lot of help from watching your channel as a MS DS student. Thank yoiu,2021-08-30T22:49:05Z
UgxnnLnHPNcqrTSX-y14AaABAg,@oguzhanakkurt3947,,1,PlpUo6bHsBQ,1,0,2021-08-25T18:57:44Z,"If you can shoot a video each week, it will be amazing for us. Thanks Nate :)",2021-08-25T18:57:44Z
UgxnnLnHPNcqrTSX-y14AaABAg.9RU1iHBBGdU9RUA4W1H-3u,@stratascratch,UgxnnLnHPNcqrTSX-y14AaABAg,2,PlpUo6bHsBQ,0,1,2021-08-25T20:10:48Z,That was my original plan! It&#39;s been difficult but I have more videos coming out soon.,2021-08-25T20:10:48Z
Ugx8UNdgNXwORtvm_IN4AaABAg,@fadwa2413,,1,PlpUo6bHsBQ,0,0,2021-08-21T14:15:13Z,"best of the best<br>please, keep going",2021-08-21T14:15:13Z
UgwqiQxv4A5E68GONop4AaABAg,@d.xinshengguo970,,1,PlpUo6bHsBQ,0,2,2021-08-19T21:03:07Z,Just wanted to say that your videos and the StrataScratch platform have been incredibly helpful in preparing for interviews. Thanks for the amazing content.,2021-08-19T21:03:07Z
UgzwBp_iOFnpJIP6aQR4AaABAg,@shivaniagarwal1841,,1,PlpUo6bHsBQ,1,3,2021-08-15T09:32:56Z,"Hi, thank you for your time and effort in creating these videos! Immensely helpful!I had a query about the date syntax - Is there a specific reason you use the operator clause for filtering instead of between?",2021-08-15T09:32:56Z
UgzwBp_iOFnpJIP6aQR4AaABAg.9R3H7n6q8ff9R4P26JLtSD,@stratascratch,UgzwBp_iOFnpJIP6aQR4AaABAg,2,PlpUo6bHsBQ,0,1,2021-08-15T20:01:18Z,"No there was no specific reason. In reality, you can use either. I often switch. between the two myself. Thanks for watching!",2021-08-15T20:01:18Z
UgzPl7t9-vff0liswLN4AaABAg,@rajatsharma6088,,1,PlpUo6bHsBQ,0,2,2021-08-09T19:59:31Z,"I tried a different way.. please review it <br><br>SELECT country FROM (<br>SELECT country, year, rnk, <br>lag(rnk, 1) OVER(partition by country order by [year] ) as rnk_lastyear <br>FROM(<br>SELECT b.*,<br>RANK() OVER(PARTITION BY [year]  order by [cnt]) as rnk<br>from (<br>Select country, [year], count(*) as cnt from (<br>select u.user_id, <a href=""http://u.country/"">u.country</a>,created_at,  year(c.created_at) as [year] from fb_comment_count c join fb_active_users u <br>on u.[user_id] = c.[user_id]<br>where Convert(datetime, created_at) between &#39;12/01/2019&#39; AND &#39;01/31/2020&#39;<br>) a<br>group by <a href=""http://a.country/"">a.country</a>, [year]<br>) b<br>) c<br>)d where rnk_lastyear  is not null <br>and rnk &gt; rnk_lastyear",2021-08-09T19:59:31Z
UgxsLqi8eRtAs6AA0sV4AaABAg,@nikhilverma2605,,1,PlpUo6bHsBQ,7,0,2021-08-09T18:19:45Z,Should i learn MYSQL or Postgresql<br>please tell me that,2021-08-09T18:19:45Z
UgxsLqi8eRtAs6AA0sV4AaABAg.9Qple4BZBbe9Qpx1C6jYQE,@stratascratch,UgxsLqi8eRtAs6AA0sV4AaABAg,2,PlpUo6bHsBQ,0,2,2021-08-09T19:59:09Z,"Given the 2, I would learn postgres bc it&#39;s the engine is created for analytics in mind. Many analytical teams use postgres. But in practice, it doesn&#39;t really matter. Most industry jobs will use other engines like Snowflake or HIVE. What&#39;s important is that you learn any SQL engine and learn to think about how to solve analytical questions.",2021-08-09T19:59:09Z
UgxsLqi8eRtAs6AA0sV4AaABAg.9Qple4BZBbe9QqH5mRwSst,@nikhilverma2605,UgxsLqi8eRtAs6AA0sV4AaABAg,2,PlpUo6bHsBQ,0,0,2021-08-09T23:03:17Z,Would you reccomend learning from udemy,2021-08-09T23:03:17Z
UgxsLqi8eRtAs6AA0sV4AaABAg.9Qple4BZBbe9QqIFW37yus,@stratascratch,UgxsLqi8eRtAs6AA0sV4AaABAg,2,PlpUo6bHsBQ,0,0,2021-08-09T23:13:21Z,@@nikhilverma2605 yea some courses are ok. I don‚Äôt know if any Udemy courses unfortunately. Try some free courses first like Mode Analytics before paying.,2021-08-09T23:13:21Z
UgxsLqi8eRtAs6AA0sV4AaABAg.9Qple4BZBbe9QrutcDpUoH,@nikhilverma2605,UgxsLqi8eRtAs6AA0sV4AaABAg,2,PlpUo6bHsBQ,0,0,2021-08-10T14:18:58Z,well what about data analytic by google at cousera,2021-08-10T14:18:58Z
UgxsLqi8eRtAs6AA0sV4AaABAg.9Qple4BZBbe9Qs48_R8NNb,@stratascratch,UgxsLqi8eRtAs6AA0sV4AaABAg,2,PlpUo6bHsBQ,0,0,2021-08-10T15:48:33Z,@@nikhilverma2605 ahh yea that&#39;s right. I forgot about that one. I hear great things about it.,2021-08-10T15:48:33Z
UgwL3Sc_U6fT_2ptklR4AaABAg,@faiazrummankhan5589,,1,PlpUo6bHsBQ,1,1,2021-08-09T14:04:45Z,Your videos helped me a lot in increasing my SQL skills ! Do you use any other language (python / R) at your work ?! Thanks !,2021-08-09T14:04:45Z
UgwL3Sc_U6fT_2ptklR4AaABAg.9QpJTTTZtn19QpN_p-KNqN,@stratascratch,UgwL3Sc_U6fT_2ptklR4AaABAg,2,PlpUo6bHsBQ,0,1,2021-08-09T14:40:43Z,"Yes, I use python a lot! Some people use R but python helps with automation",2021-08-09T14:40:43Z
UgzBGOvZjHEIc5cbL694AaABAg,@beyzayurt5350,,1,PlpUo6bHsBQ,4,1,2021-08-09T03:00:59Z,"Amazing video! One question: Every Facebook Data Science interview example I&#39;ve seen included SQL only, so I was wondering if FB only asks SQL coding questions? Or do they also ask Python?",2021-08-09T03:00:59Z
UgzBGOvZjHEIc5cbL694AaABAg.9Qo7VtrMlLg9QpNXzfRqKA,@stratascratch,UgzBGOvZjHEIc5cbL694AaABAg,2,PlpUo6bHsBQ,0,1,2021-08-09T14:40:19Z,I think most people decide to use SQL. You can pick any scripting language to answer coding questions on FB interviews (from my experience).,2021-08-09T14:40:19Z
UgzBGOvZjHEIc5cbL694AaABAg.9Qo7VtrMlLg9S3ZgijNvp6,@SelmanAy,UgzBGOvZjHEIc5cbL694AaABAg,2,PlpUo6bHsBQ,0,0,2021-09-09T08:46:31Z,"Hi Nate, I love your videos and they are very helpful, would you also prefer similar videos for python? for those people who prepares data engineer interviews ( like me :) )  would be very useful as well. Thanks for creating amazing content.",2021-09-09T08:46:39Z
UgzBGOvZjHEIc5cbL694AaABAg.9Qo7VtrMlLg9S4dL7qcC7K,@stratascratch,UgzBGOvZjHEIc5cbL694AaABAg,2,PlpUo6bHsBQ,0,3,2021-09-09T18:46:21Z,"@@SelmanAy Yes, I&#39;ve been playing around with the idea of some python videos! I&#39;m planning to add some algorithm questions that pops up on a few interviews so I may use python for that. Thanks for watching!",2021-09-09T18:46:21Z
UgzBGOvZjHEIc5cbL694AaABAg.9Qo7VtrMlLg9S60WucpxCs,@SelmanAy,UgzBGOvZjHEIc5cbL694AaABAg,2,PlpUo6bHsBQ,0,0,2021-09-10T07:36:55Z,@@stratascratch so happy to hear this! looking forward to it!,2021-09-10T07:36:55Z
UgzJyH4GvVonzPsKpT54AaABAg,@ishannahsi,,1,PlpUo6bHsBQ,1,5,2021-08-09T02:36:47Z,For sure the best resources online for interview prep for not just solving SQL problem but also approaching it conceptually. Thanks again.,2021-08-09T02:36:47Z
UgzJyH4GvVonzPsKpT54AaABAg.9Qo4jaepzII9QpNTxrHPiJ,@stratascratch,UgzJyH4GvVonzPsKpT54AaABAg,2,PlpUo6bHsBQ,0,1,2021-08-09T14:39:46Z,Thanks for watching! Really appreciate the support.,2021-08-09T14:39:46Z
UgzkudLg0afSaBBTrdx4AaABAg,@VloggingMemories,,1,yY7yau9j3xk,0,0,2022-11-28T17:23:52Z,"My Solution:<br><br>select user_id,<br>sum(number_of_comments)<br>from fb_comments_count<br>where created_at between cast(&#39;2020-02-10&#39; as date) - interval &#39;30 day&#39; and cast(&#39;2020-02-10&#39; as date)<br>group by 1<br>order by 1;",2022-11-28T17:23:52Z
UgwwYVYLELETugNy8Nd4AaABAg,@faeezaroos5908,,1,yY7yau9j3xk,1,0,2022-10-02T04:32:39Z,"hey nate, thanks for the thorough walkthrough. It seems like the cast function does not work for Mysql. I tried copying the same code, and I keep getting an error.",2022-10-02T04:32:39Z
UgwwYVYLELETugNy8Nd4AaABAg.9gfB3sVq9hw9glsmomBj0R,@stratascratch,UgwwYVYLELETugNy8Nd4AaABAg,2,yY7yau9j3xk,0,0,2022-10-04T18:58:51Z,try something like cast(column_name as data_type) for it to work on MySQL. The syntax I used was from postgres.,2022-10-04T18:58:51Z
UgzTQBaWyT80g_h-9p14AaABAg,@yuthpatirathi2719,,1,yY7yau9j3xk,1,0,2022-06-19T02:49:12Z,Thanks Nate,2022-06-19T02:49:12Z
UgzTQBaWyT80g_h-9p14AaABAg.9cRclMfrfNb9dmlTGsV7y1,@stratascratch,UgzTQBaWyT80g_h-9p14AaABAg,2,yY7yau9j3xk,0,0,2022-07-22T13:39:56Z,You are welcome!,2022-07-22T13:39:56Z
Ugxg2Rl_kQSTyxJ2Yxl4AaABAg,@juhairahamed5342,,1,yY7yau9j3xk,0,0,2022-04-06T12:41:46Z,Good explanation,2022-04-06T12:41:46Z
UgwvFQIJBrjsSUzTWUh4AaABAg,@allisonchaang7072,,1,yY7yau9j3xk,2,0,2022-02-27T18:31:48Z,"Hey Nate, is there a reason why you chose to do (30 * INTERVAL &#39;1 day&#39;) instead of  just (INTERVAL &#39;30 day&#39;)?",2022-02-27T18:31:48Z
UgwvFQIJBrjsSUzTWUh4AaABAg.9Yxvb66o2619YyvwA5qCA1,@stratascratch,UgwvFQIJBrjsSUzTWUh4AaABAg,2,yY7yau9j3xk,0,1,2022-02-28T03:53:55Z,"Nope, no reason =)",2022-02-28T03:53:55Z
UgwvFQIJBrjsSUzTWUh4AaABAg.9Yxvb66o2619Yz0EcraFqb,@allisonchaang7072,UgwvFQIJBrjsSUzTWUh4AaABAg,2,yY7yau9j3xk,0,0,2022-02-28T04:40:16Z,Oh cool good to know. I was wondering if INTERVAL ‚Äò30 days‚Äô would work the same way?,2022-02-28T04:40:16Z
UgywM0-iBKBrg6u5Yyh4AaABAg,@yuvrajmahendru2149,,1,yY7yau9j3xk,1,0,2022-02-17T06:09:42Z,"Hi, why don&#39;t you add general python questions on strata scratch besides pandas? Not DS and Algo, just general python programming questions",2022-02-17T06:09:42Z
UgywM0-iBKBrg6u5Yyh4AaABAg.9YXqj6XJVEg9YYznWKLHKc,@stratascratch,UgywM0-iBKBrg6u5Yyh4AaABAg,2,yY7yau9j3xk,0,1,2022-02-17T16:48:11Z,"We&#39;ll definitely be putting algo questions on the platform soon. But we don&#39;t ask general python programming questions bc they&#39;re not asked on interviews. Data wrangling/manipulation, structures, and algos are typically asked on interviews so we try to keep the questions on the platform strictly related to what industry tests you on.",2022-02-17T16:48:11Z
UgwXvIO8XqtT-gg3N1N4AaABAg,@kumarrohit3730,,1,yY7yau9j3xk,1,0,2021-11-28T05:27:49Z,Can we use DateAdd function..??,2021-11-28T05:27:49Z
UgwXvIO8XqtT-gg3N1N4AaABAg.9VHCYP0IoHo9VIm1X2Ntvq,@stratascratch,UgwXvIO8XqtT-gg3N1N4AaABAg,2,yY7yau9j3xk,0,0,2021-11-28T20:05:52Z,I believe so! Never tried it,2021-11-28T20:05:52Z
Ugx0YOnZCvI8-iKsqet4AaABAg,@shobhamourya8396,,1,yY7yau9j3xk,0,0,2021-10-27T05:59:03Z,"Hi Nate. INTERVAL is indeed useful for datetime condition statements, thanks!<br>Here&#39;s my solution using the number of days :<br><br>select user_id, sum(comments) as total_comments<br>from<br>(<br>    select user_id, number_of_comments as comments, created_at as date,<br>      (&#39;2020-02-10&#39;::date  - created_at ) as days<br>      from fb_comments_count<br>) a<br>where days &gt;= 0 and days &lt;= 30 <br>group by user_id<br>order by user_id",2021-10-27T06:06:32Z
UgzZVfkhtbi3b4gi6jV4AaABAg,@sanjanagupta8781,,1,yY7yau9j3xk,1,0,2021-10-19T06:37:05Z,"Hi Nate, your content is so helpful. And I started practicing in StrataScratch after watching your videos. Looking forward for your further videos..",2021-10-19T06:37:05Z
UgzZVfkhtbi3b4gi6jV4AaABAg.9TfKfkvy37O9Tg45_j5-j7,@stratascratch,UgzZVfkhtbi3b4gi6jV4AaABAg,2,yY7yau9j3xk,0,0,2021-10-19T13:31:26Z,Thanks for watching! Really appreciate it,2021-10-19T13:31:26Z
Ugx6BmlSKpbYkpaaz1l4AaABAg,@joaopedroreissilva7075,,1,yY7yau9j3xk,0,0,2021-10-14T12:33:55Z,Amazing!,2021-10-14T12:33:55Z
UgzyiAbK9y_XLOBQTi14AaABAg,@fadwa2413,,1,yY7yau9j3xk,0,0,2021-08-22T16:12:22Z,always Great<br>Thank You so much,2021-08-22T16:12:22Z
Ugwz33k64q3RHWoAShx4AaABAg,@tanyasinha2897,,1,yY7yau9j3xk,1,2,2021-08-16T02:07:17Z,Hey Can we use datediff function for this?,2021-08-16T02:07:17Z
Ugwz33k64q3RHWoAShx4AaABAg.9R52viv0UiO9R6WZXDyEsw,@stratascratch,Ugwz33k64q3RHWoAShx4AaABAg,2,yY7yau9j3xk,0,1,2021-08-16T15:45:30Z,"Yes, you most certainly can",2021-08-16T15:45:30Z
UgwI9ZI0hsN5BkQfo9R4AaABAg,@prateek2159,,1,yY7yau9j3xk,0,1,2021-07-24T16:30:00Z,"Hey Nate, your videos are just too good. I love how your channel is so dedicated towards real word data science. By the way I noticed that you started a video series, &quot;For your Data Science Project&quot; and I really want you to continue making videos for this particular series because there&#39;s literally no one on YouTube with such guidance on DS projects and I have been looking for one since a very long time because I have my placements just after 12 months and I really want to make a full stack data science project. Thank you.",2021-07-24T16:30:00Z
Ugz9igtZ1Tp8_z6XDDd4AaABAg,@jb4928,,1,yY7yau9j3xk,1,0,2021-07-23T03:01:49Z,"I realized you are giving solutions in PostgreSQL, not MSSQL, and for few minutes I was thinking are we really able to cast date with::date ? . BTW I loved your way of solving any problem.",2021-07-23T03:01:49Z
Ugz9igtZ1Tp8_z6XDDd4AaABAg.9Q7M58wh32Q9Q7P7WXGrrJ,@stratascratch,Ugz9igtZ1Tp8_z6XDDd4AaABAg,2,yY7yau9j3xk,0,1,2021-07-23T03:28:22Z,"haha yea I&#39;m using Postgres. But I&#39;ll add MSSQL to the platform soon, as well as other db engines. I should probably do a better job and use more standard functions. You can cast using the cast() function!",2021-07-23T03:28:22Z
UgwJVJH4pYdfN_RHIDN4AaABAg,@iitian2012,,1,yY7yau9j3xk,2,0,2021-07-21T11:23:54Z,Hi Nate...what is the role of nosql in data science? Is it equally important like sql or can be more important in the coming future? Also do you have any upcoming plans to include this part as well on stratascratch for practice just like sql and python.,2021-07-21T11:23:54Z
UgwJVJH4pYdfN_RHIDN4AaABAg.9Q35xfSG9Nh9Q3_rIKJaVN,@stratascratch,UgwJVJH4pYdfN_RHIDN4AaABAg,2,yY7yau9j3xk,0,1,2021-07-21T15:53:54Z,"Not all companies have nosql dbs. But the concept of unstructured data and key-value pairs are used in data science. You might use python to do this type of work. I do plan to add more data engineering, python (not pandas), algorithms in the future. NoSQL isn&#39;t something that&#39;s explicitly tested on (at least in my experience and in my research) unless the company specifically needs this skillset.",2021-07-21T15:53:54Z
UgwJVJH4pYdfN_RHIDN4AaABAg.9Q35xfSG9Nh9Q3b5SLeTK9,@iitian2012,UgwJVJH4pYdfN_RHIDN4AaABAg,2,yY7yau9j3xk,0,0,2021-07-21T16:04:43Z,@@stratascratch Thanks a lot Nate..i must say that stratascratch is a great platform i have ever came across and you are doing a great job ‚Ä¶‚ò∫Ô∏è,2021-07-21T16:04:43Z
UgyxyMGmM7PxSnf4Rpl4AaABAg,@francisaddae8797,,1,yY7yau9j3xk,1,0,2021-07-20T23:06:42Z,You‚Äôre a lifesaver. You videos are extremely informative and your approach to the problem is second to none. Keep it up Nate. Will love to pick your brain about a thing or two?,2021-07-20T23:06:42Z
UgyxyMGmM7PxSnf4Rpl4AaABAg.9Q1maJcEFzs9Q2AlMUqxGN,@stratascratch,UgyxyMGmM7PxSnf4Rpl4AaABAg,2,yY7yau9j3xk,0,0,2021-07-21T02:46:40Z,"Thank you. Feel free to ping me at nate@<a href=""http://stratascratch.com/"">stratascratch.com</a>",2021-07-21T02:46:40Z
Ugzat_BxO66GQ2fAW_t4AaABAg,@459B,,1,yY7yau9j3xk,1,0,2021-07-20T11:46:44Z,"At first glance, I just saw you want to solve an easy question. Nevertheless, in the video, you talked about how to impress an interviewer with a dynamic method. Instead of just showing a solution, you showed how to write a code which can impress interviewer. Really good work, Nate. I look forward to see more good teaching videos from you.",2021-07-20T11:46:44Z
Ugzat_BxO66GQ2fAW_t4AaABAg.9Q0Zm2R6lgW9Q0sKvsaaoG,@stratascratch,Ugzat_BxO66GQ2fAW_t4AaABAg,2,yY7yau9j3xk,0,3,2021-07-20T14:37:39Z,Thanks for the kind words! An easy question is not always an easy question =),2021-07-20T14:37:39Z
UgyqLby-AAQlGolxUbJ4AaABAg,@aloknag4698,,1,yY7yau9j3xk,1,1,2021-07-20T07:22:20Z,"Greetings, Nate! I really like your videos. Your videos not only teach how to solve problems in more structured ways, but how to keep the interviewer in the loop too while working on the problem. It has helped me in a my interviews.",2021-07-20T07:22:20Z
UgyqLby-AAQlGolxUbJ4AaABAg.9Q05WX5bTeR9Q0sMqr4FnX,@stratascratch,UgyqLby-AAQlGolxUbJ4AaABAg,2,yY7yau9j3xk,0,0,2021-07-20T14:37:55Z,Thanks! I&#39;m glad I was able to help with your interviews.,2021-07-20T14:37:55Z
UgyqTy3SdsXa_K3KH454AaABAg,@VloggingMemories,,1,i-E4pdU2qXM,0,0,2023-02-03T19:41:04Z,"My Solution:<br>select *<br>from<br>(select <br>    customer_id, <br>    count(event_id) events,<br>    dense_rank() over (order by count(event_id)) rank<br>from fact_events<br>where client_id=&#39;mobile&#39;<br>group by 1)x<br>where rank&lt;3<br>order by events;",2023-02-03T19:41:04Z
UgxJacCSxG03wFeBbAR4AaABAg,@marshalllee9599,,1,i-E4pdU2qXM,0,0,2022-05-11T17:19:17Z,"Hey Nate here&#39;s my solution with cte/case when. Look correct?<br><br>with cte as (<br>SELECT customer_id, <br>	SUM(CASE WHEN event_type = &#39;mobile&#39; then 1 ELSE 0 END) as mobile_usage,<br>    dense_rank() oVER(order by SUM(CASE WHEN event_type = &#39;mobile&#39; then 1 ELSE 0 END) dr<br>FROM fact_events<br>GROUP BY 1)<br><br>SELECT customer_id, <br>mobile_usage<br>FROM cte<br>WHERE dr &lt;= 2<br>ORDER BY mobile_usage",2022-05-11T17:19:17Z
UgwUjglKwSf8ql83iWl4AaABAg,@aZnPriDe707,,1,i-E4pdU2qXM,0,0,2022-02-26T02:52:34Z,Thank you Nate! I love your in-depth walkthroughs.,2022-02-26T02:52:34Z
Ugzq0p7WmIU4vAF89uR4AaABAg,@mohitupadhayay1439,,1,i-E4pdU2qXM,1,0,2022-01-31T17:35:35Z,Laminate and put it on the wall type explanation,2022-01-31T17:35:35Z
Ugzq0p7WmIU4vAF89uR4AaABAg.9XsIhh1OXod9XsQtvVg7Mc,@stratascratch,Ugzq0p7WmIU4vAF89uR4AaABAg,2,i-E4pdU2qXM,0,0,2022-01-31T18:47:09Z,=),2022-01-31T18:47:09Z
UgxxK_GY7OGVAw6FX-t4AaABAg,@krishnaKumar-fx6eu,,1,i-E4pdU2qXM,0,0,2021-12-20T17:46:00Z,Its like i daily learn something new from your videos...,2021-12-20T17:46:00Z
UgwuteW5P9Jn-8-87IR4AaABAg,@ahyoungkim8256,,1,i-E4pdU2qXM,1,0,2021-10-31T07:49:49Z,This video is perfect to understand different ranking functions! Thank you,2021-10-31T07:50:04Z
UgwuteW5P9Jn-8-87IR4AaABAg.9U9MXxQaTbc9UAEkhgtN09,@stratascratch,UgwuteW5P9Jn-8-87IR4AaABAg,2,i-E4pdU2qXM,0,0,2021-10-31T16:01:02Z,Thank you for watching! Glad you learned something,2021-10-31T16:01:02Z
Ugzdri3NYEVP9mFP4Od4AaABAg,@joaopedroreissilva7075,,1,i-E4pdU2qXM,0,0,2021-10-14T14:13:44Z,Amazing!,2021-10-14T14:13:44Z
UgxXKqrThhUmV2EqnLZ4AaABAg,@fadwa2413,,1,i-E4pdU2qXM,2,0,2021-08-22T16:21:25Z,"Hi, are there any different plans for Egypt to subscribe on Stratascratch?",2021-08-22T16:21:25Z
UgxXKqrThhUmV2EqnLZ4AaABAg.9RM1Rmb0s9P9RM949nug8q,@stratascratch,UgxXKqrThhUmV2EqnLZ4AaABAg,2,i-E4pdU2qXM,0,1,2021-08-22T17:28:06Z,We do have parity pricing for people from different country. Try discount code ss15 for 15% off any of the plans!,2021-08-22T17:28:06Z
UgxXKqrThhUmV2EqnLZ4AaABAg.9RM1Rmb0s9P9RMC2j2-Q0S,@fadwa2413,UgxXKqrThhUmV2EqnLZ4AaABAg,2,i-E4pdU2qXM,0,0,2021-08-22T17:54:07Z,@@stratascratch Thanks a lot,2021-08-22T17:54:07Z
UgyrMA2DqyToBL-SuQN4AaABAg,@mathewsjoy8464,,1,i-E4pdU2qXM,3,0,2021-07-18T16:49:49Z,"Hey! really enjoyed this video, so when we use subqueries and we do the where and select clause, dont we need to do like subquery.rank to access rank and <a href=""http://subquery.events/"">subquery.events</a>? or is it fine to just leave it as rank and events when trying to access the columns (still quite new to sql) thanks! Also are CTES generally just for readability purposes as they dont have any performance benefits of subqueries, correct me if Iam wrong.",2021-07-18T16:51:44Z
UgyrMA2DqyToBL-SuQN4AaABAg.9PwxsAkuGom9Py2htditru,@stratascratch,UgyrMA2DqyToBL-SuQN4AaABAg,2,i-E4pdU2qXM,0,0,2021-07-19T03:00:05Z,"I&#39;m not sure what subquery.rank and <a href=""http://subquery.events/"">subquery.events</a> mean? Can you give me an example? If you&#39;re talking about aliasing the subquery, then yes, you typically should alias because calling a column. <br><br>In terms of CTEs, yes they are generally more readable. But also you can reuse CTEs in other parts of your code if you have 1 giant query. So in that case, you are optimizing the performance.<br><br>Thanks for watching!",2021-07-19T03:00:05Z
UgyrMA2DqyToBL-SuQN4AaABAg.9PwxsAkuGom9PyO3bFDJDA,@mathewsjoy8464,UgyrMA2DqyToBL-SuQN4AaABAg,2,i-E4pdU2qXM,0,0,2021-07-19T06:06:41Z,@@stratascratch so for example why do we need to give an alias if we are already putting the query inside the () brackets after the from statment of the outer query,2021-07-19T06:06:41Z
UgyrMA2DqyToBL-SuQN4AaABAg.9PwxsAkuGom9PzRjqx6LV8,@stratascratch,UgyrMA2DqyToBL-SuQN4AaABAg,2,i-E4pdU2qXM,0,1,2021-07-19T15:58:03Z,@@mathewsjoy8464 Oh i see what you mean now. You don&#39;t have to put the alias. It&#39;s sometimes standard or best practice too depending on your style and/or your companies style guide. I do it from habit.,2021-07-19T15:58:03Z
Ugz0ZflHCe6QJluYaQp4AaABAg,@shipragupta4382,,1,i-E4pdU2qXM,1,1,2021-07-14T12:08:22Z,"Hi Nate, I really like your all SQL videos. Can you please make few videos on Python as well. Solving same problem?",2021-07-14T12:08:22Z
Ugz0ZflHCe6QJluYaQp4AaABAg.9Pm9UHY1KAW9Pmb0u4JroN,@stratascratch,Ugz0ZflHCe6QJluYaQp4AaABAg,2,i-E4pdU2qXM,0,0,2021-07-14T16:17:46Z,"Hi! Yes, I can certainly do some in python for future videos. Thanks for the suggestion!",2021-07-14T16:17:46Z
UgwQVRlOHt4on1HGdQF4AaABAg,@mudit6875,,1,i-E4pdU2qXM,1,3,2021-07-13T08:33:55Z,I love stratascratch..i almost solved all the free questions on it and i really love the real scenario based questions in it..problem is that its very expensive for me to buy the premium plan..i live in india and currency difference makes it even costlier..i wish stratascratch will have different plans for indian users in future.,2021-07-13T08:33:55Z
UgwQVRlOHt4on1HGdQF4AaABAg.9PjC8kFpCZJ9PjwXzcLORZ,@stratascratch,UgwQVRlOHt4on1HGdQF4AaABAg,2,i-E4pdU2qXM,0,2,2021-07-13T15:28:03Z,Hi! We do have parity pricing for many countries. For India there is 30% off any plan using the code PPP30 that you can use on checkout.,2021-07-13T15:28:03Z
UgzmX5wA3Jff8EMOZxF4AaABAg,@girimadhav5816,,1,i-E4pdU2qXM,3,0,2021-07-13T06:53:18Z,"Hi Nate thanks for these amazing videos. I couldn&#39;t solve problems on stratascratch site. Whenever, I click on Question it redirects to blank page. How can I resolve this issue.",2021-07-13T06:53:18Z
UgzmX5wA3Jff8EMOZxF4AaABAg.9Pj0ciN-orU9PjwUuvetpj,@stratascratch,UgzmX5wA3Jff8EMOZxF4AaABAg,2,i-E4pdU2qXM,0,0,2021-07-13T15:27:38Z,That&#39;s odd. It&#39;s working for myself and my team. Have you tried a different browser or clearing your browser cache?,2021-07-13T15:27:38Z
UgzmX5wA3Jff8EMOZxF4AaABAg.9Pj0ciN-orU9Pjx5RL0KnW,@girimadhav5816,UgzmX5wA3Jff8EMOZxF4AaABAg,2,i-E4pdU2qXM,0,0,2021-07-13T15:32:54Z,"@@stratascratch I&#39;ve tried on incognito too but I couldn&#39;t open freemium coding Questions. However, non-coding Questions open up.",2021-07-13T15:32:54Z
UgzmX5wA3Jff8EMOZxF4AaABAg.9Pj0ciN-orU9PjzFqoU-wF,@stratascratch,UgzmX5wA3Jff8EMOZxF4AaABAg,2,i-E4pdU2qXM,0,1,2021-07-13T15:51:48Z,"@@girimadhav5816 You tried clearing your browser cache? You have no javascript blockers? And you are going to <a href=""https://platform.stratascratch.com/coding"">https://platform.stratascratch.com/coding</a>? If you tried all of this can you email me at nate@stratascratch",2021-07-13T15:51:48Z
Ugwd0-loV_MBwqkU4iF4AaABAg,@VloggingMemories,,1,T1UhSuKqy3A,0,0,2023-02-05T08:51:22Z,"Please validate my approach, I think this can be done in this way too :)<br>My Solution: <br>with filtered_dataset as<br>    (SELECT <br>        &#39;1&#39; as partition_no,<br>        restaurant_id, <br>        sum(order_total) as revenue<br>    FROM doordash_delivery<br>    where to_char(customer_placed_order_datetime, &#39;MM-YYYY&#39;)=&#39;05-2020&#39;<br>    group by 2)<br>select *<br>from<br>    (select <br>        restaurant_id,<br>        revenue,<br>        sum(revenue) over (partition by partition_no) total_revenue,<br>        (revenue / sum(revenue) over (partition by partition_no))*100 as revenue_percentage<br>    from<br>        filtered_dataset)a<br>where revenue_percentage&lt;=2;",2023-02-05T08:53:14Z
Ugzlyjw1I6eLNbzPnEl4AaABAg,@ashimahorra9623,,1,T1UhSuKqy3A,1,0,2022-06-20T23:59:52Z,@strataScratch Why cant we use PERCENTILE_CONT here?,2022-06-20T23:59:52Z
Ugzlyjw1I6eLNbzPnEl4AaABAg.9cWTynBAqpm9cvOtsPBU4Z,@stratascratch,Ugzlyjw1I6eLNbzPnEl4AaABAg,2,T1UhSuKqy3A,0,0,2022-07-01T01:35:45Z,I don&#39;t know...does it give you the same output if you use it? How is PERCENTILE_CONT different than what I&#39;m using? There could be an edge case that you&#39;re not considering or I&#39;m not considering.,2022-07-01T01:35:45Z
UgynS5jAytFmSGHTQs94AaABAg,@papatingle9964,,1,T1UhSuKqy3A,1,1,2022-05-22T10:43:42Z,"I would like to challenge the usage of NTILE function in this question. While it is a possibility things need to be clarified in front of the interviewer but in my opinion NTILE is the least valid option out of the 3 possible here.<br><br>1. NTILE as used in the video<br>2. ROW_NUMBER() OVER(ORDER BY  SUM(order_total))/(SELECT COUNT(*) FROM doordash_delivery)*100<br>3. RANK() OVER(ORDER BY SUM(order_total))/(SELECT COUNT(*) FROM doordash_delivery)*100<br><br>To present my arguement let&#39;s assume there are 101 restaurants and we were tasked with giving the bottom 2%<br><br><br>Let&#39;s also assume the bottom 5 restaurants have the exact same revenue. We&#39;d get the following results when querying for the bottom 2% (AKA ntile=1 or row_number()&lt;=2 or rank()&lt;=2)<br><br>1. we&#39;d get 3 restaurants - So neither the entire 5 restaurants that have the same revenue nor 2 restaurants which would probably better indicate 2% then 3 restaurants (2/101 closer to 2% then 3/101) - This is becasue for NTILE(n) where N is the number of rows if N/n isn&#39;t a whole number the first modulo of N/n buckets would have 1 more entry (so for instance NTILE(50) for 114 rows would mean that the first 14 rows have 1 more entry than the other 36). We could get around this by NTILE ordering by DESC and choosing NTILE=50 but in that case I&#39;d argue even if it&#39;s a bit more efficient since we don&#39;t run a subquery of count it is much less clear.<br>2. we&#39;d get 2 restaurants - since 3/101*100 is bigger than 2.<br>3. we&#39;d get 5 restaurants since all of them have the same revenue.<br><br>Again, the interviewer might specify we need to choose the bottom 2% no matter what even if revenue is even and in that case NTILE can be used (but not in the way presented in the video).",2022-05-22T10:43:42Z
UgynS5jAytFmSGHTQs94AaABAg.9bKNoAeTg5d9bQMC6CcHNs,@stratascratch,UgynS5jAytFmSGHTQs94AaABAg,2,T1UhSuKqy3A,0,1,2022-05-24T18:25:04Z,"Hi papa tingle,<br><br>That&#39;s a great comment, thank you for the very detailed and easy to follow argumentation.<br><br>You detected a real problem - what to do in case revenue/score of different entities are tied. Do we want to output all the tied entities, or we want to limit ourselves to return as closely to 5% of data points as possible?<br><br>If we want to just return 5% of data, then NTILE and row_number are good solutions for us. <br><br>If we want to return all the tied entities - we&#39;d go with the RANK option. One alternative to the RANK option that is more intuitive to us is PERCENTILE_CONT/PERCENTILE_DISC function, e.g. &quot;SELECT percentile_disc(0.05) within group (order by summed_total_order) as percentile_05&quot;. This gets you the score that is at 5% and then you have the flexibility to do what you want with it. <br><br>Always discuss with your interviewer what is expected from you in such cases. When we think about it, returning all the tied entities makes more sense. But someone could have a different opinion.",2022-05-24T18:25:04Z
UgypUNlCsHa-puPZZsF4AaABAg,@owenren1626,,1,T1UhSuKqy3A,0,0,2022-04-28T01:39:45Z,"Great Video, just a question, when would we use PERCENT_RANK()? Is it because the question says evenly distributed buckets?",2022-04-28T01:39:45Z
UgzfkjgBG06rq1jNs7R4AaABAg,@AAMIRKHAN1602,,1,T1UhSuKqy3A,0,0,2022-03-31T04:07:15Z,Very good explanation and it is helping me to think clearly and give solutions.<br>Your videos are great! <br>Thanks,2022-03-31T04:07:15Z
Ugx7ptNjviSqa5Zdbi54AaABAg,@PawanSharma-gs7cy,,1,T1UhSuKqy3A,0,0,2022-02-09T08:41:37Z,Thank you !! keep providing such informative SQL concepts..,2022-02-09T08:41:37Z
UgxGSo1AHKVVZcY9_FV4AaABAg,@grainofsalt2113,,1,T1UhSuKqy3A,1,1,2022-01-10T03:25:42Z,I love how methodical you are. I totally stole this for my own approach lol,2022-01-10T03:25:42Z
UgxGSo1AHKVVZcY9_FV4AaABAg.9X-hkcJbACk9X-mY-S899d,@stratascratch,UgxGSo1AHKVVZcY9_FV4AaABAg,2,T1UhSuKqy3A,0,0,2022-01-10T04:07:32Z,I hope you do and apply it to your interviews!,2022-01-10T04:07:32Z
UgxeT9q2ykBYh9jpHzx4AaABAg,@moutaouakkilyoussef5825,,1,T1UhSuKqy3A,1,1,2021-12-06T12:09:54Z,Thank you for the clean tutorials!! Should&#39;ve much more views considering the quality and information in the channel,2021-12-06T12:09:54Z
UgxeT9q2ykBYh9jpHzx4AaABAg.9VbWvNlkyZ_9VcvR_JwZma,@stratascratch,UgxeT9q2ykBYh9jpHzx4AaABAg,2,T1UhSuKqy3A,0,0,2021-12-07T01:12:07Z,"Thanks, man! Appreciate it",2021-12-07T01:12:07Z
UgyiUrl-scJ6YA0db8N4AaABAg,@lecryptojames,,1,T1UhSuKqy3A,1,0,2021-11-12T04:09:48Z,Great! I landed a DE Consultant role where my potential client is DoorDash.,2021-11-12T04:09:48Z
UgyiUrl-scJ6YA0db8N4AaABAg.9UcrtzszSTi9UeGcPV0F70,@stratascratch,UgyiUrl-scJ6YA0db8N4AaABAg,2,T1UhSuKqy3A,0,1,2021-11-12T17:13:50Z,Congrats! Hope these videos are helpful!,2021-11-12T17:13:50Z
UgzqTET6BNyQp8ofjuF4AaABAg,@joaopedroreissilva7075,,1,T1UhSuKqy3A,0,0,2021-10-14T16:56:27Z,"Thank you, Nate!<br>Impressive content, like always.",2021-10-14T16:56:27Z
Ugxj419abzUcnBnAzQV4AaABAg,@hariprasath9997,,1,T1UhSuKqy3A,1,0,2021-09-21T10:00:11Z,suppose we can split the dataset into two means then how the query will be ? any one guide me please<br>those 2 tables name was : table_one and table_two,2021-09-21T10:00:11Z
Ugxj419abzUcnBnAzQV4AaABAg.9SYaelX01hx9TZPikTd9H7,@nikhilgarakapati566,Ugxj419abzUcnBnAzQV4AaABAg,2,T1UhSuKqy3A,0,0,2021-10-16T14:06:30Z,"You can use the CTE concept here. <br><br>If the number of rows are even, you can put filter like from start to count(rows)/2, that would give you first half table and (count(rows)/2)+1 to count(rows), that would give you the second half. You can use this as a subquery, but I would prefer a CTE as it can be used anytime whereas subquery, we have to run it even though it wont be needed.<br><br>If the number of rows are odd, we can cannot evenly split. In that case, you write you condition accordingly.",2021-10-16T14:06:58Z
Ugy3Y68kjm3A1nqAa5J4AaABAg,@Tridentor,,1,T1UhSuKqy3A,0,4,2021-09-04T19:39:07Z,"Thanks a lot! Actually now that you mentioned SQL optimization, a video on generic optimization approach/best practices / thinking will be highly appreciated!",2021-09-04T19:39:07Z
Ugzebv3orkugMx7AR694AaABAg,@poojagupta-dy1gv,,1,T1UhSuKqy3A,1,0,2021-08-20T00:44:39Z,"Hi Nate, Can we create 100 bucket and choose two for bottom 2% ?",2021-08-20T00:44:39Z
Ugzebv3orkugMx7AR694AaABAg.9RFCdtvfLuk9RMpIe8OdC0,@yashovardhan9841,Ugzebv3orkugMx7AR694AaABAg,2,T1UhSuKqy3A,0,0,2021-08-22T23:45:49Z,"I tried ntile(100) and then n&lt;=2, but the answer is incorrect. By intuition, I thought of this to be correct but it took me a while to understand why it is wrong. So 2/100=1/50, hence we want to find the &#39;1&#39; in &#39;50&#39;. If the question was find bottom 3%, then it would approximately be 1/33, so &#39;1&#39; in &#39;33&#39;. Hope this helps in understating the logic.",2023-09-19T20:50:10Z
UgwjhQKBQdPQsnZpRIB4AaABAg,@poisson12376,,1,T1UhSuKqy3A,1,0,2021-08-07T10:53:38Z,"Man, your contents are really good! Have got much chance to work on SQL in my current job, need to practice more.",2021-08-07T10:53:38Z
UgwjhQKBQdPQsnZpRIB4AaABAg.9Qjp-wjTnNZ9QkmfZ_dszs,@stratascratch,UgwjhQKBQdPQsnZpRIB4AaABAg,2,T1UhSuKqy3A,0,0,2021-08-07T19:52:29Z,Thanks for watching! Keep practicing!,2021-08-07T19:52:29Z
Ugwc9CwRL2QQ0TQ_dhh4AaABAg,@nadiaennab4586,,1,T1UhSuKqy3A,1,1,2021-08-04T16:13:26Z,"Thank you, Nate! Could you please clarify the following, I am a little confused on these:<br><br>1. How does the window function work with the group by clause? I have never used a window function with group by. Does the query execute the [SELECT restaurant_id, SUM(order_total) as total_order FROM doordash_delivery GROUP BY retaurant_id] and then assign ntiles to the grouped rows? Or, is something else going on? <br><br>2. Why do you have to do a subquery/CTE instead of using a having clause after group by? I guess this is related to my first question regarding order of operations. Is the ntile part not executed before group by such that it cannot be referenced in a having statement?<br><br>3. This is trivial, but when you used the ntile statement, you did not assign it an alias name. Does it automatically become called &quot;ntile&quot; such that you were able to say ntile=1 in the where statement?",2021-08-04T16:13:56Z
Ugwc9CwRL2QQ0TQ_dhh4AaABAg.9QcfDZ8x4Xi9Qcjyn_bsaz,@stratascratch,Ugwc9CwRL2QQ0TQ_dhh4AaABAg,2,T1UhSuKqy3A,0,1,2021-08-04T16:54:58Z,Thanks for watching the video and trying out a solution! Would you be able to copy and paste your question on the forum attached to the question on the platform? Someone from my team will answer each of your questions within 3 days. It&#39;s the best way to get a timely answer! Thanks again!,2021-08-04T16:54:58Z
Ugxq25UBBUjVqd4776p4AaABAg,@v.sprasad8399,,1,T1UhSuKqy3A,0,0,2021-07-19T22:38:18Z,Really helpful Nate,2021-07-19T22:38:18Z
UgwtvKf5MMUfovhsAAd4AaABAg,@deepshikasharma8436,,1,T1UhSuKqy3A,2,1,2021-07-07T18:52:45Z,Your videos are hands down the best resource I have come across so far in answering DS interview questions! Could you also do a series on how to explain advanced SQL functions/concepts?,2021-07-08T14:17:10Z
UgwtvKf5MMUfovhsAAd4AaABAg.9PVrBwoInsJ9PWCQ5C3yg1,@stratascratch,UgwtvKf5MMUfovhsAAd4AaABAg,2,T1UhSuKqy3A,0,0,2021-07-07T22:06:55Z,Thanks for watching! Most of my videos on DS interview questions do contain some level of explanation on advanced functions I use. But what I can do is add the advanced concept/function in the title of the video and also provide a more in-depth explanation on the concept. I think that would help.,2021-07-07T22:06:55Z
UgwtvKf5MMUfovhsAAd4AaABAg.9PVrBwoInsJ9PXxj-v3j7y,@deepshikasharma8436,UgwtvKf5MMUfovhsAAd4AaABAg,2,T1UhSuKqy3A,0,0,2021-07-08T14:28:19Z,"@@stratascratch Thanks for responding! That would help immensely! The reason I asked, is because you mentioned that while answering a code question, we should explain which operators we&#39;re using and why. I get tripped up when I try to explain the (advanced) operator I&#39;m using in non-technical/simple terms while answering the question.",2021-07-08T14:28:19Z
UgxEBxRjC4PtA-4zmFV4AaABAg,@gajjelapriyanka5301,,1,T1UhSuKqy3A,2,0,2021-06-24T04:46:22Z,"Hi Nate, how can we find creation date of jobs from June 2020 to June 2021 in sap bods? I need a SQL query for this?",2021-06-24T04:46:22Z
UgxEBxRjC4PtA-4zmFV4AaABAg.9Oxs-RdzWRh9OzmSeTDPVU,@stratascratch,UgxEBxRjC4PtA-4zmFV4AaABAg,2,T1UhSuKqy3A,0,0,2021-06-24T22:36:24Z,In sap bods? What do you mean?,2021-06-24T22:36:24Z
UgxEBxRjC4PtA-4zmFV4AaABAg.9Oxs-RdzWRh9P7960L7aXH,@gajjelapriyanka5301,UgxEBxRjC4PtA-4zmFV4AaABAg,2,T1UhSuKqy3A,0,0,2021-06-28T04:36:57Z,@@stratascratch need an SQL query for how to know the list of jobs name which was created in between the 2020june and 2021 june,2021-06-28T04:36:57Z
UgyCvJj3FjVPRi9ksVN4AaABAg,@shipragupta4382,,1,T1UhSuKqy3A,0,2,2021-06-22T08:35:31Z,"Hi Nate, Your videos are really helpful. Thanks. Can you make a video on &quot; How to transpose variable from rows to columns, using case when and pivot&quot;?",2021-06-22T08:35:31Z
Ugzq6cWMkzJI9gEPuWF4AaABAg,@InYourHorror,,1,T1UhSuKqy3A,1,1,2021-06-21T01:40:42Z,"Keep up great work, your videos helped me get a pretty good starter job right after uni.",2021-06-21T01:40:42Z
Ugzq6cWMkzJI9gEPuWF4AaABAg.9OpoMzQtR6S9Opzn72rL_O,@stratascratch,Ugzq6cWMkzJI9gEPuWF4AaABAg,2,T1UhSuKqy3A,0,0,2021-06-21T03:20:31Z,That‚Äôs great! I‚Äôm glad I was able to help in some way. Good luck in your new job.,2021-06-21T03:20:31Z
UgxPwJg7V93vnqHzfn14AaABAg,@TheBartboy007,,1,T1UhSuKqy3A,1,8,2021-06-20T19:14:19Z,"Thank you for putting these polished tutorials together. Very helpful in preparing for BI, Analyst and DS interviews. Thought I&#39;d share a little bit from my experience:<br><br>A reasonable goal to give yourself is to do at least one technical problem every day. While it will probably be difficult at first, it becomes a lot easier over time. For anyone studying for these interviews I can&#39;t stress how important repetition is. if you&#39;re new to sql or even just coding/scripting: at first it will seem like every sql question is novel and requires very specific knowledge but over time you will see that there are a limited number of &#39;tricks&#39; that can solve &gt;80% of these interview questions. Additionally, I also recommend that if you incorrectly solve a problem that you run through your incorrect solution one step at a time until you figure out exactly why your query fails. Otherwise you&#39;re setting yourself up to make the same mistake later. Lastly, if you screw up in the interview try your best to actually LISTEN to the interviewer and demonstrate that you understand their feedback. Please don&#39;t just pack it in when you make one mistake: no one wants to work with someone who can&#39;t/wont listen, and if you can respond to feedback/hints in a productive manner it might even mean more to the interviewer than a flawless solution. Hope this is helpful.",2021-06-20T19:14:19Z
UgxPwJg7V93vnqHzfn14AaABAg.9Op794APxf19OpIQ1zLUtQ,@stratascratch,UgxPwJg7V93vnqHzfn14AaABAg,2,T1UhSuKqy3A,0,0,2021-06-20T20:52:46Z,"Thanks so much for sharing, Gavin. Appreciate you sharing your experience. And totally agree with everything you just said.",2021-06-20T20:52:46Z
UgxLdKRJOrcfhWV-oix4AaABAg,@priyankalad7789,,1,T1UhSuKqy3A,4,0,2021-06-15T20:36:15Z,why we are not doing NITILE(50) over(partition by sum(order_total)  asc) instead ?,2021-06-15T20:36:15Z
UgxLdKRJOrcfhWV-oix4AaABAg.9OcOYkniu3C9OcQ2Qwp924,@stratascratch,UgxLdKRJOrcfhWV-oix4AaABAg,2,T1UhSuKqy3A,0,0,2021-06-15T20:49:19Z,I get where you&#39;re going with this. The result is just 1 NTILE when implemented for some reason. Did you have code on the platform that worked? Can you copy and paste it here?,2021-06-15T20:49:19Z
UgxLdKRJOrcfhWV-oix4AaABAg.9OcOYkniu3C9OcbUr6LVQH,@priyankalad7789,UgxLdKRJOrcfhWV-oix4AaABAg,2,T1UhSuKqy3A,0,0,2021-06-15T22:38:03Z,"@@stratascratch select a.restaurant_id,<br><a href=""http://a.total/"">a.total</a><br>from<br>(select restaurant_id,<br>sum(order_total) as total,<br>ntile(50) over (partition by sum(order_total) asc) as bucket<br>from doordash_delivery<br>where customer_placed_order_datetime between &#39;2020-05-01&#39; and &#39;2020-05-31&#39;<br>group by restaurant_id)a<br>where a.bucket = 1<br>order by 2 desc",2021-06-15T22:38:30Z
UgxLdKRJOrcfhWV-oix4AaABAg.9OcOYkniu3C9Od4jxP-RGi,@stratascratch,UgxLdKRJOrcfhWV-oix4AaABAg,2,T1UhSuKqy3A,0,0,2021-06-16T03:02:23Z,"@@priyankalad7789 This isn&#39;t executing for me on the platform. I get an error. But either way, I&#39;m not sure what the PARTITION is supposed to do to help solve the problem. Once I have it in 50 buckets, I just need to find ntile = 1 to get to the bottom 2% since each ntile is worth 2% of the distribution.",2021-06-16T03:02:23Z
UgxLdKRJOrcfhWV-oix4AaABAg.9OcOYkniu3C9Od8Y0Fz8n9,@priyankalad7789,UgxLdKRJOrcfhWV-oix4AaABAg,2,T1UhSuKqy3A,0,0,2021-06-16T03:35:35Z,@@stratascratch ok yea. Partition doesn&#39;t make sense here. This is first of this kind in your series of videos. Thanks,2021-06-16T03:35:35Z
Ugw0I1Azgz25jrSZSiB4AaABAg,@vrajpatel8256,,1,T1UhSuKqy3A,1,0,2021-06-15T05:14:31Z,Great video,2021-06-15T05:14:31Z
Ugw0I1Azgz25jrSZSiB4AaABAg.9Oak3oJf07z9OblzP-zHP-,@stratascratch,Ugw0I1Azgz25jrSZSiB4AaABAg,2,T1UhSuKqy3A,0,1,2021-06-15T14:50:30Z,Thanks for watching,2021-06-15T14:50:30Z
UgziXmphVXqnI8R1afV4AaABAg,@austinocampo2410,,1,2iE3JgNTwVU,1,1,2022-03-17T16:31:28Z,"each video it takes you 10 mins to explain that you write down logic before code. we get that already, please just move on to code execution please,",2022-03-17T16:31:28Z
UgziXmphVXqnI8R1afV4AaABAg.9Zg38YajUM89Zg5x5mNVZ9,@stratascratch,UgziXmphVXqnI8R1afV4AaABAg,2,2iE3JgNTwVU,0,3,2022-03-17T16:55:59Z,Just skip to the code execution part then. It&#39;s important for people new to interviewing to understand what needs to be done prior to writing actual code. That&#39;s actually more important than writing code.,2022-03-17T16:55:59Z
UgyQHlbGHObSVcOuUS94AaABAg,@tanmaysinghi1868,,1,2iE3JgNTwVU,0,0,2021-12-21T11:53:18Z,"thanks for the website, thanks for the content, its all a great help! providing free eduction is a noble work.",2021-12-21T11:53:18Z
UgzFI1Hg9gRwqzx_mu14AaABAg,@ahyoungkim8256,,1,2iE3JgNTwVU,0,0,2021-10-30T09:37:59Z,"Thank you , it‚Äôs so clear !",2021-10-30T09:37:59Z
UgxynRxA4dUsg7ol2QF4AaABAg,@TheFiratayrilik,,1,2iE3JgNTwVU,1,0,2021-10-20T06:30:01Z,"Great explanation. Excuse my question, What is &#39;interval&#39; for?",2021-10-20T06:30:01Z
UgxynRxA4dUsg7ol2QF4AaABAg.9ThtevDy5Ct9Tj3LUZ0HmM,@stratascratch,UgxynRxA4dUsg7ol2QF4AaABAg,2,2iE3JgNTwVU,0,1,2021-10-20T17:22:36Z,Thanks for watching. Interval is used to shift a timeframe based on the interview you choose. So now()-interval &#39;1 month&#39; will shift the date 1 month in the past from today&#39;s date.,2021-10-20T17:22:36Z
UgwHm5d87SObYFnTgsl4AaABAg,@fadwa2413,,1,2iE3JgNTwVU,0,0,2021-08-23T14:40:16Z,Always The Best,2021-08-23T14:40:16Z
Ugy0OdJqgT076FmKVbF4AaABAg,@ruprecht3109,,1,2iE3JgNTwVU,0,0,2021-08-17T00:12:42Z,Would the DATEADD() function work here as well?,2021-08-17T00:12:42Z
UgwQXoo7ZMhNLDG8yp94AaABAg,@tanyasinha2897,,1,2iE3JgNTwVU,1,0,2021-08-16T03:33:07Z,"Please let me know if I can use datediff(Month,transaction_start_date,Getdate()) for 10months. We can filter the output on 10?",2021-08-16T03:33:07Z
UgwQXoo7ZMhNLDG8yp94AaABAg.9R5CkM-ejuN9R6WYOZ7rE6,@stratascratch,UgwQXoo7ZMhNLDG8yp94AaABAg,2,2iE3JgNTwVU,0,0,2021-08-16T15:45:21Z,I think that&#39;s another good way to get the date diff. Try it out on the platform!,2021-08-16T15:45:21Z
UgwvhEOYoh0esxRRca54AaABAg,@joyo2122,,1,2iE3JgNTwVU,0,0,2021-08-09T21:01:21Z,i never communicated when i solved questions like this thx,2021-08-09T21:01:21Z
UgyAWbHZ34vQ1S_zobt4AaABAg,@benjaminappiah9558,,1,2iE3JgNTwVU,1,0,2021-07-20T22:58:21Z,"Nice example. I was wondering if we need the signups table, the signups ID is already in the transactions table and we are not getting anything unique from the signups.",2021-07-20T22:58:21Z
UgyAWbHZ34vQ1S_zobt4AaABAg.9Q1ld8s9DJF9Q2BCn0GAEg,@stratascratch,UgyAWbHZ34vQ1S_zobt4AaABAg,2,2iE3JgNTwVU,0,0,2021-07-21T02:50:33Z,"Yeah after taking a look at the code again, you might not need signups if you&#39;re using an inner join. The assumption there is that you only care about the users in the transactions table (users that have made a transaction). However, if you want to keep a list of all users (users from the signups table), you&#39;ll do a left join so that you keep the 0s of users that never made a transaction. I probably needed to either use a LEFT JOIN if I had that assumption or not use the signup table if my assumption is that I don&#39;t care about users that did not make a transaction. Thanks for catching that.",2021-07-21T02:50:33Z
UgyxxG9TEDJHDFln-4V4AaABAg,@bemanojkumar722,,1,2iE3JgNTwVU,1,0,2021-06-15T15:58:56Z,"Hey I see many videos in the playlist are deleted, or may be we are not able to view it could you please look into it  @Nate ?",2021-06-15T15:58:56Z
UgyxxG9TEDJHDFln-4V4AaABAg.9ObtodvdwyX9Oc3iNTKM7G,@stratascratch,UgyxxG9TEDJHDFln-4V4AaABAg,2,2iE3JgNTwVU,0,2,2021-06-15T17:34:12Z,"Yes, some have been deleted as the questions on the platform have either been re-written or removed. I&#39;m going to be adding more videos in the upcoming weeks/months to replace the ones that were deleted. Thanks",2021-06-15T17:34:12Z
Ugx2Mzw_4GDGYhh_9hB4AaABAg,@khushimehta6574,,1,2iE3JgNTwVU,1,1,2021-06-12T19:50:08Z,Great content. This is really helpful. Thank you Nate.,2021-06-12T19:50:08Z
Ugx2Mzw_4GDGYhh_9hB4AaABAg.9OV_tK1FcZH9OXj7KAffbZ,@stratascratch,Ugx2Mzw_4GDGYhh_9hB4AaABAg,2,2iE3JgNTwVU,0,0,2021-06-13T15:49:18Z,Thanks for watching! Glad you found it helpful.,2021-06-13T15:49:18Z
Ugz4bxTtJN9sLpdrmb94AaABAg,@kylehuang7926,,1,2iE3JgNTwVU,5,0,2021-06-06T22:32:32Z,"I really love your framework to tackle SQL live coding interview. I just interviewed with Uber for a SQL question, but what i did not realized is that I was not able to preview the dataset like I usually did on the platform, so I was panic and did not fully understand the question and gave wrong answer... should have watched this viz earlier lol",2021-06-06T22:32:32Z
Ugz4bxTtJN9sLpdrmb94AaABAg.9OGQhbQR5Se9OJ5ADA1hdG,@stratascratch,Ugz4bxTtJN9sLpdrmb94AaABAg,2,2iE3JgNTwVU,0,0,2021-06-07T23:22:04Z,"ah yes, I&#39;ve realized that many people don&#39;t know that you typically can&#39;t preview the datasets on interviews. I&#39;ve only just now started talking about solving problems without viewing the data. This video would have helped =)",2021-06-07T23:22:04Z
Ugz4bxTtJN9sLpdrmb94AaABAg.9OGQhbQR5Se9R8eghK1Ub8,@tanneremery9297,Ugz4bxTtJN9sLpdrmb94AaABAg,2,2iE3JgNTwVU,0,0,2021-08-17T11:43:45Z,i guess I am kinda off topic but does anyone know of a good place to stream newly released series online?,2021-08-17T11:43:45Z
Ugz4bxTtJN9sLpdrmb94AaABAg.9OGQhbQR5Se9R8h63wlf6C,@harveymason3329,Ugz4bxTtJN9sLpdrmb94AaABAg,2,2iE3JgNTwVU,0,0,2021-08-17T12:04:49Z,@Tanner Emery Flixportal :),2021-08-17T12:04:49Z
Ugz4bxTtJN9sLpdrmb94AaABAg.9OGQhbQR5Se9R8ixstsjE4,@tanneremery9297,Ugz4bxTtJN9sLpdrmb94AaABAg,2,2iE3JgNTwVU,0,0,2021-08-17T12:21:03Z,"@Harvey Mason thank you, I went there and it seems like they got a lot of movies there :D I really appreciate it !",2021-08-17T12:21:03Z
Ugz4bxTtJN9sLpdrmb94AaABAg.9OGQhbQR5Se9R8k5v8hzOl,@harveymason3329,Ugz4bxTtJN9sLpdrmb94AaABAg,2,2iE3JgNTwVU,0,0,2021-08-17T12:31:01Z,@Tanner Emery No problem :D,2021-08-17T12:31:01Z
UgyAmH-QzzrNGNTGVAV4AaABAg,@priyankalad7789,,1,2iE3JgNTwVU,1,1,2021-06-05T05:07:04Z,"where t.transaction_start_date &lt; DATEADD(month,-10,getdate()) will this work instead of now() - 10 * interval &#39;1 month&#39;",2021-06-05T05:07:04Z
UgyAmH-QzzrNGNTGVAV4AaABAg.9OBzGW6KhvC9ODaXAooM-8,@stratascratch,UgyAmH-QzzrNGNTGVAV4AaABAg,2,2iE3JgNTwVU,0,1,2021-06-05T20:09:23Z,they both work! depends on what sql engine you&#39;re using but both are good to use.,2021-06-05T20:09:23Z
UgwSzMIFrRLZQtaKmP54AaABAg,@golammuhaimeen2825,,1,2iE3JgNTwVU,1,3,2021-06-04T15:20:18Z,really great walkthroughs Nate! love StrataScratch! it&#39;s really helped me a lot,2021-06-04T15:20:18Z
UgwSzMIFrRLZQtaKmP54AaABAg.9OAVe4Y_jia9OAlmsUu23z,@stratascratch,UgwSzMIFrRLZQtaKmP54AaABAg,2,2iE3JgNTwVU,0,1,2021-06-04T17:50:03Z,That&#39;s great! I&#39;m glad you like the walkthrough series!,2021-06-04T17:50:03Z
Ugxx7TQXXQsVJGmUj494AaABAg,@bruhyoukidding2036,,1,2iE3JgNTwVU,2,1,2021-06-04T06:02:41Z,Respectfully asking for more SQL questions (especially for FB) on strata,2021-06-04T06:02:41Z
Ugxx7TQXXQsVJGmUj494AaABAg.9O9VpsIl10D9OAT1qX5D_A,@stratascratch,Ugxx7TQXXQsVJGmUj494AaABAg,2,2iE3JgNTwVU,0,0,2021-06-04T14:57:28Z,Will keep on adding questions to StrataScratch and making these videos! I&#39;ll try to find more FB ones as I collect them!,2021-06-04T14:57:28Z
Ugxx7TQXXQsVJGmUj494AaABAg.9O9VpsIl10D9OAuk8VYgZN,@bruhyoukidding2036,Ugxx7TQXXQsVJGmUj494AaABAg,2,2iE3JgNTwVU,0,0,2021-06-04T19:08:19Z,@@stratascratch thanks! Really enjoyed my membership,2021-06-04T19:08:19Z
UgxlkCLskGTNCQNryRN4AaABAg,@VloggingMemories,,1,xtbMCAVXDmU,0,0,2023-02-08T11:49:19Z,"My Solution:<br>with month_dataset as<br>(select <br>    to_char(time_id, &#39;MM-YYYY&#39;) as month_year,<br>    string_agg(user_id, &#39;;#&#39;) as user_id_list<br>from fact_events<br>group by 1),<br>user_status as<br>(select distinct<br>    to_char(t1.time_id, &#39;MM-YYYY&#39;) as month_year,<br>    t1.user_id,<br>    case <br>        when t2.user_id_list &lt;&gt; &#39;&#39; <br>            then &#39;existing user&#39;<br>            else &#39;new user&#39;<br>    end as user_status<br>from fact_events t1<br>left join month_dataset t2 on <br>        to_char(t1.time_id, &#39;MM-YYYY&#39;) &gt; t2.month_year<br>        and t2.user_id_list like &#39;%&#39; || t1.user_id ||&#39;%&#39;)<br>select <br>    month_year,<br>    round(sum(existing_user)/ (sum(existing_user)+sum(new_user))::decimal, 2) share_existing_users,<br>    round(sum(new_user)/ (sum(existing_user)+sum(new_user))::decimal, 2) share_new_users,<br>    sum(existing_user)+sum(new_user) all_users<br>from<br>(select <br>    month_year,<br>    case <br>        when user_status=&#39;existing user&#39;<br>        then 1<br>        else 0<br>    end as existing_user,<br>    case <br>        when user_status=&#39;new user&#39;<br>        then 1<br>        else 0<br>    end as new_user<br>from user_status)x<br>group by 1<br>order by 1;",2023-02-08T11:49:19Z
UgydctcVfWiYYVQyqCt4AaABAg,@chillvibe4745,,1,xtbMCAVXDmU,0,0,2022-09-24T12:56:09Z,"Here is my solution:<br>WITH users AS (    <br>    SELECT  EXTRACT(month from time_id) AS month, user_id, DENSE_RANK() OVER (PARTITION BY user_id ORDER BY  EXTRACT(month from time_id)) AS rnk<br>    FROM fact_events<br>    ORDER BY month )<br>SELECT month, COUNT(DISTINCT CASE WHEN rnk = 1 THEN user_id END) / COUNT(DISTINCT user_id)::float AS share_of_new_users, COUNT(DISTINCT CASE WHEN rnk != 1 THEN user_id END) / COUNT(DISTINCT user_id)::float AS share_of_existing_users<br>FROM users<br>GROUP BY month",2022-09-24T12:56:09Z
UgxJYklYoH_OxBP6tMF4AaABAg,@oscararmandocisnerosruvalc8503,,1,xtbMCAVXDmU,0,0,2022-08-15T18:03:21Z,why are we using 1 - and then the formula ???.<br>regards,2022-08-15T18:03:21Z
UgwWZDz1hrkOkjP4-5d4AaABAg,@TheMHud,,1,xtbMCAVXDmU,0,0,2022-03-31T21:37:29Z,I don‚Äôt know how you do it but after watching one of your videos I always come up with a solution for one of my coding problems. I am new on my job as a business analyst and am setting up dashboards for the company. Thank you so much for this quality content - keep up the good work.,2022-03-31T21:37:29Z
UgxcsDciCaHyRGymOoZ4AaABAg,@dannyrodin1151,,1,xtbMCAVXDmU,1,0,2022-01-15T11:20:29Z,I don&#39;t get the &quot;with&quot; at the beginning. Too bad you never explained it.,2022-01-15T11:20:29Z
UgxcsDciCaHyRGymOoZ4AaABAg.9XDR3U0cug-9XEIlg2r92u,@stratascratch,UgxcsDciCaHyRGymOoZ4AaABAg,2,xtbMCAVXDmU,0,1,2022-01-15T19:27:15Z,Sorry about that. I assume there&#39;s a level of experience people have with SQL since these vids focus on how to approach the solution as if you&#39;re on an interview. WITH is the start of a CTE (common table expression) that&#39;s popular in postgres. Not all db engines have this function. MySQL doesn&#39;t allow for this for example. But you can always create temp tables or subqueries in place of CTEs.,2022-01-15T19:27:15Z
UgyyfNRMA-rKyu1j0G14AaABAg,@ajaxaj6749,,1,xtbMCAVXDmU,0,0,2021-12-23T17:37:44Z,we can use lag function instead,2021-12-23T17:37:44Z
Ugx5g5EvJU4I_3tHw8V4AaABAg,@mohitnisar3866,,1,xtbMCAVXDmU,1,0,2021-12-07T11:37:29Z,"Hey, just found out about your channel, been going through videos almost all day and wanted to give my thanks! The content is amazing.",2021-12-07T11:37:29Z
Ugx5g5EvJU4I_3tHw8V4AaABAg.9Ve2-s15vRL9Vfms9oOW2L,@stratascratch,Ugx5g5EvJU4I_3tHw8V4AaABAg,2,xtbMCAVXDmU,0,0,2021-12-08T03:54:58Z,Thanks so much! More to come!,2021-12-08T03:54:58Z
UgwhuHQLyXRiYVickxB4AaABAg,@vivekkamisetti4,,1,xtbMCAVXDmU,1,1,2021-12-05T12:21:28Z,"Hi Nate, I think there&#39;s one thing wrong about this query. What if for a month there are 0 new users? By doing an inner join you&#39;re missing out on that logic.<br><br>In my opinion, the right way to do it is doing a left join between all users and new users and then assign a 0 wherever there&#39;s a null(in new users table).<br><br>Let me know what you think.",2021-12-05T12:21:28Z
UgwhuHQLyXRiYVickxB4AaABAg.9VZySFpazh49V__5YjnJp9,@stratascratch,UgwhuHQLyXRiYVickxB4AaABAg,2,xtbMCAVXDmU,0,0,2021-12-05T17:59:09Z,I think you&#39;re correct with this edge case. I like that logic!,2021-12-05T17:59:09Z
UgyDNv4y72BV6oX-mNN4AaABAg,@VinodKumar-nn7go,,1,xtbMCAVXDmU,1,0,2021-10-24T20:01:33Z,"Hi Nate, In the problem statement it says  ratio of new users to &quot;existing users&quot;. I was wondering whether we should exclude the new users from all users and then get the existing users?? Thanks",2021-10-24T20:01:45Z
UgyDNv4y72BV6oX-mNN4AaABAg.9TtdiCQOk4s9TtqXVwrP2R,@stratascratch,UgyDNv4y72BV6oX-mNN4AaABAg,2,xtbMCAVXDmU,0,0,2021-10-24T21:53:33Z,"Yes, you can do that! i think that makes sense. I think it just depends on what the interviewer defines as &quot;existing users&quot;.",2021-10-24T21:53:33Z
Ugxy6GrfKUJiP-06u8Z4AaABAg,@joaopedroreissilva7075,,1,xtbMCAVXDmU,1,0,2021-10-18T13:10:14Z,Impressive.<br>The people who work with you and especially the team you lead are certainly very lucky.,2021-10-18T13:10:14Z
Ugxy6GrfKUJiP-06u8Z4AaABAg.9TdSsRcjwsL9TdeQu9wv00,@stratascratch,Ugxy6GrfKUJiP-06u8Z4AaABAg,2,xtbMCAVXDmU,0,1,2021-10-18T14:59:56Z,"Thanks, buddy! Really appreciate it",2021-10-18T14:59:56Z
UgzdPAvQQCReWEsSxI14AaABAg,@albinsantiago2686,,1,xtbMCAVXDmU,1,1,2021-10-01T13:47:29Z,Videos are great. Thanks.,2021-10-01T13:47:29Z
UgzdPAvQQCReWEsSxI14AaABAg.9SxkcKbyjxg9SxnZciLQP0,@stratascratch,UgzdPAvQQCReWEsSxI14AaABAg,2,xtbMCAVXDmU,0,0,2021-10-01T14:13:11Z,Thanks for watching! üòé,2021-10-01T14:13:11Z
UgwB-DIKQE5C7mUBRFx4AaABAg,@ismafoot11,,1,xtbMCAVXDmU,1,1,2021-08-21T16:31:47Z,"You dont need the count(distinct user_id) at <a href=""https://www.youtube.com/watch?v=xtbMCAVXDmU&amp;t=9m47s"">9:47</a>. you did a min in your inner query which keeps once instance of that column per user_id. you said a user could show up multiple times in the same month but the min takes care of that",2021-08-21T16:32:32Z
UgwB-DIKQE5C7mUBRFx4AaABAg.9RJTptGwJ-19UcvX6LN8Db,@paul_devos,UgwB-DIKQE5C7mUBRFx4AaABAg,2,xtbMCAVXDmU,0,0,2021-11-12T04:41:29Z,"that&#39;s correct, when using a GROUP BY here you don&#39;t even need a DISTINCT as the &quot;Map -&gt; Reduce&quot; like feature of the Group By would do all the work. And depending on the query engine underneath, you rarely ever want to use DISTINCT as it could lead to a cartesian product (n!) timespace. Altho, I do think most query engines underneath, now know to substitute the group by for distinct anyway.",2021-11-12T04:41:29Z
UgwX6f7p0k3eK0PZRJ14AaABAg,@Nadyusa,,1,xtbMCAVXDmU,0,0,2021-08-19T13:02:31Z,"I have been doing similar task at work utilising my colleagues shorter approach. In cte I count new user date, then in the main query I calculate shares like this: sum(iif(new user date&gt;=month,1,0))/count(user_id), and group by month. Less subqueries, would it be quicker as well? And thanks for great videos, looking forward to solving many problems with window functions you taught.",2021-08-19T13:02:31Z
UgwnCzuMnmPI5Nlz7VF4AaABAg,@sujayshashank1198,,1,xtbMCAVXDmU,1,1,2021-07-15T18:32:23Z,Why we need to use distinct user_id ?? When we already have filtered out the data in subquery and did a group by on user_id and fetched the min(time_id).<br>So even if user would be having multiple entries in the same month but will result only one row when we did min(time_id),2021-07-15T18:32:23Z
UgwnCzuMnmPI5Nlz7VF4AaABAg.9PpQDj8oIC-9PpjVrx7O5x,@stratascratch,UgwnCzuMnmPI5Nlz7VF4AaABAg,2,xtbMCAVXDmU,0,1,2021-07-15T21:29:37Z,"absolutely true. No reason to use a distinct but it just is a fail safe in case there&#39;s a duplicate user somehow. As you mentioned, there shouldn&#39;t be.",2021-07-15T21:29:37Z
Ugykqc5AurUXbZ1ghYJ4AaABAg,@ajeetvishwakarma7964,,1,xtbMCAVXDmU,2,1,2021-07-13T01:09:46Z,‚ÄúNew users are defined as users who started using services in the current month.‚Äù But where are we checking that condition? Your solution checks min value of the time_id column. As per my understanding the current month should the month in which we exceute the query. Could you please explain.,2021-07-13T01:09:46Z
Ugykqc5AurUXbZ1ghYJ4AaABAg.9PiPJbA3MXV9Pk-2g8nLdF,@stratascratch,Ugykqc5AurUXbZ1ghYJ4AaABAg,2,xtbMCAVXDmU,0,0,2021-07-13T15:58:44Z,"We are checking that condition with the min() function as you mentioned. In the outer query, we extract the month from the min(time_id) of the user to get to when the user executed the query.",2021-07-13T15:58:44Z
Ugykqc5AurUXbZ1ghYJ4AaABAg.9PiPJbA3MXV9YMpWkL8hKV,@pranavlal2289,Ugykqc5AurUXbZ1ghYJ4AaABAg,2,xtbMCAVXDmU,0,0,2022-02-12T23:27:30Z,"@@stratascratch Thanks Nate, but I agree with Ajeet here, the question clearly mentions that the user who starts in the current month is the new user, we need to check the month(min(time_id)) to be the same as the month of current_date and that should be the users who are new users. What do you think?",2022-02-12T23:27:30Z
UgwUg0HS37IHBKPcP7N4AaABAg,@ameyraj4947,,1,xtbMCAVXDmU,1,0,2021-05-23T19:56:49Z,Awesome loved the videoüéâüéâ,2021-05-23T19:56:49Z
UgwUg0HS37IHBKPcP7N4AaABAg.9Nh5kqvmfnV9Nh8a1lGd4i,@stratascratch,UgwUg0HS37IHBKPcP7N4AaABAg,2,xtbMCAVXDmU,0,0,2021-05-23T20:21:34Z,Thanks so much! check out the other ones on my channel!,2021-05-23T20:21:34Z
UgznEmaj44z6tIe2_fp4AaABAg,@leonardlau4951,,1,xtbMCAVXDmU,1,0,2021-05-10T00:32:35Z,"i love your videos narrating your mind map of creating queries, which is very helpful for sql learners!",2021-05-10T03:25:04Z
UgznEmaj44z6tIe2_fp4AaABAg.9N8YBOCDmId9N8qJUJILE6,@stratascratch,UgznEmaj44z6tIe2_fp4AaABAg,2,xtbMCAVXDmU,0,1,2021-05-10T03:19:43Z,Thank you! Hope you keep watching!,2021-05-10T03:19:43Z
UgxXHrQOJGfWNAsaOfN4AaABAg,@oliviaou1940,,1,xtbMCAVXDmU,1,0,2021-05-08T04:12:45Z,great video! I&#39;ve learned a lot through your videos!! Thanks. Please keep posting this kind of videos,2021-05-08T04:12:45Z
UgxXHrQOJGfWNAsaOfN4AaABAg.9N3mn5BqGZv9N4qhDIhu5Z,@stratascratch,UgxXHrQOJGfWNAsaOfN4AaABAg,2,xtbMCAVXDmU,0,0,2021-05-08T14:06:08Z,I definitely will! Thank you for watching my videos,2021-05-08T14:06:08Z
UgxIZYCt5gdhdsKyjkd4AaABAg,@jerrygong8976,,1,xtbMCAVXDmU,1,1,2021-05-05T18:50:59Z,good video. only catch is that you don&#39;t need the &#39;distinct&#39; keyword in the first CTE because it&#39;s already grouped by user_id and user_id will always be distinct in that case,2021-05-05T18:50:59Z
UgxIZYCt5gdhdsKyjkd4AaABAg.9MycuvO65lh9MynSbCZsRn,@stratascratch,UgxIZYCt5gdhdsKyjkd4AaABAg,2,xtbMCAVXDmU,0,0,2021-05-05T20:23:06Z,Thanks so much for the insight and glad to see you&#39;re watching the video! You&#39;re definitely right about now needing distinct.,2021-05-05T20:23:06Z
UgzDFM4ODuxtoyS171x4AaABAg,@myrandomandboringvideos,,1,xtbMCAVXDmU,3,1,2021-05-03T13:13:12Z,Your videos are helpful. Insights on how to break a problem statement are very detailed.,2021-05-03T13:13:12Z
UgzDFM4ODuxtoyS171x4AaABAg.9Mssf5KLTRO9Mtk5viBoxl,@stratascratch,UgzDFM4ODuxtoyS171x4AaABAg,2,xtbMCAVXDmU,0,1,2021-05-03T21:17:35Z,Thanks for the kind words! I&#39;ll keep doing the best I can with these videos!,2021-05-03T21:17:35Z
UgzDFM4ODuxtoyS171x4AaABAg.9Mssf5KLTRO9N6sM8i4yM0,@myrandomandboringvideos,UgzDFM4ODuxtoyS171x4AaABAg,2,xtbMCAVXDmU,0,0,2021-05-09T08:59:05Z,"@@stratascratch Hey Nate, I was solving this question &#39;Finding User Purchases&#39; and wrote this query but this is not getting accepted as the right solution. if it is okay, could you please let me know why?<br><br>select <br>    distinct user_id<br>from<br>    (<br>    Select <br>        user_id, <br>        created_at - last_purchase_date as days_difference<br>        from<br>    (<br>            SELECT<br>                user_id,<br>                created_at,<br>                lag(created_at,1) over (partition by user_id order by created_at) as last_purchase_date<br>            FROM AMAZON_TRANSACTIONS<br>            order by user_id asc, created_at asc<br>    ) a<br>    ) b<br>    where days_difference &gt;1 and difference &lt; 8",2021-05-09T08:59:05Z
UgzDFM4ODuxtoyS171x4AaABAg.9Mssf5KLTRO9N7vJp3cJav,@stratascratch,UgzDFM4ODuxtoyS171x4AaABAg,2,xtbMCAVXDmU,0,1,2021-05-09T18:44:13Z,@@myrandomandboringvideos Thanks for giving it a try and definitely can help. But can you post this code and your question on the user forum of the platform? Either myself or one of my teammates will help debug the code for you =),2021-05-09T18:44:13Z
UgxA_B-ZbyVN7aT7nc54AaABAg,@kamakshijoshi3655,,1,xtbMCAVXDmU,1,0,2021-05-02T08:19:07Z,"Hello Nate, quiet good video, just wanted to know that the question you share, are these type of question useful for a Data Engineer Sql rounds as well?",2021-05-02T08:19:07Z
UgxA_B-ZbyVN7aT7nc54AaABAg.9MpmDM1RkFX9MqbkRJFimo,@stratascratch,UgxA_B-ZbyVN7aT7nc54AaABAg,2,xtbMCAVXDmU,0,1,2021-05-02T16:06:54Z,"Yes, some are useful for DE sql interviews as well. Later in the year I&#39;ll do better to distinguish DS with DE questions and release some DE specific questions.",2021-05-02T16:06:54Z
UgxrSGxJ5-7FiSz4WM14AaABAg,@maheshchandra3833,,1,xtbMCAVXDmU,3,0,2021-04-30T13:47:04Z,Just came across your videos and I really like your explanations! I was checking out StrataScratch and was wondering how the difficulty level maps to the difficulty level in Leetcode Database. Are your ‚ÄúHard‚Äù questions harder than Leetcode ‚ÄúHard‚Äù?,2021-04-30T13:47:04Z
UgxrSGxJ5-7FiSz4WM14AaABAg.9MlD9VhMzlt9Mlasz_dwLQ,@stratascratch,UgxrSGxJ5-7FiSz4WM14AaABAg,2,xtbMCAVXDmU,0,0,2021-04-30T17:23:07Z,"Sort of...StrataScratch has data science type of questions so the approach and solution is much different than Leetcode which tends to be more algorithm based. The LC hard database questions are of the level of the SS hard questions in terms of syntax and functions needed in the solution, but I think the hard questions for SS are much harder since there&#39;s a data science aspect to the question.",2021-04-30T17:23:07Z
UgxrSGxJ5-7FiSz4WM14AaABAg.9MlD9VhMzlt9MldYb8Z6OZ,@maheshchandra3833,UgxrSGxJ5-7FiSz4WM14AaABAg,2,xtbMCAVXDmU,0,0,2021-04-30T17:46:25Z,"@@stratascratch Thanks for the reply. For a new grad SQL interview, would you say &quot;Hard&quot; level SS questions are overkill? Or you think &quot;Hard&quot; level questions are quite common?",2021-04-30T17:46:25Z
UgxrSGxJ5-7FiSz4WM14AaABAg.9MlD9VhMzlt9Mlh0SFOBOF,@stratascratch,UgxrSGxJ5-7FiSz4WM14AaABAg,2,xtbMCAVXDmU,0,0,2021-04-30T18:16:42Z,‚Äã@@maheshchandra3833 No it&#39;s not quite overkill but you probably won&#39;t get too many of those questions. Make sure you know to do the medium ones for sure. That&#39;s most common. You&#39;ll get 1-2 hard ones during the later rounds.,2021-04-30T18:16:42Z
Ugx7sMOjs__iwiiUe8R4AaABAg,@priyankalad7789,,1,xtbMCAVXDmU,2,0,2021-04-27T22:17:13Z,@Nate Could you please post some more data science or data engineer SQL questions?,2021-04-27T22:17:13Z
Ugx7sMOjs__iwiiUe8R4AaABAg.9MeP9KYDnGa9MgK7YnxNfO,@stratascratch,Ugx7sMOjs__iwiiUe8R4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-28T16:11:45Z,Yes! I will keep posting both ds and de style sql questions in the future. I have a variety of other types of content coming out over the next few months but coding questions is something I plan to keep doing.,2021-04-28T16:11:45Z
Ugx7sMOjs__iwiiUe8R4AaABAg.9MeP9KYDnGa9MgOrsSupwm,@priyankalad7789,Ugx7sMOjs__iwiiUe8R4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-28T16:53:10Z,@@stratascratch Great Thanks for your contribution,2021-04-28T16:53:10Z
UgxKC_3VEkak5wKhb9h4AaABAg,@SuperLOLABC,,1,xtbMCAVXDmU,3,0,2021-04-26T18:57:15Z,"Hey Nate, is it necessary to know the time complexity of queries during the interview? I understand it is expected in a coding interview but is it necessary for SQL interviews too?",2021-04-26T18:57:15Z
UgxKC_3VEkak5wKhb9h4AaABAg.9MbTTw6U36G9MbgFOTilrI,@stratascratch,UgxKC_3VEkak5wKhb9h4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-26T20:57:36Z,Time complexities no. But understanding what can make execution time longer in your queries is definitely fair game. You just need to point out parts of your code where you could potentially optimize to speed execution time.,2021-04-26T20:57:36Z
UgxKC_3VEkak5wKhb9h4AaABAg.9MbTTw6U36G9MbpxJn0QE0,@SuperLOLABC,UgxKC_3VEkak5wKhb9h4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-26T22:22:23Z,@@stratascratch Thank you for your prompt reply! I have an interview coming up in two weeks and you along with your platform are a godsend!,2021-04-26T22:22:23Z
UgxKC_3VEkak5wKhb9h4AaABAg.9MbTTw6U36G9MdulqEgC6P,@stratascratch,UgxKC_3VEkak5wKhb9h4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-27T17:42:59Z,@@SuperLOLABC Good luck! Hope you do well!,2021-04-27T17:42:59Z
UgwZ-6TkHcEasujta194AaABAg,@SuperLOLABC,,1,xtbMCAVXDmU,2,0,2021-04-19T16:18:22Z,"Great video, Nate! I am a new grad and I have a question; whenever I try to solve a medium or hard problem, I can‚Äôt seem to think beyond ‚Äòjoining‚Äô tables. I can‚Äôt seem to think of using CTEs or sub queries. Can you point me to any resources or use cases where I can learn to use CTEs/subqueries intuitively? Also are CTEs and subquiries have the same use case?",2021-04-19T16:18:22Z
UgwZ-6TkHcEasujta194AaABAg.9MK9j5rR0k-9MKQ068C4J8,@stratascratch,UgwZ-6TkHcEasujta194AaABAg,2,xtbMCAVXDmU,0,3,2021-04-19T18:40:38Z,"I think it really just takes more practice. I have an entire playlist dedicated to these questions and many of them require CTEs/subqueries. With more practice, you&#39;ll get an understanding of how to think about your approach to the problem and organically will start using ctes/subqueries in your solution. So suggest doing more problems on StrataScratch and Leetcode to get comfortable with solving these types of questions. Hackerrank is another source that could help. But I don&#39;t know many other platforms that give you a lot of practice problems.",2021-04-19T18:40:38Z
UgwZ-6TkHcEasujta194AaABAg.9MK9j5rR0k-9MKfHcLeftk,@SuperLOLABC,UgwZ-6TkHcEasujta194AaABAg,2,xtbMCAVXDmU,0,1,2021-04-19T21:02:50Z,@@stratascratch Thanks for your reply. Practice does make one perfect I guess. Great work on the platform by you and your team!,2021-04-19T21:02:50Z
UgxCz2hwMz7p7XYj43N4AaABAg,@PATRICKCHUAD,,1,xtbMCAVXDmU,1,0,2021-04-16T12:19:39Z,Very Clear explanation. Thanks Nate for creating a video for this problem.,2021-04-16T12:19:39Z
UgxCz2hwMz7p7XYj43N4AaABAg.9MC015ad4BO9MCOc6VTdKD,@stratascratch,UgxCz2hwMz7p7XYj43N4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-16T15:54:34Z,Thanks for watching!,2021-04-16T15:54:34Z
UgwVniL9SdEvwL_ANXl4AaABAg,@anumitamondal1320,,1,xtbMCAVXDmU,3,0,2021-04-15T16:24:09Z,Nate could you post some tutorial about Data Modelling question,2021-04-15T16:24:09Z
UgwVniL9SdEvwL_ANXl4AaABAg.9M9sCvEWkgd9MAMsgUCpoi,@stratascratch,UgwVniL9SdEvwL_ANXl4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-15T21:00:52Z,"Yea, I&#39;ll add it to the queue. I have a few months of videos on the list right now but we can do some modeling questions afterwards. I&#39;m assuming these are for data engineering interviews?",2021-04-15T21:00:52Z
UgwVniL9SdEvwL_ANXl4AaABAg.9M9sCvEWkgd9MK9QE28lAe,@anumitamondal1320,UgwVniL9SdEvwL_ANXl4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-19T16:15:39Z,"@@stratascratch  Yes Nate. For Data Engineer , there&#39;s a section for Data Modelling and forming the SQL on top of it. If you can make some video on that..it would be nice. Waiting for it.",2021-04-19T16:15:39Z
UgwVniL9SdEvwL_ANXl4AaABAg.9M9sCvEWkgd9MKQGrsgsaD,@stratascratch,UgwVniL9SdEvwL_ANXl4AaABAg,2,xtbMCAVXDmU,0,0,2021-04-19T18:42:55Z,"@@anumitamondal1320 I&#39;ll see if I can find any questions on data modeling. If I don&#39;t have one in my current repository, I will try to find one. I plan to add some DE questions to this channel and stratascratch later in the year so there will be some more content around in later. Thanks for offering this suggestion.",2021-04-19T18:42:55Z
UgyS1SWdjdQVie3Jx7J4AaABAg,@its_me7363,,1,xtbMCAVXDmU,1,2,2021-04-15T14:36:53Z,waiting for API videos,2021-04-15T14:36:53Z
UgyS1SWdjdQVie3Jx7J4AaABAg.9M9fwFO3Aq39M9kBnqQhpP,@stratascratch,UgyS1SWdjdQVie3Jx7J4AaABAg,2,xtbMCAVXDmU,0,2,2021-04-15T15:14:05Z,"It&#39;s right here (<a href=""https://youtu.be/fklHBWow8vE)"">https://youtu.be/fklHBWow8vE)</a>. I walk through using the requests library for python to extract data from the Youtube API. But you can extend the request library for any API. In about 2 weeks, I&#39;ll have the next step of the process which is saving the data to a database in the cloud.",2021-04-15T15:14:05Z
Ugwc3MKpyaWr-UTPTNl4AaABAg,@adwaithks,,1,XRwxYOhHdE8,0,0,2023-04-29T14:38:36Z,"I did it this way : <br><br>with usa_people_count as (<br>select count(*) as total_people_count from fb_active_users where country = &#39;USA&#39;<br>), active_people_count_in_usa as (<br>select count(*) as active_people_count from fb_active_users where country = &#39;USA&#39; and status = &#39;open&#39;<br>)<br><br>select cast(active_people_count as decimal) / total_people_count as share from usa_people_count, active_people_count_in_usa;",2023-04-29T14:38:36Z
UgwfI7XyaoY5s5PCN7B4AaABAg,@friendsplain,,1,XRwxYOhHdE8,1,0,2023-04-07T21:00:52Z,Nate your content is gold! Thanks for your detailed walkthrough of the problem and solution. It is a huge help for data science interviewees like me!,2023-04-07T21:00:52Z
UgwfI7XyaoY5s5PCN7B4AaABAg.9oDSoNTWaEH9oEsg-PfKma,@stratascratch,UgwfI7XyaoY5s5PCN7B4AaABAg,2,XRwxYOhHdE8,0,0,2023-04-08T10:14:53Z,"Awesome, thank you!",2023-04-08T10:14:53Z
Ugyi9Q_kbEJ5K9T2vg14AaABAg,@VloggingMemories,,1,XRwxYOhHdE8,0,0,2023-02-08T13:00:18Z,"My Solution:<br>select us_active_users/us_total_users::decimal share_active_users<br>from<br>(select <br>    count(case <br>            when status=&#39;open&#39; and country=&#39;USA&#39; <br>                then user_id else null end) us_active_users,<br>    count(case<br>            when country=&#39;USA&#39; <br>                then user_id else null end) us_total_users<br>from fb_active_users)a;",2023-02-08T13:00:18Z
UgxYKs3l-i_7tTmgHY54AaABAg,@dwaipayansaha4443,,1,XRwxYOhHdE8,1,1,2022-08-06T07:27:05Z,"Hi Nate, my solution is:-<br>select (count(*))/(select count(*) from fb_active_users where country=&#39;USA&#39;) as monthly_share from fb_active_users<br>where status=&#39;open&#39; and country=&#39;USA&#39;;",2022-08-06T07:27:05Z
UgxYKs3l-i_7tTmgHY54AaABAg.9eNii_koVVb9eOxCkROHZg,@stratascratch,UgxYKs3l-i_7tTmgHY54AaABAg,2,XRwxYOhHdE8,0,1,2022-08-06T18:52:55Z,That is just wonderful!,2022-08-06T18:52:55Z
UgzaoSif9ytTaSeqlWF4AaABAg,@madhulikasuman2803,,1,XRwxYOhHdE8,2,1,2022-07-15T18:36:29Z,"hey guys, does the fb data analyst interviews has whiteboard or IDE for sql test?",2022-07-15T18:36:29Z
UgzaoSif9ytTaSeqlWF4AaABAg.9dWGqCZmwSu9dWIgCdGLx8,@stratascratch,UgzaoSif9ytTaSeqlWF4AaABAg,2,XRwxYOhHdE8,0,0,2022-07-15T18:52:36Z,"From what I recall, their video screening (1st round) is a SQL test that&#39;s on their platform called BlueJeans (or something like that). Basically it&#39;s a glorified notepad. You can&#39;t execute any code. The in-person rounds are whiteboard. Hope that helps!",2022-07-15T18:52:36Z
UgzaoSif9ytTaSeqlWF4AaABAg.9dWGqCZmwSu9dWVtLn5xCX,@madhulikasuman2803,UgzaoSif9ytTaSeqlWF4AaABAg,2,XRwxYOhHdE8,0,0,2022-07-15T20:47:59Z,"@@stratascratch yes..that is right, thanks alot üòä",2022-07-15T20:47:59Z
Ugx90PgETmQ2h0q2-j54AaABAg,@relaxationmusic7094,,1,XRwxYOhHdE8,0,0,2022-03-11T02:38:38Z,Really nice video. Thank you so much!,2022-03-11T02:38:38Z
UgztBd44swtkWpSie454AaABAg,@nilsaha8021,,1,XRwxYOhHdE8,1,0,2022-03-04T21:04:27Z,"User_id 82: country Australia, User_id 34: Donald Ross country China in the table. But when you filtered out the country, both of those users are shown in the output. Could you please explain it?",2022-03-04T21:04:27Z
UgztBd44swtkWpSie454AaABAg.9ZA42V1QHXn9ZAPwZy1wAU,@stratascratch,UgztBd44swtkWpSie454AaABAg,2,XRwxYOhHdE8,0,0,2022-03-05T00:15:44Z,What&#39;s the code you used? Not sure why you&#39;re getting that,2022-03-05T00:15:44Z
Ugx-e3n2pq5DgeAvVWl4AaABAg,@ranitdey5829,,1,XRwxYOhHdE8,0,0,2022-02-28T09:22:33Z,Your content is amazing. I love the way you have explained the solutions.,2022-02-28T09:22:33Z
UgznJPMnkkFTezo2NR14AaABAg,@bindidesai8564,,1,XRwxYOhHdE8,0,0,2022-02-25T07:02:40Z,"An attendance log for every student in a school district attendance_events :<br>date | student_id | attendance<br>‚Ä¢ A summary table with demographics for each student in the district all_<br>students : student_id | school_id | grade_level | date_of_birth | hometown<br>Using this data, you could answer questions like the following:<br>‚Ä¢ What percent of students attend school on their birthday?<br>‚Ä¢ Which grade level had the largest drop in attendance between yesterday<br>and today?",2022-02-25T07:02:40Z
UgzAt-pYB-AnsVfw6yt4AaABAg,@ranajparida5412,,1,XRwxYOhHdE8,0,0,2022-02-10T17:58:46Z,"Hey Nat, by looking the question, i am not able to find where question is asking about the ratio..???",2022-02-10T17:58:46Z
Ugwd4QlvbkTil_mKqKh4AaABAg,@Arslanqadri,,1,XRwxYOhHdE8,1,2,2022-02-01T00:49:33Z,you are not allowed to explore the data,2022-02-01T00:49:33Z
Ugwd4QlvbkTil_mKqKh4AaABAg.9Xt4NCaSfoi9YGvepVWzRt,@intrepidgrovyle9950,Ugwd4QlvbkTil_mKqKh4AaABAg,2,XRwxYOhHdE8,0,0,2022-02-10T16:25:43Z,"yeah exactly. during the interview you are just given a preview, and a static CoderPad environment with no way to run and test output",2022-02-10T16:25:43Z
Ugy1eiZBIHOTitGH8El4AaABAg,@MUSKAN0896,,1,XRwxYOhHdE8,0,0,2022-01-25T17:09:39Z,This was really helpful! Thank you very much. Please do more such videos!,2022-01-25T17:09:39Z
UgynvpybIKpGHu3qG2d4AaABAg,@zmey2k,,1,XRwxYOhHdE8,1,0,2022-01-09T10:37:46Z,"I wrote my code just before you explained your solution and the only thing I changed is I passed the step with doubling user_id checking, instead I counted them with distinct prefix &quot;count(distinct user_id)&quot;, everything else was the same (including nested select).",2022-01-09T10:37:46Z
UgynvpybIKpGHu3qG2d4AaABAg.9WyuPJkznyD9WzYLyT_p5M,@stratascratch,UgynvpybIKpGHu3qG2d4AaABAg,2,XRwxYOhHdE8,0,0,2022-01-09T16:35:34Z,That looks like it works! Great job and thanks for following along with the video.,2022-01-09T16:35:34Z
UgzDDx6ANzSYbLTf_KN4AaABAg,@adolfoir6177,,1,XRwxYOhHdE8,2,0,2021-12-24T08:44:52Z,"Hi Nate,<br><br>Does the SQL implementation (Oracle, MySQL, PostgreSQL..) matters when interviewing with FB?<br><br>Which one would you recommend for FB particularly",2021-12-24T08:44:52Z
UgzDDx6ANzSYbLTf_KN4AaABAg.9WKVlNe9NMZ9WLNPqGJwe3,@stratascratch,UgzDDx6ANzSYbLTf_KN4AaABAg,2,XRwxYOhHdE8,0,2,2021-12-24T16:51:07Z,It doesn&#39;t at all. You basically just get a notepad when interviewing with FB. None of your code will execute so you can write in any SQL flavor you want.,2021-12-24T16:51:07Z
UgzDDx6ANzSYbLTf_KN4AaABAg.9WKVlNe9NMZ9WLZROKBMyQ,@adolfoir6177,UgzDDx6ANzSYbLTf_KN4AaABAg,2,XRwxYOhHdE8,0,0,2021-12-24T18:36:11Z,"@@stratascratch <br>Thank you Nate, this is very helpful",2021-12-24T18:36:11Z
Ugxn7m-FuxQ6zNjWmrt4AaABAg,@hhliu7287,,1,XRwxYOhHdE8,1,1,2021-11-26T20:11:49Z,"Hi Nate, I didn&#39;t get it why  you didn&#39;t select the distinct user during your code. as count(case when status=&#39;open&#39; then user_id else null)<br>if a user showed twice, it will be 2 for his count",2021-11-26T20:11:49Z
Ugxn7m-FuxQ6zNjWmrt4AaABAg.9VDd7HfhU959VDvUQMu-02,@stratascratch,Ugxn7m-FuxQ6zNjWmrt4AaABAg,2,XRwxYOhHdE8,0,0,2021-11-26T22:52:16Z,That&#39;s a great point! I should have caught that edge case but I didn&#39;t. We should definitely add a distinct so we don&#39;t double count.,2021-11-26T22:52:16Z
UgxFko93U9gOhOqNHmZ4AaABAg,@shobhamourya8396,,1,XRwxYOhHdE8,0,0,2021-10-22T08:08:51Z,Here&#39;s my solution:<br>select count(case when status = &#39;open&#39; then user_id else null end)*1.0/count(*)*1.0 as active_user_share<br>from fb_active_users<br>where country = &#39;USA&#39;,2021-10-22T08:08:51Z
UgwbeEQD2oEIluX0aHB4AaABAg,@chrisshaw1707,,1,XRwxYOhHdE8,2,0,2021-10-12T05:08:49Z,Is casting the ratio as a float something that needs to be done only in certain SQL environments? The mysql editor on leetcode does produce a ratio when dividing an integer by another integer. Maybe this is because I used the round function though to go out 2 decimal places?,2021-10-12T05:08:49Z
UgwbeEQD2oEIluX0aHB4AaABAg.9TO905XYkR39TPYKGBFIMH,@stratascratch,UgwbeEQD2oEIluX0aHB4AaABAg,2,XRwxYOhHdE8,0,0,2021-10-12T18:09:15Z,"It depends on the data type of the number. If they are both integers and you&#39;re dividing them, then you&#39;ll get an integer as the output, regardless of SQL engine being used. But if one of the numbers is a numeric or float then you&#39;ll get back a float/numeric. I would be wary of sql editors on platforms like leetcode, hackerrank, and stratascratch because there&#39;s a lot of things going on in the background that are outside of normal sql behavior. For example, with stratascratch we pass the sql output to python so that we can run our solution validation algorithm to see if you got the right answer. The conversion from sql to python isn&#39;t always clean and isn&#39;t always tied to how sql would normally behave...that&#39;s why you see our float outputs rounded to the nearest 3rd decimal. Hope this makes sense.",2021-10-12T18:09:15Z
UgwbeEQD2oEIluX0aHB4AaABAg.9TO905XYkR39TYElyyswQK,@chrisshaw1707,UgwbeEQD2oEIluX0aHB4AaABAg,2,XRwxYOhHdE8,0,0,2021-10-16T03:11:35Z,"@@stratascratch Awesome answer, thanks!",2021-10-16T03:11:35Z
Ugw-H1_vC3mTXgRP9lF4AaABAg,@mensenvau,,1,XRwxYOhHdE8,0,0,2021-08-31T18:13:49Z,How can I apply for a Data Base on facebook?.,2021-08-31T18:14:02Z
Ugz2vlGkBBjvImHIdGV4AaABAg,@Zzzz-hk5ft,,1,XRwxYOhHdE8,0,0,2021-08-26T00:17:37Z,Solid thanks man,2021-08-26T00:17:37Z
Ugw98OzJyTA-XCelxYp4AaABAg,@charansai1133,,1,XRwxYOhHdE8,2,0,2021-07-02T10:15:49Z,Hi nate your vedios boost my confidence man Thanks a Lot,2021-07-02T10:15:49Z
Ugw98OzJyTA-XCelxYp4AaABAg.9PI33QtAexx9PItaQQlYA6,@stratascratch,Ugw98OzJyTA-XCelxYp4AaABAg,2,XRwxYOhHdE8,0,0,2021-07-02T18:03:35Z,Glad to have helped! Good luck on your interviews if you have any lined up,2021-07-02T18:03:35Z
Ugw98OzJyTA-XCelxYp4AaABAg.9PI33QtAexx9PItvb89GjU,@charansai1133,Ugw98OzJyTA-XCelxYp4AaABAg,2,XRwxYOhHdE8,0,0,2021-07-02T18:06:28Z,@@stratascratch I want to do my master&#39;s on data engineering any suggestion,2021-07-02T18:06:28Z
Ugy6WNSMCxJ4nop6MFR4AaABAg,@neelanshunisingh971,,1,XRwxYOhHdE8,1,0,2021-06-27T20:15:09Z,Which portal you use for practise?,2021-06-27T20:15:09Z
Ugy6WNSMCxJ4nop6MFR4AaABAg.9P6FfjAcY1z9P6HGvXSNdX,@stratascratch,Ugy6WNSMCxJ4nop6MFR4AaABAg,2,XRwxYOhHdE8,0,0,2021-06-27T20:29:06Z,"I&#39;m using <a href=""http://www.stratascratch.com/"">www.stratascratch.com</a>",2021-06-27T20:29:06Z
UgzLdeqiFFJ5q70_rz94AaABAg,@tarekelias463,,1,XRwxYOhHdE8,0,0,2021-06-20T17:06:51Z,Thanks a lot for that!,2021-06-20T17:06:51Z
UgxRDOyLPT0qn3aTm8h4AaABAg,@mrblahblihblih,,1,XRwxYOhHdE8,3,5,2021-05-30T23:54:37Z,"Hi Nate, awesome video! Your content is really helping me with my upcoming interview with Facebook. I was wondering, would it have been possible to just do a left join with two subqueries instead? Or are you just optimizing for runtime",2021-05-30T23:54:37Z
UgxRDOyLPT0qn3aTm8h4AaABAg.9NzYXcgBrjJ9O0jvPFKs55,@stratascratch,UgxRDOyLPT0qn3aTm8h4AaABAg,2,XRwxYOhHdE8,0,3,2021-05-31T20:21:20Z,"You could do that but it would be really inefficient and unnecessary. This query is actually already pretty inefficient. I wouldn&#39;t even use a subquery to answer this. I would just use the case statements in the numerator and perform a count(*) for the denominator. That is probably the most efficient way to solve it. If you go to the platform, there is a tab called &quot;solution from other users&quot; where you&#39;ll find other ways users have solved the question. Some are really clever and concise. Goodluck on FB!",2021-05-31T20:21:20Z
UgxRDOyLPT0qn3aTm8h4AaABAg.9NzYXcgBrjJ9O1Mb68ufoO,@mrblahblihblih,UgxRDOyLPT0qn3aTm8h4AaABAg,2,XRwxYOhHdE8,0,0,2021-06-01T02:08:05Z,@@stratascratch got it thanks! Would you say writing the most efficient query is important in the interview?,2021-06-01T02:08:05Z
UgxRDOyLPT0qn3aTm8h4AaABAg.9NzYXcgBrjJ9O1TPWPKJ3a,@stratascratch,UgxRDOyLPT0qn3aTm8h4AaABAg,2,XRwxYOhHdE8,0,1,2021-06-01T03:07:32Z,@@mrblahblihblih It&#39;s not that important in an interview. Not THE most efficient that is. Your code should make sense and not have a bunch of unnecessary sections/clauses. You&#39;ll usually have an opportunity in the end to optimize it so you can tell the interviewer how you&#39;d write more efficient code.,2021-06-01T03:07:32Z
UgxUUCyLOXAimE7p5HJ4AaABAg,@radhsri,,1,XRwxYOhHdE8,1,0,2021-05-27T05:06:16Z,"Hi Nate, thanks for the awesome videos. At <a href=""https://www.youtube.com/watch?v=XRwxYOhHdE8&amp;t=7m13s"">7:13</a>,  I dont understand how count(NULL) will NOT count that particular row. I was going to use SUM(CASE(WHEN status = open, 0, 1)) but will a COUNT skip a row having a NULL value? thanks",2021-05-27T05:06:57Z
UgxUUCyLOXAimE7p5HJ4AaABAg.9Npo0juLwhk9NqvW9XXZhw,@stratascratch,UgxUUCyLOXAimE7p5HJ4AaABAg,2,XRwxYOhHdE8,0,2,2021-05-27T15:30:58Z,"That&#39;s basically how the functions are meant to work. count() won&#39;t count any NULL rows and sum() will count all rows whether Null or not. In your case the sum() + Case statement is how you should approach it if you want to count only opens. Here&#39;s more info <a href=""https://discuss.codecademy.com/t/when-do-we-use-count-or-sum/351543"">https://discuss.codecademy.com/t/when-do-we-use-count-or-sum/351543</a>",2021-05-27T15:30:58Z
Ugzb_Kdo636GZFmbFSx4AaABAg,@priyankalad7789,,1,XRwxYOhHdE8,1,0,2021-04-13T07:33:05Z,I am afraid if in real interviews we can test parts of logic or carry out code increment? What do we do in that case?,2021-04-13T07:34:26Z
Ugzb_Kdo636GZFmbFSx4AaABAg.9M3lqWYqPOc9M4g-jT87Ty,@stratascratch,Ugzb_Kdo636GZFmbFSx4AaABAg,2,XRwxYOhHdE8,0,2,2021-04-13T16:01:17Z,"Yea total valid point. In an interview, I would just split up the logic into steps and then write the code in steps. Then pause so that the interviewer can evaluate the code. This way they can catch any problems before you write out the entire solution. This framework has saved me several times on interviews =)",2021-04-13T16:01:17Z
UgyyT3Rr8sVHLCBk1eB4AaABAg,@cookiemonstauhh,,1,XRwxYOhHdE8,1,1,2021-03-22T16:24:30Z,Hi Nate i just wanna say thanks for the videos! Your data science interview question with solutions are really helpful and engaging:),2021-03-22T16:24:44Z
UgyyT3Rr8sVHLCBk1eB4AaABAg.9LC4AXL9lSB9LC7IpgEaYT,@stratascratch,UgyyT3Rr8sVHLCBk1eB4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-22T16:51:51Z,Thank you and thanks for watching these videos. I try to make these videos from the POV of the interviewee or at least how I would want an interviewee to answer the question if I was the interviewer.,2021-03-22T16:51:51Z
Ugwcqz8z8Mg5Ht5Cwl14AaABAg,@vrajpatel8256,,1,XRwxYOhHdE8,1,0,2021-03-15T12:17:16Z,Great video!,2021-03-15T12:17:16Z
Ugwcqz8z8Mg5Ht5Cwl14AaABAg.9KubJYahPCx9Kuvlx-cszL,@stratascratch,Ugwcqz8z8Mg5Ht5Cwl14AaABAg,2,XRwxYOhHdE8,0,0,2021-03-15T15:16:02Z,Thank you for watching!,2021-03-15T15:16:02Z
Ugxx_ghnBEvkVphv9wV4AaABAg,@MrJohn2brown,,1,XRwxYOhHdE8,5,1,2021-03-11T19:14:15Z,"Hey @ Nate, great video‚Äôs and thanks so much for the practical scenarios. I have been asked about Mapping in a data analysis interview and was told alot of mapping will be part ofthe job. Any videos on that any time soon ? Thanks",2021-03-11T19:14:15Z
Ugxx_ghnBEvkVphv9wV4AaABAg.9Kl2r8lo4EB9KlHTNTiNJh,@stratascratch,Ugxx_ghnBEvkVphv9wV4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-11T21:21:56Z,Hi Tahir. Great job on your analysis interview so far. Can you elaborate on mapping? What do you mean exactly?,2021-03-11T21:21:56Z
Ugxx_ghnBEvkVphv9wV4AaABAg.9Kl2r8lo4EB9KlLlbRkiLz,@MrJohn2brown,Ugxx_ghnBEvkVphv9wV4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-11T21:59:31Z,"@@stratascratch thank you, Hopefully better luck next time. Mapping data base. I didn‚Äôt understand and probably thats why I didn‚Äôt pass. The company had Apache Hive. Im guessing it had to do with creating tables not just query from them. Is that a normal task of a data analysts ?",2021-03-11T21:59:31Z
Ugxx_ghnBEvkVphv9wV4AaABAg.9Kl2r8lo4EB9KlWat6qs-l,@stratascratch,Ugxx_ghnBEvkVphv9wV4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-11T23:34:10Z,@@MrJohn2brown To me mapping means creating tables where you&#39;re mapping one table to another. Basically it&#39;s an intermediate table that connects 2 other tables together. Sometimes there&#39;s a lot of clean up and logic required to map 2 tables together so a mapping/intermediate table can be used. Or another definition could be creating a table that has data from multiple other tables together. Like if you have a user table but some info comes from engagement tables and other info comes from a payments table. That could be considered a mapping table too. Both require creating databases. Hope that helps.,2021-03-11T23:34:10Z
Ugxx_ghnBEvkVphv9wV4AaABAg.9Kl2r8lo4EB9KmQFoQwhw3,@MrJohn2brown,Ugxx_ghnBEvkVphv9wV4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-12T07:57:58Z,"@@stratascratch I see, so its like joining on temp tables. Thanks Nate",2021-03-12T07:57:58Z
Ugxx_ghnBEvkVphv9wV4AaABAg.9Kl2r8lo4EB9KnSZXnQnIF,@stratascratch,Ugxx_ghnBEvkVphv9wV4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-12T17:37:23Z,@@MrJohn2brown Similar yes. That&#39;s my guess without actually seeing the question or exercise you were required to do =),2021-03-12T17:37:23Z
UgxrgRo0jB10nSYCsuR4AaABAg,@PATRICKCHUAD,,1,XRwxYOhHdE8,7,1,2021-03-11T04:19:23Z,"Hi Nate, now I&#39;m beginning to practice the steps on solving the problem, not just go directly and dive to the coding. your steps is very logical and can be very advantageous in more complex question. Divide and conquer is the focus to come up with a logical solution. Thanks for continuous sharing your knowledge.",2021-03-11T04:19:23Z
UgxrgRo0jB10nSYCsuR4AaABAg.9KjSRvLLdKz9KlH_f2DvWD,@stratascratch,UgxrgRo0jB10nSYCsuR4AaABAg,2,XRwxYOhHdE8,0,1,2021-03-11T21:22:56Z,Thanks for continuing to watch the videos. I&#39;ll keep pumping more and more out!,2021-03-11T21:22:56Z
UgxrgRo0jB10nSYCsuR4AaABAg.9KjSRvLLdKz9KlTUK7sxXR,@PATRICKCHUAD,UgxrgRo0jB10nSYCsuR4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-11T23:06:55Z,@@stratascratch BTW I&#39;m incline to subscribe to your platform for Premium account  to learn in deep on SQL solution for data science and maybe python as well. Does StrataScratch team help out if member is having difficulty solving any of hard problem in there ?,2021-03-11T23:06:55Z
UgxrgRo0jB10nSYCsuR4AaABAg.9KjSRvLLdKz9KlWLlnKnes,@stratascratch,UgxrgRo0jB10nSYCsuR4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-11T23:31:58Z,"@@PATRICKCHUAD Hey that&#39;s great. yes, we do help with any questions you have. There&#39;s a discussion board for each question. If you leave a question, we can answer them for you. It takes about 2-3 days for an answer. Check out the free questions to see the video solutions and discussion forum before upgrading. See if you like it!",2021-03-11T23:31:58Z
UgxrgRo0jB10nSYCsuR4AaABAg.9KjSRvLLdKz9Kl_1W0Fiun,@PATRICKCHUAD,UgxrgRo0jB10nSYCsuR4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-12T00:04:09Z,@@stratascratch Yes I have visited the site already several time and also tried to solve the problem in there. That is why I want to upgrade to premium to know if my solution is correct and also to know other solution possible to solve the problem. I pretty much like the platform. I maybe get the 1 year premium plan since it is a lot cheaper compared to the monthly one.,2021-03-12T00:04:09Z
UgxrgRo0jB10nSYCsuR4AaABAg.9KjSRvLLdKz9KldNWX6F-e,@PATRICKCHUAD,UgxrgRo0jB10nSYCsuR4AaABAg,2,XRwxYOhHdE8,0,0,2021-03-12T00:42:07Z,Seems no paypal option for payment. Is it correct ?,2021-03-12T00:42:07Z
UgxeXjTM-HuRfw-H5qh4AaABAg,@tamaboy311,,1,QenwDm5oWdU,1,0,2023-07-30T21:01:21Z,I love SQL. It&#39;s almost like a game.,2023-07-30T21:01:21Z
UgxeXjTM-HuRfw-H5qh4AaABAg.9so-Uc31bOt9soWG6hOeqX,@stratascratch,UgxeXjTM-HuRfw-H5qh4AaABAg,2,QenwDm5oWdU,0,1,2023-07-31T01:47:43Z,I always thought of them as puzzles!,2023-07-31T01:47:43Z
UgxAlkbmFE4jdYhYd894AaABAg,@Seikenguy,,1,QenwDm5oWdU,1,0,2023-07-16T07:55:01Z,"This was a &quot;Hard&quot; question on stratascratch, but you explained it in a way that one might think it&#39;s elementary. Since I&#39;ve had experiences with teachers explaining these concepts where I understood virtually nothing, I think you&#39;re a really great teacher. Thank you!",2023-07-16T07:55:01Z
UgxAlkbmFE4jdYhYd894AaABAg.9sDYN5T0LP09s_rUECwUIo,@stratascratch,UgxAlkbmFE4jdYhYd894AaABAg,2,QenwDm5oWdU,0,0,2023-07-25T09:13:16Z,Glad it was helpful!,2023-07-25T09:13:16Z
Ugz9NGPNWunvBCt8dMN4AaABAg,@VloggingMemories,,1,QenwDm5oWdU,0,0,2023-02-19T16:18:17Z,"My Solution:<br>select <br>    to_char(created_at::date, &#39;YYYY-MM&#39;) year_month,<br>    round(((sum(value) - lag(sum(value)) over (order by to_char(created_at::date, &#39;YYYY-MM&#39;)))/lag(sum(value)) over (order by to_char(created_at::date, &#39;YYYY-MM&#39;)))*100, 2) revenue_diff<br>from sf_transactions<br>group by 1<br>order by 1;",2023-02-19T16:18:17Z
Ugwa2_1BxgG-TgnxHqV4AaABAg,@Lakshmi1010,,1,QenwDm5oWdU,0,0,2022-09-20T05:20:27Z,"Hi Sir, I am learning advanced SQL and have a query here ... How its working when Year_month is not given in group by clause before aggregation of data by year_month wise ?",2022-09-20T05:20:27Z
Ugz3Wug_h279HDi735d4AaABAg,@morninglory8988,,1,QenwDm5oWdU,0,0,2022-09-19T12:47:21Z,"I have a question about that TO_CHAR part. It basically converted the date data type to a string data type. So for example, If I wanted to do some calculations on these dates (for example how many days have passed from one date to another) I wouldn&#39;t be able to do so, right? <br>That TO_CHAR part is only to let us order by the month, right?",2022-09-19T12:47:21Z
UgxQWT-p4BL7UlGweJ94AaABAg,@AlirezaAminian,,1,QenwDm5oWdU,1,1,2022-09-08T06:47:58Z,"Hi, thanks for the video. I have a question though. When you the GROUP BY clause, you only put year_month. However, when using GROUP BY, you have to indicate all the columns specified in SELECT in the GROUP BY clause otherwise it will through an error. Can you please advise?",2022-09-08T06:47:58Z
UgxQWT-p4BL7UlGweJ94AaABAg.9fhcU-Jvq7k9g-WCM21hXT,@stratascratch,UgxQWT-p4BL7UlGweJ94AaABAg,2,QenwDm5oWdU,0,0,2022-09-15T14:49:58Z,"This is the case, but not when there are window functions. The window functions are not allowed in the GROUP BY clause. They work on the query result; that&#39;s why it works without putting the revenue_diff column in the GROUP BY.",2022-09-15T14:49:58Z
UgyuUrcc5RJBqvwU5nZ4AaABAg,@bodwiser100,,1,QenwDm5oWdU,2,0,2022-07-08T10:35:12Z,"Thanks for the video! quick question - in the over() clause, why did we not use partition by instead of order by?",2022-07-08T10:35:12Z
UgyuUrcc5RJBqvwU5nZ4AaABAg.9dDOC-E_RJH9dWZqJVcPrE,@stratascratch,UgyuUrcc5RJBqvwU5nZ4AaABAg,2,QenwDm5oWdU,0,0,2022-07-15T21:22:31Z,"Do you mean to simply replace the ORDER BY clause with the PARTITION BY? In this case, the code would be:<br><br>SELECT to_char(created_at::date, &#39;YYYY-MM&#39;) AS year_month,<br>(sum(value) - lag(sum(value), 1) OVER (PARTITION BY to_char(created_at::date, &#39;YYYY-MM&#39;)))/<br>lag(sum(value), 1) OVER (PARTITION BY to_char(created_at::date, &#39;YYYY-MM&#39;))<br>FROM sf_transactions<br>GROUP BY year_month<br><br>This wouldn&#39;t work because, in the LAG() and LEAD() functions, the ORDER BY clause is mandatory. In other words, these functions won&#39;t work without it.<br><br>Also, the PARTITION BY and ORDER BY clauses have different purposes in the window functions. They can&#39;t be simply replaced. If that&#39;s what you&#39;re asking.<br><br>The PARTITION BY groups data into subsets according to the specified column. ORDER BY determines the order of the window function execution. In this case, it&#39;s from the oldest to the newest month.<br><br>Does that answer your question?",2022-07-15T21:22:31Z
UgyuUrcc5RJBqvwU5nZ4AaABAg.9dDOC-E_RJH9ego-9Zv8bE,@localmartian9047,UgyuUrcc5RJBqvwU5nZ4AaABAg,2,QenwDm5oWdU,0,0,2022-08-14T02:37:59Z,"Here partition by was not needed because we need to apply lag on whole table not just inside a window over some col. Like this. We could have taken an empty partition by in spark sql which would be the same.<br><br>While order by yyyy_mm was needed because we want to find data of previous month to find the revenue difference. All window functions like lead, lag, rank, dense rank, row number need the window to be ordered so that applying the function makes sense unlike normal agg functions like avg sum",2022-08-14T02:37:59Z
UgyECFWwHDZZrTaUtgJ4AaABAg,@ItsWithinYou,,1,QenwDm5oWdU,1,0,2022-06-19T13:54:23Z,Many thanks for sharing this example!,2022-06-19T13:54:23Z
UgyECFWwHDZZrTaUtgJ4AaABAg.9cSotFzOErr9eGmhTvh-aG,@stratascratch,UgyECFWwHDZZrTaUtgJ4AaABAg,2,QenwDm5oWdU,0,0,2022-08-03T14:47:12Z,Glad it was helpful!,2022-08-03T14:47:12Z
Ugy09M88nvKOs-EqoCZ4AaABAg,@KumarSameer,,1,QenwDm5oWdU,1,0,2022-05-11T02:22:40Z,Thanks for breaking it down in this great detail. It really gives me a structured approach to solving interview SQL questions!,2022-05-11T02:22:40Z
Ugy09M88nvKOs-EqoCZ4AaABAg.9as9ipAILm09kUzkhP4Ey_,@stratascratch,Ugy09M88nvKOs-EqoCZ4AaABAg,2,QenwDm5oWdU,0,0,2023-01-05T06:18:58Z,Very welcome!,2023-01-05T06:18:58Z
UgwQfD7xy9uA9EG1eQt4AaABAg,@vibekdutta6539,,1,QenwDm5oWdU,0,0,2022-05-01T19:23:27Z,"Solid explanation, looking forward to some more awesome contents, thanks a lot",2022-05-01T19:23:39Z
Ugw4cOP_5STFJzoaFFR4AaABAg,@TheHamous,,1,QenwDm5oWdU,0,0,2022-04-27T13:20:50Z,"interesting, but i doubt that this is an amzon test interview because i think window aliasing is not available in aws redshift isnt it?",2022-04-27T13:20:50Z
Ugy_2fm_s3agsjYOCq14AaABAg,@myskillset5833,,1,QenwDm5oWdU,0,0,2022-03-29T19:39:04Z,Good  üëç explanation.  Is there possibility for data set so more helpful for  practice @ local  üñ•,2022-03-29T19:39:04Z
UgzGf41GqYQanZhcM9d4AaABAg,@awsomeboyz8112,,1,QenwDm5oWdU,1,1,2022-03-19T19:26:12Z,"Hi, i notice in some of your videos you say that you cant put the alias in the group by clause, but here you do. Is there general rules of thumb in terms of when you can group by an alias vs writing the full derived column out?",2022-03-19T19:26:12Z
UgzGf41GqYQanZhcM9d4AaABAg.9ZlWj5AcnoV9Zm-HxrbpRz,@stratascratch,UgzGf41GqYQanZhcM9d4AaABAg,2,QenwDm5oWdU,0,0,2022-03-19T23:53:15Z,"Great attention to detail =). It depends on the SQL engine dialect you&#39;re on. With postgres, you can use an alias. But with other engines like HIVE, you can&#39;t. The platform, StrataScratch, is on postgres and MySQL so you can use an alias. Hope that helps.",2022-03-19T23:53:15Z
UgwR_1SYBSOSl4HKxql4AaABAg,@arkamukherjee457,,1,QenwDm5oWdU,0,0,2022-03-08T06:01:06Z,"For MySQL, I used: date_format(created_at, &#39;%Y-%m&#39;)",2022-03-08T06:01:06Z
UgzJnCYmuUM-FQMiYNJ4AaABAg,@rick2591,,1,QenwDm5oWdU,0,0,2022-03-01T17:20:30Z,I never knew about a window alias. That is cool. PostgreSQL has a lot of cool features. embedding an update within a with clause allows one to enter and leave a critical section within one SQL statement....,2022-03-01T17:20:30Z
UgwniP_BBNxsmeuVMIN4AaABAg,@nareshvadlamudi,,1,QenwDm5oWdU,0,0,2022-02-15T21:54:37Z,thanks a lot.! great example to cover many topics!,2022-02-15T21:54:37Z
Ugz4JHq3V9i7Vq-Om4Z4AaABAg,@pallavi_joseph,,1,QenwDm5oWdU,0,0,2022-02-08T20:37:42Z,You are a star,2022-02-08T20:37:42Z
Ugzuvm87QtXBlp-n9914AaABAg,@iklintsov,,1,QenwDm5oWdU,0,0,2022-02-08T10:38:56Z,Is this only me understood &quot;last month&#39;s revenue&quot; as revenue for &quot;2019-12&quot;? Wouldn&#39;t it be better to call &quot;next month&#39;s revenue&quot;?,2022-02-08T10:38:56Z
UgxE8a2E6z8-_3B1CZd4AaABAg,@SuddenlySubtle,,1,QenwDm5oWdU,0,1,2022-02-06T21:22:21Z,"Thank you for the video!<br><br>What would happen if a particular month had no values? Then lag of 1 would not work... Self join on month and the calculated version of previous month, in a CTE could work... A month key = year*12 + month...prev month = month - 1",2022-02-06T21:22:21Z
UgzUql28qJhZicrA1BR4AaABAg,@brianchuck9264,,1,QenwDm5oWdU,1,0,2022-01-25T04:12:53Z,"Great content.. Keep it coming! If possible please reference differences in syntaxes across the different rdms.For me, the explanation of the double colon helped...as an example..  I use snowflake and terradata",2022-01-25T04:12:53Z
UgzUql28qJhZicrA1BR4AaABAg.9XbQ4PB5xzx9Xd1AtRL8Fn,@stratascratch,UgzUql28qJhZicrA1BR4AaABAg,2,QenwDm5oWdU,0,0,2022-01-25T19:13:48Z,Will do! I use HIVE at work so I get what you&#39;re saying. Will add more explanations between nuances. Thanks for the feedback.,2022-01-25T19:13:48Z
UgxW7ZwFdGg5V8VUqI94AaABAg,@bandhammanikanta6302,,1,QenwDm5oWdU,0,0,2021-12-24T14:13:37Z,"The way you explain is excellent, Nate. Keep it up, please. Thank you :)",2021-12-24T14:13:37Z
Ugz8qSRPqj_LODVN6Bl4AaABAg,@bandhammanikanta6302,,1,QenwDm5oWdU,0,0,2021-12-24T11:20:02Z,"Thank you for putting this video explanation.<br><br>but When I searched for this question &quot;Monthly percentage difference stratascratch&quot;, youtube suggested me this video but I wasn&#39;t sure if this the exact one until I hit the question link from description.<br>Please change the title according to the question. Thank you.",2021-12-24T11:20:02Z
UgzrRFrbElZ8-t07oPV4AaABAg,@mithunnambiar1433,,1,QenwDm5oWdU,3,0,2021-12-12T18:49:15Z,Amazing vid!!! Great going....Could you also the schema used in the creation of this problem?,2021-12-12T18:49:15Z
UgzrRFrbElZ8-t07oPV4AaABAg.9VrgOaG5X9M9Vs1sheZsHs,@stratascratch,UgzrRFrbElZ8-t07oPV4AaABAg,2,QenwDm5oWdU,0,0,2021-12-12T22:05:44Z,"Sorry, could I what with the schema?",2021-12-12T22:05:44Z
UgzrRFrbElZ8-t07oPV4AaABAg.9VrgOaG5X9M9VsfGY_c2fp,@mithunnambiar1433,UgzrRFrbElZ8-t07oPV4AaABAg,2,QenwDm5oWdU,0,0,2021-12-13T03:58:39Z,@@stratascratch typo*... Could you also upload the schema on your site,2021-12-13T03:58:39Z
UgzrRFrbElZ8-t07oPV4AaABAg.9VrgOaG5X9M9Vsg4MdI_he,@stratascratch,UgzrRFrbElZ8-t07oPV4AaABAg,2,QenwDm5oWdU,0,0,2021-12-13T04:05:44Z,@@mithunnambiar1433 In the description you&#39;ll see a link to the platform. And it has the schema below the question. Hope that helps!,2021-12-13T04:05:44Z
UgwNr3Bj7NG6U7QX8UF4AaABAg,@amberyang802,,1,QenwDm5oWdU,1,0,2021-12-01T18:19:06Z,why not using partition by in the window function instead of group by?,2021-12-01T18:19:06Z
UgwNr3Bj7NG6U7QX8UF4AaABAg.9VQJC8kbm5l9VRAvW4w4xB,@stratascratch,UgwNr3Bj7NG6U7QX8UF4AaABAg,2,QenwDm5oWdU,0,1,2021-12-02T02:26:02Z,Yea you can probably do that too. Many solutions to these types of problems. That&#39;s the beauty of it!,2021-12-02T02:26:02Z
UgxJvgJKi4jclBaq4TB4AaABAg,@laurak5196,,1,QenwDm5oWdU,4,0,2021-11-30T18:41:09Z,Thanks so much for sharing.   I‚Äôm new to this concept.  What was the window clause called?   And why was it at the end?   I was thinking of a cte which would be at beginning along with the with clause?    So this was something new for me.   Thanks again for your help.,2021-11-30T18:41:09Z
UgxJvgJKi4jclBaq4TB4AaABAg.9VNlvf8Ig659VOsMFO4GoD,@stratascratch,UgxJvgJKi4jclBaq4TB4AaABAg,2,QenwDm5oWdU,0,1,2021-12-01T04:56:35Z,It&#39;s just called an ALIAS. It&#39;s in the end just because that&#39;s where it&#39;s usually written. It&#39;s not something that&#39;s used a lot but it does help with code readability. You don&#39;t have to use it. Glad you learned something!,2021-12-01T04:56:35Z
UgxJvgJKi4jclBaq4TB4AaABAg.9VNlvf8Ig659VPuUQo2OU3,@laurak5196,UgxJvgJKi4jclBaq4TB4AaABAg,2,QenwDm5oWdU,0,0,2021-12-01T14:34:25Z,"@@stratascratch I always learn something from your work, even if it‚Äôs just hearing your thought process.  I get impatient when I get stuck on a problem, and I know I need more practice.   You and others here on YouTube make it look so easy - does it take awhile to be able to quickly solve problems like this, or am I just slow lol?",2021-12-01T14:34:25Z
UgxJvgJKi4jclBaq4TB4AaABAg.9VNlvf8Ig659VRAlHsQsWe,@stratascratch,UgxJvgJKi4jclBaq4TB4AaABAg,2,QenwDm5oWdU,0,1,2021-12-02T02:24:38Z,@@laurak5196 It takes a while. I wrote in SQL everyday for several years at my job to a point where I could write SQL better than English ;). The thought process and approach to solving the problem is something that you&#39;ll keep learning though. Problems get harder as you get more experience but you&#39;ll get the syntax down after a while. Good luck!,2021-12-02T02:24:38Z
UgxJvgJKi4jclBaq4TB4AaABAg.9VNlvf8Ig659VRBTXjh5YY,@laurak5196,UgxJvgJKi4jclBaq4TB4AaABAg,2,QenwDm5oWdU,0,0,2021-12-02T02:30:49Z,"@@stratascratch thanks so much, this means a lot coming from you, Nate üôèüèª I really loved SQL from the start, and sometimes it does get frustrating but hearing that from you really encourages me to keep pressing forward.   Thanks again for your time and expertise; you‚Äôre helping a lot of people.  üòÄ",2021-12-02T02:30:49Z
Ugx1I4VN0LWSzrwP60F4AaABAg,@azadehmshchn4114,,1,QenwDm5oWdU,0,0,2021-11-22T10:54:15Z,speechless! brilliant!,2021-11-22T10:54:15Z
UgytTrJyjACfHkVZfLh4AaABAg,@aashi9781,,1,QenwDm5oWdU,2,0,2021-11-15T19:08:45Z,"Hey Nate or anyone who has appeared for Amazon DS interviews,¬†I have questions-<br>1) Do you think these questions are the part of initial round in Amazon interview? Do you think that the question may be hard/medium at this first one hour interview or are they more likely to be the part of interview further down the process?<br><br>2)Also I see that in Strata stat we get option of Python/PostgreSQL. Do you think that in the interviews these options are given to the candidate?",2021-11-15T19:08:45Z
UgytTrJyjACfHkVZfLh4AaABAg.9UmC9mWWJAw9UmX69sbaTc,@stratascratch,UgytTrJyjACfHkVZfLh4AaABAg,2,QenwDm5oWdU,0,1,2021-11-15T22:11:46Z,I have never been on an Amazon interview but all those questions on StrataScratch come from real Amazon interviews. Typically you&#39;ll get easier questions in the initial round (medium level) and hard questions come in the last rounds. Hope that helps.,2021-11-15T22:11:46Z
UgytTrJyjACfHkVZfLh4AaABAg.9UmC9mWWJAw9UnALh7jig_,@aashi9781,UgytTrJyjACfHkVZfLh4AaABAg,2,QenwDm5oWdU,0,0,2021-11-16T04:12:09Z,@@stratascratch Thanks for you reply Nate. Gaining a lot from watching your videos.,2021-11-16T04:12:09Z
UgxRWrLNgmGj8iDAU6B4AaABAg,@kritiverma1342,,1,QenwDm5oWdU,0,0,2021-10-29T21:24:21Z,"How would this look like in SQL Server sql query:  <br><br>SELECT CONCAT(DATEPART(yy, created_at), ‚Äò-‚Äò, DATEPART(mm, created_at)) as year_month,  <br>((SUM(value) -  LAG(SUM( value), 1) OVER ( ORDER BY year_month))/ LAG(SUM( value), 1) OVER ( ORDER BY year_month))*100<br>       FROM sf_transactions)<br>GROUP BY year_month<br>ORDER BY year_month",2021-10-29T21:24:21Z
UgxEvrN7RdlPE029H7l4AaABAg,@shalinigarg859,,1,QenwDm5oWdU,1,0,2021-10-29T19:46:52Z,Nate! Your efforts are much appreciated üëç<br>Thank you so much for uploading these helpful videos üî• You are the best teacher in SQL series. Love from India üòä,2021-10-29T19:49:31Z
UgxEvrN7RdlPE029H7l4AaABAg.9U5V0-4IX9i9U5_vrHjKfg,@stratascratch,UgxEvrN7RdlPE029H7l4AaABAg,2,QenwDm5oWdU,0,1,2021-10-29T20:38:35Z,Thank you for watching!,2021-10-29T20:38:35Z
Ugzqxr3s9VZZPO7gHUp4AaABAg,@VinodKumar-nn7go,,1,QenwDm5oWdU,1,0,2021-10-24T04:08:31Z,"Hi Nate, can we not use PARTITION BY inside the window function instead of Group by ? Will that work? I will try and let you know if that works may be I will have to use a CTE",2021-10-24T04:08:31Z
Ugzqxr3s9VZZPO7gHUp4AaABAg.9TrwdwEBxjf9TtqBqt0utM,@stratascratch,Ugzqxr3s9VZZPO7gHUp4AaABAg,2,QenwDm5oWdU,0,0,2021-10-24T21:50:35Z,It could work. There are many ways to solve the problem. I think a PARTITION BY should work and would eliminate the need for a GROUP BY so give it a try on the platform!,2021-10-24T21:50:35Z
UgxS7axlVau9j0aO3yV4AaABAg,@joaopedroreissilva7075,,1,QenwDm5oWdU,2,0,2021-10-23T18:53:48Z,"Hi, Nate!<br><br>So, first of all, as always, impressive! Thank you for sharing.<br><br>But, I tried to solve it by myself before and, I couldn‚Äôt undertand why my code stoped working at this point, as below:<br><br><br>WITH t1 AS (SELECT<br>                    to_date (to_char(created_at :: date ,&#39;YYYY-MM&#39;), &#39;YYYY-MM-01&#39;) AS month_year,<br>                    SUM(value) AS tt_value<br>                FROM sf_transactions<br>                GROUP BY 1<br>                ORDER BY 1)<br><br>SELECT<br>    month_year,<br>    tt_value,<br>    LEAD (tt_value, 1, 0) OVER (ORDER BY month_year ASC) AS val_2<br>FROM t1<br><br><br><br>Could you help me? I reviewed it several times, but I couldn‚Äôt find my mistake.<br><br>Regardless, I understood that, in this case, lag is more apropriate than lead...",2021-10-23T18:53:48Z
UgxS7axlVau9j0aO3yV4AaABAg.9TqxA7TICwY9Tr8GwMmax_,@stratascratch,UgxS7axlVau9j0aO3yV4AaABAg,2,QenwDm5oWdU,0,1,2021-10-23T20:39:35Z,Thanks for watching the video and giving it a try. Can you post your question on the discussion forum of the question? The link to the question is in the description. Someone from my team or myself will be able to get to it within a few days!,2021-10-23T20:39:35Z
UgxS7axlVau9j0aO3yV4AaABAg.9TqxA7TICwY9TsyVZ4IbEW,@joaopedroreissilva7075,UgxS7axlVau9j0aO3yV4AaABAg,2,QenwDm5oWdU,0,0,2021-10-24T13:43:57Z,"@@stratascratch You&#39;re welcome. Of course, thanks for your attention.",2021-10-24T13:43:57Z
Ugwz3DuIoICv5LfuB5B4AaABAg,@zuzuyun4944,,1,QenwDm5oWdU,1,0,2021-10-17T21:23:11Z,"Quick question! Based on the SQL query execution sequence rule, shouldn&#39;&#39;t we use the full query, to_char(created at::date, &#39;YYYY-MM&#39;, instead of year month next to GROUP BY?",2021-10-17T21:23:11Z
Ugwz3DuIoICv5LfuB5B4AaABAg.9TblV-Uvbv99TdmrghDS41,@stratascratch,Ugwz3DuIoICv5LfuB5B4AaABAg,2,QenwDm5oWdU,0,1,2021-10-18T16:13:38Z,I&#39;m not sure what the sequence rule is. Can you share your code that would use a full query? The YYYY-MM and group by merely aggregates all the date to a YYYY-MM format so we can group the data by year-month,2021-10-18T16:13:38Z
UgyH5F1eUs9JyT6U_Gh4AaABAg,@henryyin2731,,1,QenwDm5oWdU,1,0,2021-09-27T05:41:45Z,very well explained... do you think this is a medium level interview question or hard level?,2021-09-27T05:41:45Z
UgyH5F1eUs9JyT6U_Gh4AaABAg.9Sm_rG3zntT9SqFZFUN6LF,@stratascratch,UgyH5F1eUs9JyT6U_Gh4AaABAg,2,QenwDm5oWdU,0,0,2021-09-28T15:52:37Z,I would think this is a hard level question for an interview.,2021-09-28T15:52:37Z
UgxYry3kCAurav71y0J4AaABAg,@prabhaker9031,,1,QenwDm5oWdU,1,0,2021-09-19T06:09:58Z,What a great insight into the solution besides the solution itself.,2021-09-19T06:11:02Z
UgxYry3kCAurav71y0J4AaABAg.9ST1ixWCyrs9SU3XW2OSXL,@stratascratch,UgxYry3kCAurav71y0J4AaABAg,2,QenwDm5oWdU,0,0,2021-09-19T15:44:59Z,"Yes, of course. Try <a href=""http://stratascratch.com/"">stratascratch.com</a> =).",2021-09-19T15:44:59Z
Ugz2Aupz57aV5q5JQFp4AaABAg,@hiovanycubillosgomez5901,,1,QenwDm5oWdU,0,0,2021-09-11T11:01:09Z,"Hey man  that&#39;s great video, beginning with this, can you recommend sites and also tools to practice topics T-SQL, really i appreciate it, thank you so much.",2021-09-11T11:01:09Z
UgwMVOzMW1dQhZVGWtB4AaABAg,@cabaymau5132,,1,QenwDm5oWdU,0,0,2021-09-07T13:49:05Z,"This content is absolutely underrated, why didn&#39;t I find you sooner :&lt; Pls do more topics on SQL",2021-09-07T13:49:05Z
UgwPCDptaesNBggXpMN4AaABAg,@marinamondadorigessinger6390,,1,QenwDm5oWdU,0,0,2021-09-04T21:51:08Z,Wow! you are the best!,2021-09-04T21:51:08Z
UgzDt3sV-ZxDANi6Xlh4AaABAg,@ashutoshsingh5568,,1,QenwDm5oWdU,0,12,2021-08-31T03:59:00Z,What a great and simple explanation it was. You‚Äôre a great teacher. <br><br>I hope you will teach more important topics for SQL.<br><br>I appreciate your hard work. Thanks,2021-08-31T03:59:00Z
Ugwjr3MPp0m28M5JGeJ4AaABAg,@aleixmp95,,1,QenwDm5oWdU,0,0,2021-08-29T12:21:28Z,This is a really good video. Very clear and easy to understand. Thank you !,2021-08-29T12:21:28Z
UgwSpF64TfVCOjGC1QF4AaABAg,@aztxaz,,1,QenwDm5oWdU,0,0,2021-08-23T18:37:45Z,This is an awesome video! I learned something new. Would love to see more videos like this.,2021-08-23T18:38:26Z
UgwZIU6mTSv2dOq9_wF4AaABAg,@shobhamourya8396,,1,QenwDm5oWdU,3,0,2021-06-17T16:11:14Z,"Window function aliases allow the complex calculations to be reused and with CETs programming logic can be modularized for readability and reusability<br><br>with <br>cet1<br>as<br>(<br>-- extract YYYY-MM from created_at    <br>    select id, created_at, <br>      to_char(cast(created_at as date), &#39;YYYY-MM&#39;) as year_month,<br>      value<br>    from sf_transactions<br>),<br>cet2<br>as<br>(<br>-- calculate the monthly revenue<br>   select year_month,<br>      sum(value) monthly_revenue<br>    from cet1<br>    group by year_month<br>    order by year_month<br>),<br>cet3<br>as(<br>-- get the revenue from previous month and calculate the diff wrt current month<br>    select year_month, monthly_revenue, <br>           LAG(monthly_revenue,1) over(order by year_month) prev_revenue,<br>           (monthly_revenue - LAG(monthly_revenue,1) over(order by year_month)) revenue_diff<br>    from cet2<br>)<br>-- calculate percentage change in revenue from previous month<br>-- select * from cet3<br>select year_month, round((revenue_diff*1.00/prev_revenue*1.0)*100,2) percent_change<br>from cet3",2021-06-17T16:12:36Z
UgwZIU6mTSv2dOq9_wF4AaABAg.9Oh3oQBT5Ax9Oh4ijwjtzU,@stratascratch,UgwZIU6mTSv2dOq9_wF4AaABAg,2,QenwDm5oWdU,0,1,2021-06-17T16:19:11Z,Love it. That&#39;s why I like using CTEs!,2021-06-17T16:19:11Z
UgwZIU6mTSv2dOq9_wF4AaABAg.9Oh3oQBT5Ax9Ovqq-vZWoD,@shobhamourya8396,UgwZIU6mTSv2dOq9_wF4AaABAg,2,QenwDm5oWdU,0,0,2021-06-23T09:57:43Z,@@stratascratch I just realised it&#39;s CTE and not CET üòÄ,2021-06-23T09:57:43Z
UgwZIU6mTSv2dOq9_wF4AaABAg.9Oh3oQBT5Ax9OwkepuRu1f,@stratascratch,UgwZIU6mTSv2dOq9_wF4AaABAg,2,QenwDm5oWdU,0,1,2021-06-23T18:23:00Z,@@shobhamourya8396 No worries. I got what you mean. CTE = common table expression,2021-06-23T18:23:00Z
Ugy7wyKaa6VS3n11--Z4AaABAg,@winnumber101,,1,QenwDm5oWdU,1,0,2021-06-08T18:01:05Z,Been using PostgreSQL for a few months after running around in MySQL and oracle‚Ä¶ had nooooo idea that ‚Äú::date‚Äù was all I needed lol<br>The syntax differentiation across platforms‚Ä¶ why lol,2021-06-08T18:01:19Z
Ugy7wyKaa6VS3n11--Z4AaABAg.9OL5E5x_2549OL5sBATXbp,@stratascratch,Ugy7wyKaa6VS3n11--Z4AaABAg,2,QenwDm5oWdU,0,1,2021-06-08T18:06:41Z,haha yea casting to different data types is easy on postgres!,2021-06-08T18:06:41Z
Ugw0uiB4iq7LgHLwXTV4AaABAg,@saikatdasgupta2006,,1,QenwDm5oWdU,1,1,2021-06-02T11:16:01Z,"Here is one more approach<br><br>with cte as(select to_date(concat(date_part(&#39;year&#39;,created_at),&#39;-&#39;,date_part(&#39;month&#39;,created_at)),&#39;yyyy-mm&#39;) dt, sum(value) sum1 from sf_transactions group by dt order by dt)<br><br>select ct2.dt,<br>round(((ct2.sum1 - ct1.sum1)/ct1.sum1)*100,2)<br>from cte ct1 , cte ct2 where<br>date_part(&#39;month&#39;,ct2.dt)=date_part(&#39;month&#39;,ct1.dt)+1<br>order by dt",2021-06-02T11:16:01Z
Ugw0uiB4iq7LgHLwXTV4AaABAg.9O4v68t5LLk9_fpHUHmdI1,@anggipermanaharianja6122,Ugw0uiB4iq7LgHLwXTV4AaABAg,2,QenwDm5oWdU,0,0,2022-04-11T10:53:01Z,"yours is basically true, for readibility you can re-write even easier to read like this:<br><br>with cte as (<br>    select to_char(cast(created_at as date), &#39;YYYY-MM&#39;) as year_month,<br>       sum(value) as curr_revenue,<br>       lag(sum(value), 1) over w  as prev_revenue<br>    from sf_transactions<br>    group by 1<br>    window w as (order by to_char(cast(created_at as date), &#39;YYYY-MM&#39;))<br>)<br><br>select year_month,<br>    round(100 * (curr_revenue - prev_revenue) / prev_revenue, 2) as pct_change<br>from cte<br>order by 1 asc;",2022-04-11T10:53:01Z
UgwnGjCoDxF1VmmpsVp4AaABAg,@adityabhardwaj8007,,1,QenwDm5oWdU,2,0,2021-05-23T06:37:17Z,"In SQL Server , to_Char is not working, what we need to use?",2021-05-23T06:37:17Z
UgwnGjCoDxF1VmmpsVp4AaABAg.9NffFsC0GmS9Nh8iFG1uBk,@stratascratch,UgwnGjCoDxF1VmmpsVp4AaABAg,2,QenwDm5oWdU,0,1,2021-05-23T20:22:41Z,"I think format() should work. <a href=""https://stackoverflow.com/questions/47042286/ms-sql-server-equivalent-of-to-char/47042371"">https://stackoverflow.com/questions/47042286/ms-sql-server-equivalent-of-to-char/47042371</a>",2021-05-23T20:22:41Z
UgwnGjCoDxF1VmmpsVp4AaABAg.9NffFsC0GmS9NiGMl3prYO,@adityabhardwaj8007,UgwnGjCoDxF1VmmpsVp4AaABAg,2,QenwDm5oWdU,0,0,2021-05-24T06:48:45Z,"@@stratascratch working, Thank you!!",2021-05-24T06:48:45Z
UgzhwWZnWgm1xtkjRu94AaABAg,@flipcase,,1,QenwDm5oWdU,5,1,2021-05-06T21:18:26Z,This channel is amazing! Very well broken downüôèüôè thank you<br><br>My feedback is please keep making such amazing videos,2021-05-06T21:29:01Z
UgzhwWZnWgm1xtkjRu94AaABAg.9N0T_fR620O9N0gIAQYhoJ,@stratascratch,UgzhwWZnWgm1xtkjRu94AaABAg,2,QenwDm5oWdU,0,0,2021-05-06T23:18:14Z,Thank you so much! Will keep making these videos!,2021-05-06T23:18:14Z
UgzhwWZnWgm1xtkjRu94AaABAg.9N0T_fR620O9N5ksfZ_bET,@flipcase,UgzhwWZnWgm1xtkjRu94AaABAg,2,QenwDm5oWdU,0,0,2021-05-08T22:34:31Z,@@stratascratch I cracked the technical round of one of these organisations thanks to you. Will keep you posted! I am endorsing you to all my friends. Keep it going Nate!,2021-05-08T22:34:31Z
UgzhwWZnWgm1xtkjRu94AaABAg.9N0T_fR620O9N7ur2CnwVA,@stratascratch,UgzhwWZnWgm1xtkjRu94AaABAg,2,QenwDm5oWdU,0,1,2021-05-09T18:40:09Z,@@flipcase Congrats Sachin! Good luck to you on the next rounds! Keep prepping and you&#39;ll do well. Best of luck!,2021-05-09T18:40:09Z
UgzhwWZnWgm1xtkjRu94AaABAg.9N0T_fR620O9NXWWw0NB5Q,@flipcase,UgzhwWZnWgm1xtkjRu94AaABAg,2,QenwDm5oWdU,0,0,2021-05-19T17:19:03Z,@@stratascratch Hey Nate I made it! Thank you so much for your videos. How can I email you?,2021-05-19T17:19:03Z
UgzhwWZnWgm1xtkjRu94AaABAg.9N0T_fR620O9NXXXrjyDOs,@stratascratch,UgzhwWZnWgm1xtkjRu94AaABAg,2,QenwDm5oWdU,0,1,2021-05-19T17:27:56Z,"@@flipcase Very cool and congrats! I&#39;d love to get a testimonial to add to the site! Email me at nate@<a href=""http://stratascratch.com/"">stratascratch.com</a>",2021-05-19T17:27:56Z
UgyaXJR1oxWkh-M6ZSx4AaABAg,@kylehuang7926,,1,QenwDm5oWdU,1,0,2021-04-20T05:43:41Z,"Great video, an alternative solution is to self join the table but a.month = b.month - 1 (assuming only 2019)",2021-04-20T05:44:17Z
UgyaXJR1oxWkh-M6ZSx4AaABAg.9MLatPlDGO39MMc_PY0Hgu,@stratascratch,UgyaXJR1oxWkh-M6ZSx4AaABAg,2,QenwDm5oWdU,0,0,2021-04-20T15:17:40Z,Yea that would work too! Thanks for watching the videos.,2021-04-20T15:17:40Z
UgwjuZ9_pHJPpfiotKJ4AaABAg,@mohars,,1,QenwDm5oWdU,0,0,2021-04-09T18:57:30Z,"MySQL Query:<br><br>select date_format(created_at,&#39;%Y-%m&#39;)<br>,ROUND((SUM(value) - lag(SUM(value),1) over(w))/lag(SUM(value),1) over(w)* 100.00,2)  as revenue_diff<br>FROM sql_practice.sf_transactions group by 1 <br>WINDOW w as (order by date_format(created_at,&#39;%Y-%m&#39;))<br>order by 1;",2021-04-09T18:57:30Z
UgwBKjkQjTfzyH_cmQN4AaABAg,@priyankalad7789,,1,QenwDm5oWdU,2,0,2021-03-30T23:26:30Z,Crisp n clear explanation. Is it always after group by we have to specify Window alias ??,2021-03-30T23:26:30Z
UgwBKjkQjTfzyH_cmQN4AaABAg.9LXQpJIEtrI9LXl1UhTZLi,@stratascratch,UgwBKjkQjTfzyH_cmQN4AaABAg,2,QenwDm5oWdU,0,0,2021-03-31T02:31:48Z,No I don&#39;t think it needs to be but I&quot;m also not sure. I always put it at the bottom so it&#39;s away from the main part of the code.,2021-03-31T02:31:48Z
UgwBKjkQjTfzyH_cmQN4AaABAg.9LXQpJIEtrI9LYFsf6K3lI,@priyankalad7789,UgwBKjkQjTfzyH_cmQN4AaABAg,2,QenwDm5oWdU,0,0,2021-03-31T07:10:05Z,@@stratascratch Thanks. that helps,2021-03-31T07:10:05Z
UgxHVEIBdADFGpThrN54AaABAg,@mauriciovarela2337,,1,QenwDm5oWdU,1,0,2021-03-30T13:37:06Z,"Thank you Nathan, well explained and easy to follow. cheers",2021-03-30T13:37:06Z
UgxHVEIBdADFGpThrN54AaABAg.9LWNNNmGbD_9LWefxRF3In,@stratascratch,UgxHVEIBdADFGpThrN54AaABAg,2,QenwDm5oWdU,0,0,2021-03-30T16:17:03Z,Thanks for watching and more to come!,2021-03-30T16:17:03Z
UgwT8MQQSIooNxCpczh4AaABAg,@prathameshmahankal4180,,1,QenwDm5oWdU,1,0,2021-03-29T05:34:34Z,Such an elegant solution!,2021-03-29T05:34:34Z
UgwT8MQQSIooNxCpczh4AaABAg.9LSwMMzpysE9LU3RkjENmf,@stratascratch,UgwT8MQQSIooNxCpczh4AaABAg,2,QenwDm5oWdU,0,0,2021-03-29T16:04:27Z,Thanks for watching!,2021-03-29T16:04:27Z
UgxuOZ91UBPHUeImlz54AaABAg,@nargisparvin4267,,1,QenwDm5oWdU,1,0,2021-02-23T21:37:31Z,Excellent Video,2021-02-23T21:37:31Z
UgxuOZ91UBPHUeImlz54AaABAg.9K76XUylwFU9K798vy_S29,@stratascratch,UgxuOZ91UBPHUeImlz54AaABAg,2,QenwDm5oWdU,0,0,2021-02-23T22:00:23Z,Thanks so much for watching! Let me know if you have any requests.,2021-02-23T22:00:23Z
Ugx9sS8f3r-5t54lG-J4AaABAg,@priyankasarkar6600,,1,QenwDm5oWdU,1,2,2021-02-23T04:18:14Z,Morning Bliss Tutorial! &lt;3,2021-02-23T04:18:14Z
Ugx9sS8f3r-5t54lG-J4AaABAg.9K5FaWR3vte9K5HNANFX7I,@stratascratch,Ugx9sS8f3r-5t54lG-J4AaABAg,2,QenwDm5oWdU,0,1,2021-02-23T04:33:45Z,Thank you for watching!,2021-02-23T04:33:45Z
UgxqWETPxcZ_pfu0InR4AaABAg,@mohars,,1,QenwDm5oWdU,1,0,2021-02-22T04:01:02Z,"MySQL Syntax:<br><br>select date_format(created_at,&#39;%Y-%m&#39;) as &quot;year_month&quot;,<br>ROUND(((sum(value)-lag(sum(value),1,0) over(order by date_format(created_at,&#39;%Y-%m&#39;)))/(lag(sum(value),1,0) over(order by date_format(created_at,&#39;%Y-%m&#39;)))*100.00),2)<br>from sf_transactions<br>group by date_format(created_at,&#39;%Y-%m&#39;)<br>order by date_format(created_at,&#39;%Y-%m&#39;);",2021-02-22T04:01:02Z
UgxqWETPxcZ_pfu0InR4AaABAg.9K2dpgUryn39K2j4hsVu9v,@stratascratch,UgxqWETPxcZ_pfu0InR4AaABAg,2,QenwDm5oWdU,0,0,2021-02-22T04:46:55Z,Nice. That would also work in postgres right?,2021-02-22T04:46:55Z
UgwZnIeYczF8Rq-nPMF4AaABAg,@immanuelsuleiman7550,,1,QenwDm5oWdU,1,0,2021-02-22T00:13:47Z,Very awesome<br>Thank you,2021-02-22T00:13:47Z
Ugw_O8CrXznZQD5e_lx4AaABAg,@deepaksaldanha9103,,1,VYeevsVj4fU,0,0,2023-08-12T14:24:27Z,"the last scenario says multiple products on multiple days, shouldn&#39;t 46 not be in included in the marketing campaign as well ?",2023-08-12T14:24:27Z
UgwBM6c-m_6CzRG3Gix4AaABAg,@VloggingMemories,,1,VYeevsVj4fU,0,0,2023-02-22T05:14:23Z,"My Solution:<br>with first_user_purchase as<br>(select * <br>from <br>    (select user_id, <br>created_at, <br>string_agg(distinct product_id::varchar, &#39;;#&#39;) first_product,<br>dense_rank() over (partition by user_id order by created_at::date) rank<br>from marketing_campaign<br>group by 1, 2)x<br>where rank=1)<br>select *<br>from<br>(select t1.user_id,<br>t1.created_at,<br>t1.product_id,<br>t2.first_product<br>from marketing_campaign t1<br>join first_user_purchase t2 on t1.user_id=t2.user_id<br>and t1.created_at&lt;&gt;t2.created_at)x<br>where first_product not like &#39;%&#39;|| product_id ||&#39;%&#39;;",2023-02-22T05:14:23Z
Ugx8Symetngy8TruXxp4AaABAg,@majubodas6206,,1,VYeevsVj4fU,0,0,2023-01-16T00:18:10Z,Thanks a lot Nate!,2023-01-16T00:18:10Z
Ugz9r0X8TXjunpFYoYJ4AaABAg,@vsmanyamvadali7344,,1,VYeevsVj4fU,1,1,2022-10-15T08:59:40Z,excellent way of explanationüëçüëå,2022-10-15T08:59:40Z
Ugz9r0X8TXjunpFYoYJ4AaABAg.9hC7xwWrc-U9hGDEm7A5XM,@stratascratch,Ugz9r0X8TXjunpFYoYJ4AaABAg,2,VYeevsVj4fU,0,0,2022-10-16T23:02:45Z,Thank you!,2022-10-16T23:02:45Z
Ugxb3m_CKUvAawxZegt4AaABAg,@teddy911,,1,VYeevsVj4fU,0,0,2022-10-06T17:12:40Z,"dude your code format is ugly, so hard to read",2022-10-06T17:12:40Z
UgwvK_LFu1RqR49o_rp4AaABAg,@JuanHernandez-xh2js,,1,VYeevsVj4fU,1,1,2022-09-02T18:46:33Z,Very useful.  Thank you!,2022-09-02T18:46:33Z
UgwvK_LFu1RqR49o_rp4AaABAg.9fUSwk5dZVP9fdQuOt4g7I,@stratascratch,UgwvK_LFu1RqR49o_rp4AaABAg,2,VYeevsVj4fU,0,0,2022-09-06T15:41:09Z,You&#39;re welcome.  I am glad it helped you.,2022-09-06T15:41:09Z
UgzFfLhitR-QN8A4d354AaABAg,@heyMrSIDD,,1,VYeevsVj4fU,1,1,2022-08-29T16:11:54Z,this is very helpful thanks,2022-08-29T16:11:54Z
UgzFfLhitR-QN8A4d354AaABAg.9fJt3ZrIg329fMucyAB83P,@stratascratch,UgzFfLhitR-QN8A4d354AaABAg,2,VYeevsVj4fU,0,0,2022-08-30T20:23:19Z,You&#39;re welcome.  Visit our channel for more helpful videos.,2022-08-30T20:23:19Z
Ugxq0zpOlKMBgVOTgsZ4AaABAg,@plekkchand,,1,VYeevsVj4fU,1,0,2022-06-22T22:55:23Z,Hello. Not &quot;hey guys&quot;.,2022-06-22T22:55:23Z
Ugxq0zpOlKMBgVOTgsZ4AaABAg.9caWBIWyYIh9dbNQqn-j2s,@stratascratch,Ugxq0zpOlKMBgVOTgsZ4AaABAg,2,VYeevsVj4fU,0,0,2022-07-18T03:29:30Z,Hello,2022-07-18T03:29:30Z
Ugwj5TL6Fg-WUkE8_VZ4AaABAg,@datalearningsihan,,1,VYeevsVj4fU,0,0,2022-06-04T16:13:51Z,I will always miss out on many products many days but same as the first purchase scenario. No matter how much i revise this same question and answer.,2022-06-04T16:13:51Z
UgyrjIQl_xuCCN2HUlx4AaABAg,@mishtichatterjee24,,1,VYeevsVj4fU,0,1,2022-06-01T05:13:07Z,Thank you so much for this content . Can you start similar series for python interview for data science.,2022-06-01T05:13:07Z
UgxptQsrol8NzoOKOB94AaABAg,@Sarthak631,,1,VYeevsVj4fU,3,0,2022-05-22T00:18:46Z,"Will it work if instead of concatenating the user_id and product_id column, we just take product_id from the subquery and do product_id NOT IN subquery?",2022-05-22T00:18:46Z
UgxptQsrol8NzoOKOB94AaABAg.9bJGHxlFsT59bLZu6a8UmV,@stratascratch,UgxptQsrol8NzoOKOB94AaABAg,2,VYeevsVj4fU,0,1,2022-05-22T21:48:37Z,That sounds like a good approach. You can code it up on the platform and see if it produces the same output.,2022-05-22T21:48:37Z
UgxptQsrol8NzoOKOB94AaABAg.9bJGHxlFsT59bL_2nfQNje,@Sarthak631,UgxptQsrol8NzoOKOB94AaABAg,2,VYeevsVj4fU,0,0,2022-05-22T21:49:56Z,"@@stratascratch Yes, I will try it and share the results with you in the comments.",2022-05-22T21:49:56Z
UgxptQsrol8NzoOKOB94AaABAg.9bJGHxlFsT59bQhlnl9-BY,@Sarthak631,UgxptQsrol8NzoOKOB94AaABAg,2,VYeevsVj4fU,0,0,2022-05-24T21:42:19Z,"@@stratascratch I got your point about why we need to use CONCAT. If we don&#39;t create a new column, we will lose all the required rows as wel because they have the same ID&#39;s as well. Very nice question and an even better explanation.",2022-05-24T21:42:19Z
UgwX4J3qmHdcSZI7PU94AaABAg,@rodrigozuniga9,,1,VYeevsVj4fU,0,2,2022-05-17T00:49:58Z,I find your videos very useful but in this case I think your solution is a little more complicated than needed. You could select the user if and their min purchase date and create a table with just what they bought the first time. You can then join on the min date and eliminate all the records of the first purchase date. You then do the same join but using product id. You are then left with a table that exclude for each user their first date purchase and any latter purchase that includes any of the products purchased the first date. The count of distinct user is gives you the result of how many users were reached by your campaign.,2022-05-17T00:49:58Z
Ugz-oqvj8ErmSSvLQgB4AaABAg,@goingfurther8092,,1,VYeevsVj4fU,0,0,2022-05-15T00:07:42Z,"My answer;<br><br> SELECT<br>COUNT(DISTINCT user_id)<br>FROM(<br>SELECT<br>	 user_id<br>	,created_at<br>	,product_id<br>	,last_order<br>	,DATEDIFF(created_at,last_order) AS date_diff<br>FROM(<br>SELECT<br>	 user_id<br>	,created_at<br>	,product_id<br>	,LAG(created_at) OVER(PARTITION BY user_id ORDER BY created_at) last_order<br>FROM(<br>SELECT <br>	 user_id<br>	,created_at<br>	,product_id<br>	,ROW_NUMBER()OVER(PARTITION BY user_id,product_id ORDER BY created_at) AS row_num<br>FROM marketing_campaign)a<br>WHERE row_num = 1)b)c<br>WHERE date_diff &gt; 0",2022-05-15T00:07:42Z
Ugz0cD122Y1wl5-9Zhd4AaABAg,@prasadmundewadi,,1,VYeevsVj4fU,0,0,2022-05-05T21:09:05Z,"Nice explanation. But while testing it seems that it is missing one edge case. A user bought same product on Day 1 and Day 2 but he bought different products on Day3 and Day4. To cover all edge cases you can use below query:<br>Here is dataset (check user_id 6):<br>User_id	Prod_id	Created_at<br>1	11	1/1/2022<br>1	12	1/2/2022<br>2	13	4/1/2000<br>2	13	2/1/2022<br>3	14	3/1/2022<br>4	15	3/15/2022<br>4	16	3/15/2022<br>5	17	5/15/2022<br>5	18	5/16/2022<br>5	18	5/17/2022<br><br><br>SELECT COUNT (DISTINCT USER_ID)<br>  FROM USER3<br> WHERE USER_ID IN<br>        (  SELECT USER_ID<br>             FROM USER3<br>         GROUP BY USER_ID<br>           HAVING COUNT (DISTINCT CREATED_AT) &gt; 1 AND COUNT (DISTINCT PRODUCT_ID) &gt; 1)<br>     AND<br>        USER_ID  NOT IN <br>        (<br>          SELECT USER_ID<br>            FROM (SELECT PRODUCT_ID,<br>                         USER_ID,<br>                         RANK () OVER (PARTITION BY USER_ID ORDER BY CREATED_AT) RN<br>                    FROM USER3) A<br>           WHERE RN IN (1, 2) --rank 1 and 2 are the 1st and 2nd orders which should be different<br>        GROUP BY USER_ID<br>          HAVING COUNT (DISTINCT PRODUCT_ID) = 1<br>        )",2022-05-05T21:12:24Z
UgxQGWKdMHx7D5aBIo94AaABAg,@himanish2006,,1,VYeevsVj4fU,0,0,2022-04-22T04:17:54Z,After getting a job will stratscratch help me to become expert in database or this is just for interviews ?,2022-04-22T04:17:54Z
UgwxdLNuhrokjr179fB4AaABAg,@rick2591,,1,VYeevsVj4fU,1,1,2022-03-02T00:23:02Z,"what about using CTEs...<br><br>with fst as (<br> select user_id, min(created_at) firstDate <br> from marketing_campaign <br> group by user_id),<br>firstOrders as (<br>select * from marketing_campaign a<br>where exists (select 1 from fst b where b.user_id=a.user_id and b.firstDate=a.created_at)),<br>newOrders as (<br>select * from marketing_campaign a <br>where not exists (select 1 from firstOrders b where b.user_id=a.user_id and <br>(b.created_at=a.created_at or b.product_id=a.product_id)))<br>select count(distinct user_id) from newOrders",2022-03-02T00:23:02Z
UgwxdLNuhrokjr179fB4AaABAg.9Z2hOP9YVmn9Z36b9VCIVm,@stratascratch,UgwxdLNuhrokjr179fB4AaABAg,2,VYeevsVj4fU,0,0,2022-03-02T04:12:06Z,put it in the platform and see if it validates! =),2022-03-02T04:12:06Z
UgyzcCxr3oJPQlZiybx4AaABAg,@xl2274,,1,VYeevsVj4fU,1,0,2022-02-25T17:55:16Z,"Great video! But, I&#39;m wondering whether there is a candidate who can finish this problem within 30 min during a Amazon tech interview.",2022-02-25T17:55:16Z
UgyzcCxr3oJPQlZiybx4AaABAg.9YshpoN6ym89YtCDhKFYC5,@stratascratch,UgyzcCxr3oJPQlZiybx4AaABAg,2,VYeevsVj4fU,0,1,2022-02-25T22:29:33Z,This is a question that is likely for a white boarding session that would take place on an on-site interview. I believe you could finish the problem in 30-min on a white board since it&#39;s more collaborative with the interviewer than coding on a text editor via Zoom. =),2022-02-25T22:29:33Z
UgzQpqbyvRh3PMXGiFR4AaABAg,@caiyu538,,1,VYeevsVj4fU,0,0,2022-02-24T14:28:30Z,"Thank you for explanation,  this question is quite difficult.",2022-02-24T14:28:30Z
UgzOxzmkNKZ6DYZGyrx4AaABAg,@yashsinha16,,1,VYeevsVj4fU,0,0,2022-01-28T19:14:14Z,select count(distinct mc1.user_id)<br>from marketing_campaign mc1<br>join marketing_campaign mc2 on<br>mc1.user_id=mc2.user_id and mc2.created_at&gt;=mc1.created_at+interval &#39;1 day&#39;<br>and mc2.product_id!=mc1.product_id<br><br>i am getting 24 and not 23? can anyone tell,2022-01-28T19:14:14Z
UgxAWys-Fxj_pC0zpQN4AaABAg,@shekharbagwe,,1,VYeevsVj4fU,2,0,2022-01-25T02:09:08Z,"How about this?<br><br>select count(distinct user_id) from marketing_campaign where concat(user_id,&#39;_&#39;,product_id) not in(<br>    select usr_prd from (select concat(user_id,&#39;_&#39;,product_id) usr_prd, created_at, rank() over(partition by user_id order by created_at asc) rnk from marketing_campaign ) up where rnk = 1<br>);<br><br>subquery creates a list of user_id+product_id for all first transactions. I am using rank window function to identify all first transactions. After that using &quot;not in&quot; clause to select all other combinations.",2022-01-25T02:09:08Z
UgxAWys-Fxj_pC0zpQN4AaABAg.9XbBuwBK-iN9XbOJ9HVwgW,@stratascratch,UgxAWys-Fxj_pC0zpQN4AaABAg,2,VYeevsVj4fU,0,0,2022-01-25T03:57:26Z,Looks like it would work! Test it out on the platform!,2022-01-25T03:57:26Z
UgxAWys-Fxj_pC0zpQN4AaABAg.9XbBuwBK-iN9XbPFZc8Ehy,@shekharbagwe,UgxAWys-Fxj_pC0zpQN4AaABAg,2,VYeevsVj4fU,0,0,2022-01-25T04:05:41Z,"@@stratascratch yeah i tested this, works fine :)",2022-01-25T04:05:41Z
UgyOuIUIt3PSlN12f4N4AaABAg,@BummerSlug,,1,VYeevsVj4fU,0,0,2022-01-23T14:53:28Z,User 50 bought <br>product 118 x4 at a price of 35<br>Then next day<br>product 118 x4 at a price of 29<br>Then three weeks later<br>product 118 x5 at a price of 299<br>Then next day<br>product 118 x2 at a price of 199<br>Must have been GameStop stocks.,2022-01-23T14:53:28Z
Ugw3O1VOiMpzc0u-eo14AaABAg,@muzafferckay2609,,1,VYeevsVj4fU,1,0,2022-01-12T12:44:17Z,Since you can see multiple solutions for the same problem this is one of my favorite platforms to practice SQL skills.,2022-01-12T12:45:04Z
Ugw3O1VOiMpzc0u-eo14AaABAg.9X5rGawL-oY9X6zbgQSMQg,@stratascratch,Ugw3O1VOiMpzc0u-eo14AaABAg,2,VYeevsVj4fU,0,0,2022-01-12T23:16:27Z,Thanks for the kind words! It&#39;s one of the features everyone loves.,2022-01-12T23:16:27Z
UgwfTgAgBB8qligUY-14AaABAg,@amadysvlog3449,,1,VYeevsVj4fU,1,0,2022-01-06T20:35:44Z,I love how you break the question down and explained it . Thank so for this video and for sharing the link to that question.,2022-01-06T20:35:44Z
UgwfTgAgBB8qligUY-14AaABAg.9WsFSHyCEy-9kV-PrZMbA6,@stratascratch,UgwfTgAgBB8qligUY-14AaABAg,2,VYeevsVj4fU,0,0,2023-01-05T06:24:43Z,Glad it was helpful!,2023-01-05T06:24:43Z
UgzNvLHy3mCpN5rgSUp4AaABAg,@yujiefu2251,,1,VYeevsVj4fU,1,0,2021-12-08T19:44:31Z,I have learned a lot from you. I am trying to use your framework to in my following interviews.,2021-12-08T19:44:31Z
UgzNvLHy3mCpN5rgSUp4AaABAg.9VhUXrAIgQh9VhlB0uzexQ,@stratascratch,UgzNvLHy3mCpN5rgSUp4AaABAg,2,VYeevsVj4fU,0,0,2021-12-08T22:18:41Z,Thanks great. Glad you watched the videos!,2021-12-08T22:18:41Z
UgyRHYPHRxH0BmfN2AN4AaABAg,@laurak5196,,1,VYeevsVj4fU,2,0,2021-11-30T17:16:00Z,"Nate this is a really helpful video, as are all of these where you work through a real problem.   As someone wanting to become a data analyst, I‚Äôm wondering how in-depth my SQL interview questions are likely to be, vs those for a data science position.   Any idea?",2021-11-30T17:16:00Z
UgyRHYPHRxH0BmfN2AN4AaABAg.9VNcB5Y2oni9VOsdZ9UYkO,@stratascratch,UgyRHYPHRxH0BmfN2AN4AaABAg,2,VYeevsVj4fU,0,2,2021-12-01T04:59:05Z,"For SQL, there is no difference between a DA and DS. The DS position differs because you need to know much more modeling and statistics.",2021-12-01T04:59:05Z
UgyRHYPHRxH0BmfN2AN4AaABAg.9VNcB5Y2oni9VPq_f7bm6e,@laurak5196,UgyRHYPHRxH0BmfN2AN4AaABAg,2,VYeevsVj4fU,0,0,2021-12-01T14:00:19Z,@@stratascratch thanks Nate!,2021-12-01T14:00:19Z
UgwUICnI8_RaOWrTtCt4AaABAg,@debjyotiroy842,,1,VYeevsVj4fU,1,0,2021-11-30T08:15:06Z,Man this channel should be given a Nobel prize,2021-11-30T08:15:06Z
UgwUICnI8_RaOWrTtCt4AaABAg.9VMeHO9qNFr9VOsZBDYHY6,@stratascratch,UgwUICnI8_RaOWrTtCt4AaABAg,2,VYeevsVj4fU,0,0,2021-12-01T04:58:21Z,=),2021-12-01T04:58:21Z
UgwtbmCEyytI3miMDql4AaABAg,@vibhutikathuria7857,,1,VYeevsVj4fU,1,11,2021-11-11T22:40:43Z,"Hi Nate, Don&#39;t know if you&#39;re gonna see this. But I religiously followed your youtube channel and even took a subscription at StrataScratch for an amazon interview for the role of data engineer. I can&#39;t believe that I cracked it, just got the mail that I got selected!!! Thank you so much. Everything you taught was put to right use and came in handy during my interviews! THANK YOU SO MUCH OMG ILY",2021-11-11T22:40:43Z
UgwtbmCEyytI3miMDql4AaABAg.9UcHEkU0w9s9UcIiJjce5T,@stratascratch,UgwtbmCEyytI3miMDql4AaABAg,2,VYeevsVj4fU,0,2,2021-11-11T22:53:38Z,Congrats on your new role! Very happy for you and I&#39;m glad you found the videos useful. Thank you so much for the kind words. Good luck on the start of your new job!,2021-11-11T22:53:38Z
UgyPR_wzxT7-V4pSAPR4AaABAg,@vijaycvcU,,1,VYeevsVj4fU,0,0,2021-09-03T19:38:07Z,Exists - ensures that the current row is not the first transaction for the user<br>Not exists - ensure that the current product in the row is bought previously <br><br>Select count(distinct user_id) from Market_Campaign MC<br>Where exists (select 1 from Market_Campaign MCI where MCI.user_id = MC.user_id and MC.created_at &gt; MCI.created_at)<br>and not exists (select 1 from Market_Campaign MCI where MC.user_id = MC.user_id and MC.created_at &gt; MCI.created_at and MC.product_id = MCI.product_id),2021-09-03T19:38:07Z
Ugyi4DihpqJ8LbE_sCh4AaABAg,@jameshizon4861,,1,VYeevsVj4fU,0,2,2021-08-25T16:51:46Z,This one is pretty good. I needed to improve on understanding the logic of solving difficult SQL questions. Will have to practice more.<br><br>Recap:<br>1. Identify all scenarios and edge cases.<br>2. Individually map out solution for each scenario.<br>3. Keep the logic separate from each other.,2021-08-25T16:51:46Z
UgxcdcSmrJzMaKA8lm54AaABAg,@manrajgill3458,,1,VYeevsVj4fU,0,0,2021-07-02T06:59:42Z,"With userFirstPurchase(user_id, first_purchase_dt,created_at, product_id)<br>as<br>(select user_id, min(created_at) over (partition by user_id), created_at, product_id<br>from marketing_campaign )<br>select count(distinct user_id)<br>from userFirstPurchase<br>where created_at &gt; first_purchase_dt<br>and concat(user_id,product_id) not in (select concat(user_id,product_id)  from userFirstPurchase where first_purchase_dt=created_at )",2021-07-02T06:59:42Z
UgxR5VE99YZP5Z8vLyB4AaABAg,@shobhamourya8396,,1,VYeevsVj4fU,0,3,2021-06-16T16:49:00Z,"Solution without grouping:<br><br>with <br>cet1<br>as<br>(<br>-- Rank the purchases made by users based on created_at date<br>-- first in-app purchases will be ranked 1, next day purchases will be ranked 2 and so on<br>-- also create the user_product column to uniquely identify products purchased by each user<br><br>select user_id, created_at, product_id, <br>      rank() over(partition by user_id order by created_at) as purchase_rank,<br>      concat(user_id, &#39;-&#39;, product_id) as user_product<br>from marketing_campaign<br>order by user_id<br>),<br>cet2<br>as<br>(<br>-- remove the users who made only first in_app purchases, <br>-- that is keep the those with rank greater than 1<br>select *<br>from cet1<br>where purchase_rank &gt; 1<br>)<br>-- Remove the purchases that have products in first in-app purchases, i.e. with purchase rank 1<br>-- and remaining distinct users will be the marketing campaign target users<br>select count(distinct user_id) <br>from cet2<br>where user_product not in (select user_product <br>                                   from cet1<br>                                   where purchase_rank = 1)",2021-06-16T16:49:29Z
Ugw691DfXlO8fYkWvdp4AaABAg,@winnumber101,,1,VYeevsVj4fU,1,0,2021-06-07T23:44:50Z,The actual process for uncovering this is so elegant though‚Äîthe initial word problem made me discouraged tbh,2021-06-07T23:44:50Z
Ugw691DfXlO8fYkWvdp4AaABAg.9OJ7lwe0QvJ9OJ8LC-9Oeu,@stratascratch,Ugw691DfXlO8fYkWvdp4AaABAg,2,VYeevsVj4fU,0,1,2021-06-07T23:49:47Z,"It&#39;s definitely not great phrasing on the problem. I may have reworded it a bit on the platform but it&#39;s probably still pretty difficult to really understand. This was a last rounder, in-person interview question that&#39;s meant to be comprehensive so I tried my best to word the problem as concisely as possible. Thanks for the feedback and thanks for watching!",2021-06-07T23:49:47Z
UgzOpz-cr4Zq-Ll5gW94AaABAg,@saikatdasgupta2006,,1,VYeevsVj4fU,4,17,2021-06-02T06:56:48Z,"Easier solution<br><br>select count(distinct m.user_id) from marketing_campaign m,<br>(select user_id, min(created_at) from marketing_campaign group by user_id) a where<br>a.user_id = m.user_id and<br>m.product_id not in (<br>  select product_id from marketing_campaign where created_at=a.min and user_id=a.user_id )",2021-06-02T06:56:48Z
UgzOpz-cr4Zq-Ll5gW94AaABAg.9O4SRVgGD3m9O5dk-JqbiW,@stratascratch,UgzOpz-cr4Zq-Ll5gW94AaABAg,2,VYeevsVj4fU,0,3,2021-06-02T18:03:33Z,Much more efficient. Thanks for watching the video.,2021-06-02T18:03:33Z
UgzOpz-cr4Zq-Ll5gW94AaABAg.9O4SRVgGD3m9PTxuUS1w5D,@tarekelias463,UgzOpz-cr4Zq-Ll5gW94AaABAg,2,VYeevsVj4fU,0,1,2021-07-07T01:12:55Z,Still trying to understand it but looks really good,2021-07-07T01:12:55Z
UgzOpz-cr4Zq-Ll5gW94AaABAg.9O4SRVgGD3m9VNbtqNys2Y,@laurak5196,UgzOpz-cr4Zq-Ll5gW94AaABAg,2,VYeevsVj4fU,0,0,2021-11-30T17:13:31Z,"I think the reason this is confusing for me to read is that there‚Äôs a subquery in the from clause along with a table‚Ä¶..but I‚Äôm glad you shared this.   It‚Äôs always helpful to see the different ways to solve the same problem, keeping in mind efficiency too.   I‚Äôd also like to know how long it took people like you and Nate to be this good at SQL lol.   I‚Äôm still new but sometimes it‚Äôs frustrating when I don‚Äôt learn everything as fast as I‚Äôd like to.",2021-11-30T17:13:54Z
UgzOpz-cr4Zq-Ll5gW94AaABAg.9O4SRVgGD3m9YfBQrtOPXU,@kaushikshankar6051,UgzOpz-cr4Zq-Ll5gW94AaABAg,2,VYeevsVj4fU,0,0,2022-02-20T11:53:14Z,"I have written a similar code but im getting 1 extra user id (24 instead of 23).  Could anyone tell me what is wrong.<br><br>select count(distinct(a.user_id)) from marketing_campaign a<br>inner join (select user_id,created_at,b.product_id from marketing_campaign b group by 1,2,3 )b on a.user_id = b.user_id and a.created_at &lt; b.created_at and a.product_id &lt;&gt; b.product_id<br>;",2022-02-20T11:53:14Z
UgyYed2lND9FCh75J0t4AaABAg,@aakashbansal314,,1,VYeevsVj4fU,2,1,2021-05-25T11:31:19Z,"QQ Nate: If we remove 1st subsql, then also code should work. What is the purpose to select user_id from the subsql? We should just find  those combination of concat(userid,productid) that are bought on 1st day, and just filter out from main table. please suggest.",2021-05-25T11:31:19Z
UgyYed2lND9FCh75J0t4AaABAg.9NlLV5jGJfn9Nlrn7r-vos,@stratascratch,UgyYed2lND9FCh75J0t4AaABAg,2,VYeevsVj4fU,0,0,2021-05-25T16:22:16Z,"I believe you can do it that way. My code isn&#39;t the most efficient. A lot of that has to do with how I would explain the code to an interviewer. It&#39;s sometimes easier to write less efficient code but have the code follow logic step by step, especially if you&#39;re trying to walk someone through your solution. The last part of the interviews are usually talking about how to optimize the code, so that&#39;s where your version would work.",2021-05-25T16:22:16Z
UgyYed2lND9FCh75J0t4AaABAg.9NlLV5jGJfn9NlxRlvEdu7,@aakashbansal314,UgyYed2lND9FCh75J0t4AaABAg,2,VYeevsVj4fU,0,0,2021-05-25T17:11:38Z,@@stratascratch thank you for your advise.,2021-05-25T17:11:38Z
UgygzRGCnit9L1MU1AJ4AaABAg,@yashvirriar8833,,1,VYeevsVj4fU,1,9,2021-05-11T01:22:41Z,"Dude, seriously helpful stuff here to walk through your thought process. Very hard to translate practical experience into words, but it was extremely beneficial into breaking this down into digestible parts!",2021-05-11T01:22:41Z
UgygzRGCnit9L1MU1AJ4AaABAg.9NBCi7tbbxk9NBT_3xX4qT,@stratascratch,UgygzRGCnit9L1MU1AJ4AaABAg,2,VYeevsVj4fU,0,1,2021-05-11T03:49:59Z,Thanks so much for watching and for the kind words! Hope you stick around. Will be creating more and more videos!,2021-05-11T03:49:59Z
UgwWSqPhIDXfJelar6N4AaABAg,@priyankalad7789,,1,VYeevsVj4fU,1,12,2021-04-11T23:50:56Z,"that was a great explanation for a tough problem, Really could not think of all scenarios. I agree there isn&#39;t much good quality content of these series out there. Thanks for doing these",2021-04-11T23:50:56Z
UgwWSqPhIDXfJelar6N4AaABAg.9M0N9lUczci9M0fAcDItlz,@stratascratch,UgwWSqPhIDXfJelar6N4AaABAg,2,VYeevsVj4fU,0,1,2021-04-12T02:37:04Z,Thanks so much Priyanka! I&#39;m glad I can add to the technical content out there on Youtube.,2021-04-12T02:37:04Z
UgwuvISX_NjpJuts_Td4AaABAg,@shipragupta4382,,1,VYeevsVj4fU,1,0,2021-03-09T12:02:33Z,Amazing Nate. Thanks for this,2021-03-09T12:02:33Z
UgwuvISX_NjpJuts_Td4AaABAg.9Kf7rZugnFZ9Kfjg1tQX4J,@stratascratch,UgwuvISX_NjpJuts_Td4AaABAg,2,VYeevsVj4fU,0,0,2021-03-09T17:41:46Z,Thank you for watching the video!,2021-03-09T17:41:46Z
UgxdTdv55Q_FhGasyvt4AaABAg,@sauravkumar9454,,1,VYeevsVj4fU,1,1,2021-02-09T18:54:09Z,These kinda questions will only improve the ability to solve them. Please keep posting advance SQL questions,2021-02-09T18:54:09Z
UgxdTdv55Q_FhGasyvt4AaABAg.9JYlhhNYzP79JZAmUvnZ4E,@stratascratch,UgxdTdv55Q_FhGasyvt4AaABAg,2,VYeevsVj4fU,0,1,2021-02-09T22:42:00Z,Thanks for watching! Will keep doing these videos!,2021-02-09T22:42:00Z
UgzLo06LaTHAsnhJ06d4AaABAg,@AmanSingh-od2ue,,1,VYeevsVj4fU,2,0,2021-02-07T08:06:26Z,"Hey Nate, <br>I don&#39;t understand why do we have to remove all the users with rank 1 in the second part of the query. In that, not all users buy the same product on a different date!",2021-02-07T08:06:26Z
UgzLo06LaTHAsnhJ06d4AaABAg.9JSSzyvmXzG9JWjdF2ed9j,@stratascratch,UgzLo06LaTHAsnhJ06d4AaABAg,2,VYeevsVj4fU,0,1,2021-02-08T23:57:35Z,"I&#39;m actually only removing user-product combo for the user&#39;s 1st purchase because I don&#39;t care about their 1st transaction. If the user purchased the same items on a different date, then those transactions are also removed, which is what we want to do according to the question. What is kept are the products that are different than those the user purchased in a future transactions. This is definitely a difficult question to keep all cases in your head. What I&#39;d do is follow a few of the users that I mentioned in the video. Head over to the platform and for each logic in the query, output the user and products purchased. Then see how specific rules filter out users. Hope that helps!",2021-02-08T23:57:35Z
UgzLo06LaTHAsnhJ06d4AaABAg.9JSSzyvmXzG9JWvN7aD_Tf,@AmanSingh-od2ue,UgzLo06LaTHAsnhJ06d4AaABAg,2,VYeevsVj4fU,0,0,2021-02-09T01:40:06Z,"@@stratascratch Yeah, that helps now. I got confused for a while but I got it now. Thanks!!",2021-02-09T01:40:16Z
UgycLFQgle9zCRvt0dh4AaABAg,@DeepanshuKalra,,1,VYeevsVj4fU,1,1,2021-02-05T11:45:15Z,"Hi Nate, love your channel, and I have binge-watching your videos for the last 3 days.<br>More power to you, for helping all of us.<br><br>I came up with this solution using a self join:<br>select<br>  COUNT(DISTINCT md1.user_id)<br>from<br>  marketing_campaign md1 --day one<br>  JOIN<br>  marketing_campaign md2 --day two<br>  ON md1.user_id=md2.user_id --same user purchasing again<br>  AND md1.created_at::date&lt;md2.created_at::date --product needs to be purchased on a different day<br>  ANd md1.product_id != md2.product_id --product needs to be different<br><br>The problem is this gives me an answer of 24. Am I missing something?",2021-02-05T11:45:15Z
UgycLFQgle9zCRvt0dh4AaABAg.9JNhRt9Zob_9JPXhONygDN,@stratascratch,UgycLFQgle9zCRvt0dh4AaABAg,2,VYeevsVj4fU,0,2,2021-02-06T04:49:52Z,"That&#39;s a nice solution. You&#39;re missing an edge case where a user cannot purchase the same items as the items in their first purchase. You have user 25 in your output but if you look at the raw data, user 25 purchase both item id 114, 115 during their 1st purchase. And then purchased the same items in the future. So 114 and 115 should not count and thus user 25 should not be counted. <br><br>My code:<br>concat((user_id),&#39;_&#39;, (product_id)) not in<br>    (SELECT user_product<br>     FROM<br>       (SELECT *,<br>               rank() over(PARTITION BY user_id<br>                           ORDER BY created_at) AS rn,<br>               concat((user_id),&#39;_&#39;, (product_id)) AS user_product<br>        FROM marketing_campaign<br><br>Handles this exact edge case.",2021-02-06T04:49:52Z
UgwZIqw4g5hCgH8AbiZ4AaABAg,@nargisparvin4267,,1,VYeevsVj4fU,2,0,2021-02-01T20:22:33Z,Thank You !!!,2021-02-01T20:22:33Z
UgwZIqw4g5hCgH8AbiZ4AaABAg.9JEKTAAodNw9JEXGGVvQ7s,@stratascratch,UgwZIqw4g5hCgH8AbiZ4AaABAg,2,VYeevsVj4fU,0,0,2021-02-01T22:14:23Z,Thanks for watching. Please let me know if you have any topics you&#39;d like me to cover!,2021-02-01T22:14:23Z
UgwZIqw4g5hCgH8AbiZ4AaABAg.9JEKTAAodNw9LMgQFI9pZN,@tahiliani22,UgwZIqw4g5hCgH8AbiZ4AaABAg,2,VYeevsVj4fU,0,0,2021-03-26T19:19:50Z,"@@stratascratch  advanced SQL. basically, continue with these videos, please.",2021-03-26T19:19:50Z
UgwAZknRBk4afWJ2RpN4AaABAg,@athraaproductions,,1,VYeevsVj4fU,1,1,2021-02-01T19:44:56Z,Do you need video editor? I&#39;ll do it for you,2021-02-01T19:44:56Z
UgwAZknRBk4afWJ2RpN4AaABAg.9JEG9ctivha9JEXHD_wb0G,@stratascratch,UgwAZknRBk4afWJ2RpN4AaABAg,2,VYeevsVj4fU,0,0,2021-02-01T22:14:31Z,I&#39;m okay for now. Thanks man!,2021-02-01T22:14:31Z
Ugzz497gG1VyXikHTPt4AaABAg,@malikimrankhan9714,,1,VYeevsVj4fU,0,0,2021-02-01T19:26:36Z,Hi Sir<br>I will help you for your channel promotion &amp; 10000 organic subscribers,2021-02-01T19:26:36Z
UgxraiNmExs8rDrX3IR4AaABAg,@usmanyousaf841,,1,VYeevsVj4fU,0,0,2021-02-01T19:25:40Z,Hi Nate!<br>Hope you are doing fine.<br>Please check your email.,2021-02-01T19:25:40Z
UgxZAa4xx6DXUIRwTwx4AaABAg,@Qornv,,1,VYeevsVj4fU,3,3,2021-01-31T00:53:50Z,Very clear and nice explanation. I&#39;m watching these out of pure curiosity to discover new ways I can approach problems. If problems/explanations on the platform are just as real-world oriented it might be worth to sign up üòè,2021-01-31T00:53:50Z
UgxZAa4xx6DXUIRwTwx4AaABAg.9J9evNZ8YlB9J9wNIyOuKm,@stratascratch,UgxZAa4xx6DXUIRwTwx4AaABAg,2,VYeevsVj4fU,0,0,2021-01-31T03:26:20Z,"Thanks for watching! Yes, those questions on the platform are real interview questions from companies. Many of them are straight out of interviews, while others are re-structured but still test for the same concepts. There are a few difficult ones I put on the platform to show you what a few real world questions would look like on the job. Hope they are helpful! There&#39;s 50 free questions on the platform so just do those until you want more=)",2021-01-31T03:26:20Z
UgxZAa4xx6DXUIRwTwx4AaABAg.9J9evNZ8YlB9JCKS9_R-RC,@akashchandra2223,UgxZAa4xx6DXUIRwTwx4AaABAg,2,VYeevsVj4fU,0,0,2021-02-01T01:43:56Z,@@stratascratch are those 50 free ones important too?,2021-02-01T01:43:56Z
UgxZAa4xx6DXUIRwTwx4AaABAg.9J9evNZ8YlB9JCVGALtk-1,@stratascratch,UgxZAa4xx6DXUIRwTwx4AaABAg,2,VYeevsVj4fU,0,0,2021-02-01T03:18:25Z,"@@akashchandra2223 Yes, they are. They are/were interview questions from companies.",2021-02-01T03:18:25Z
Ugw_PQ0IHM6GXVrVQl14AaABAg,@followmycrafts8811,,1,VYeevsVj4fU,1,0,2021-01-31T00:19:08Z,Thanks a lot Sir,2021-01-31T00:19:08Z
Ugw_PQ0IHM6GXVrVQl14AaABAg.9J9ax9ZeMrb9J9wBGVvsJe,@stratascratch,Ugw_PQ0IHM6GXVrVQl14AaABAg,2,VYeevsVj4fU,0,0,2021-01-31T03:24:41Z,Thanks for watching!,2021-01-31T03:24:41Z
UgxsHvVPp_Dbdoz75Ft4AaABAg,@LeviewFPV,,1,VYeevsVj4fU,3,6,2021-01-28T13:47:38Z,"Man, thanks for these videos. I&#39;m in an interview process and always get nervous or mix up in tech interviews. These videos are really helpful to structure the questions and the answers! Already subbed!",2021-01-28T13:47:38Z
UgxsHvVPp_Dbdoz75Ft4AaABAg.9J3K58TDILC9J3gxCDBTcZ,@stratascratch,UgxsHvVPp_Dbdoz75Ft4AaABAg,2,VYeevsVj4fU,0,1,2021-01-28T17:16:07Z,Thanks for watching and I&#39;m glad you found this channel! I&#39;ll keep posting more videos for sure. Let me know if you have any topics you&#39;d like me to cover. I always try to make videos that my audience wants.,2021-01-28T17:16:07Z
UgxsHvVPp_Dbdoz75Ft4AaABAg.9J3K58TDILC9J49zMSQ7oC,@LeviewFPV,UgxsHvVPp_Dbdoz75Ft4AaABAg,2,VYeevsVj4fU,0,1,2021-01-28T21:38:33Z,"@@stratascratch keep going with your current content IMO. As you said in another video, there isn&#39;t much of this on YT! (or good quality at least)",2021-01-28T21:38:33Z
UgxsHvVPp_Dbdoz75Ft4AaABAg.9J3K58TDILC9J4LH1Dd-Cd,@stratascratch,UgxsHvVPp_Dbdoz75Ft4AaABAg,2,VYeevsVj4fU,0,1,2021-01-28T23:17:13Z,@@LeviewFPV Then I&#39;ll keep doing what I do! Thanks for watching and for your input.,2021-01-28T23:17:13Z
UgzPc8KuISOUJeoHGeN4AaABAg,@459B,,1,VYeevsVj4fU,1,5,2021-01-27T10:31:35Z,A difficult but inspiring question,2021-01-27T10:31:35Z
UgzPc8KuISOUJeoHGeN4AaABAg.9J0OrMLSrSn9J17UQAlxtL,@stratascratch,UgzPc8KuISOUJeoHGeN4AaABAg,2,VYeevsVj4fU,0,5,2021-01-27T17:19:00Z,Probably one of the more difficult questions you&#39;d see out there. The last part in handling one of the edge cases was definitely difficult so I hope I explained it right.,2021-01-27T17:19:00Z
UgyPl5N9jbYtCBa32Y94AaABAg,@vastavtailwal2235,,1,lG0PbUq4wkg,1,0,2023-02-22T11:43:31Z,"At <a href=""https://www.youtube.com/watch?v=lG0PbUq4wkg&amp;t=9m20s"">9:20</a><br>can we use<br>&quot; HAVING sum(non_paying) &gt; sum(paying) &quot;<br>after group by clause.<br><br>We can also use pivot table.",2023-02-22T11:51:04Z
UgyPl5N9jbYtCBa32Y94AaABAg.9mRA2nN3-WV9mRayjzIDD2,@stratascratch,UgyPl5N9jbYtCBa32Y94AaABAg,2,lG0PbUq4wkg,0,0,2023-02-22T15:47:30Z,test it out and see if you get the same output. I think you will though.,2023-02-22T15:47:30Z
Ugyfc0sKKFLtl_nB4eN4AaABAg,@noobshady,,1,lG0PbUq4wkg,0,0,2022-04-26T21:06:16Z,"I used where in the outer query and I have the same result<br><br>select <br> * <br>from(<br>select <br>    date,<br>    sum(case when paying_customer = &#39;yes&#39; then downloads end) as paying,<br>    sum(case when paying_customer =&#39;no&#39; then downloads end) as non_paying<br>from<br>    ms_user_dimension a<br>    left join ms_acc_dimension b on a.acc_id=b.acc_id <br>    left join ms_download_facts c on a.user_id=c.user_id <br>group by date<br>order by date asc) as t<br>where non_paying &gt; paying",2022-04-26T21:06:16Z
Ugw7bxO2HmX1QPN1DSt4AaABAg,@austinocampo2410,,1,lG0PbUq4wkg,1,1,2022-03-18T21:33:05Z,Why cant you just do a WHERE clause nonpaying &gt; paying easier,2022-03-18T21:33:05Z
Ugw7bxO2HmX1QPN1DSt4AaABAg.9ZjASXu54lL9Zm-THWM5vz,@stratascratch,Ugw7bxO2HmX1QPN1DSt4AaABAg,2,lG0PbUq4wkg,0,0,2022-03-19T23:54:47Z,Because nonpaying and paying are derived in the SELECT clause and executed at the same time as the WHERE clause so it won&#39;t know what nonpaying and paying are. HAVING is executed later so the nonpaying and paying variables have already been created. That&#39;s the reason why i chose to use it in the HAVING clause.,2022-03-19T23:54:47Z
UgxSWYDMnlr5Lb5xWmJ4AaABAg,@rameespudussery2700,,1,lG0PbUq4wkg,1,0,2022-01-02T09:27:08Z,Hi Nate. Can&#39;t we use where clause instead of group by and having in last step,2022-01-02T09:27:08Z
UgxSWYDMnlr5Lb5xWmJ4AaABAg.9WgkkunJteb9WiGB9jBpCM,@stratascratch,UgxSWYDMnlr5Lb5xWmJ4AaABAg,2,lG0PbUq4wkg,0,1,2022-01-02T23:29:43Z,"Yup, you can! And it&#39;s probably more efficient to do so also. Great catch.",2022-01-02T23:29:43Z
Ugw4q2TwVj7bHt7zQnN4AaABAg,@debjyotiroy842,,1,lG0PbUq4wkg,1,2,2021-11-04T10:11:15Z,You are amazing Nate. Your videos are really insightful. God bless ya :),2021-11-04T10:11:15Z
Ugw4q2TwVj7bHt7zQnN4AaABAg.9UJuuJWZ3At9UKjKHLwjg8,@stratascratch,Ugw4q2TwVj7bHt7zQnN4AaABAg,2,lG0PbUq4wkg,0,0,2021-11-04T17:49:18Z,Thanks for watching the vids! Glad you find it useful.,2021-11-04T17:49:18Z
Ugwawrq41lkDtX-EbKt4AaABAg,@yingqixu7002,,1,lG0PbUq4wkg,3,0,2021-10-12T00:23:08Z,"Thanks for posting these great videos! I have a quick question. Why do we need to add GROUP BY <a href=""http://t.date/"">t.date</a> t.paying and t.nonn_paying in the end? Thanks!",2021-10-12T00:23:08Z
Ugwawrq41lkDtX-EbKt4AaABAg.9TNdJiL15Zc9TNhuPTMK3U,@stratascratch,Ugwawrq41lkDtX-EbKt4AaABAg,2,lG0PbUq4wkg,0,0,2021-10-12T01:03:14Z,"Thanks for watching! To answer your question, it&#39;s because t.paying and t.non_paying are aggregates (sum()) so we need a GROUP BY since date is not an aggregate.",2021-10-12T01:03:14Z
Ugwawrq41lkDtX-EbKt4AaABAg.9TNdJiL15Zc9TNppF-B9Is,@yingqixu7002,Ugwawrq41lkDtX-EbKt4AaABAg,2,lG0PbUq4wkg,0,0,2021-10-12T02:12:26Z,@@stratascratch Sorry I am still confused. We group by date in the subquery t and the three columns in t are all at aggregate level already. So I do not see the reason to do group by in the main query. Please let me know where I am wrong. Thanks!,2021-10-12T02:12:26Z
Ugwawrq41lkDtX-EbKt4AaABAg.9TNdJiL15Zc9TO5NOqxH1_,@stratascratch,Ugwawrq41lkDtX-EbKt4AaABAg,2,lG0PbUq4wkg,0,1,2021-10-12T04:37:02Z,"@@yingqixu7002 Oh sorry I get what you mean. I am de-duplicating the rows with that 2nd group by. In theory, you don&#39;t really need to do that unless you want to get rid of dups.",2021-10-12T04:37:02Z
UgxTWh0OL-NTnRp703p4AaABAg,@joaopedroreissilva7075,,1,lG0PbUq4wkg,0,0,2021-10-06T17:57:00Z,"Amazing!<br>Thank you, Nate!",2021-10-06T17:57:00Z
UgzWz3_qAUFrqKoXNtd4AaABAg,@amberyang802,,1,lG0PbUq4wkg,3,2,2021-07-12T03:39:21Z,Why we need subquery? Can we just use HAVING statement right after the GROUP BY statement? (sort by ascending date is default also.),2021-07-12T03:39:21Z
UgzWz3_qAUFrqKoXNtd4AaABAg.9Pg5dMEAoZa9PhIp7yThvL,@stratascratch,UgzWz3_qAUFrqKoXNtd4AaABAg,2,lG0PbUq4wkg,0,1,2021-07-12T14:53:47Z,"The HAVING is to evaluate non-paying and paying customers which were evaluated using a CASE statement. You&#39;d need to add the CASE statement to the HAVING clause, which I don&#39;t think would work. There&#39;s a link to the question in the description that will bring you to the platform I was using to code. Give it a try and let me know. But I think you&#39;d need to create a subquery or CTE first and then apply the HAVING.",2021-07-12T14:53:47Z
UgzWz3_qAUFrqKoXNtd4AaABAg.9Pg5dMEAoZa9Q_1NfLdNf7,@amberyang802,UgzWz3_qAUFrqKoXNtd4AaABAg,2,lG0PbUq4wkg,0,0,2021-08-03T06:18:50Z,"@@stratascratch <br>i did try and it didn&#39;t work. i am confused why HAVING clause is not working on the aggregated function. <br>for example -<br>select date, sum(non_pay_dl) as non, sum(yes_pay_dl) as yes<br>from (<br>select date ,<br>case when paying_customer=&#39;no&#39; then downloads end  as non_pay_dl,<br>case when paying_customer=&#39;yes&#39; then downloads end  as yes_pay_dl<br>from ms_user_dimension a<br>left join ms_acc_dimension b on a.acc_id=b.acc_id<br>left join ms_download_facts c on a.user_id=c.user_id ) w<br>group by date<br>order by date<br>having sum(non_pay_dl)&gt;5<br>-- having sum(non_pay_dl) &gt;sum(yes_pay_dl)<br>;",2021-08-03T06:18:50Z
UgzWz3_qAUFrqKoXNtd4AaABAg.9Pg5dMEAoZa9Qa0dZnqaTC,@stratascratch,UgzWz3_qAUFrqKoXNtd4AaABAg,2,lG0PbUq4wkg,0,1,2021-08-03T15:31:38Z,@@amberyang802 OK I&#39;d have to take a look closer. Can you ask this question in the forum provided in the question? My team and I would take a look and you&#39;d get a reply back much faster! Thanks!,2021-08-03T15:31:38Z
UgxXaJxUK4chyXPdohJ4AaABAg,@classkori5507,,1,lG0PbUq4wkg,1,0,2021-01-14T19:56:23Z,Excellent,2021-01-14T19:56:23Z
UgxXaJxUK4chyXPdohJ4AaABAg.9IVw9jsXsAY9IWIIOrFAQM,@stratascratch,UgxXaJxUK4chyXPdohJ4AaABAg,2,lG0PbUq4wkg,0,0,2021-01-14T23:18:32Z,Thanks for watching.,2021-01-14T23:18:32Z
UgxQW3MLfuXoM7dAQJN4AaABAg,@chinmaypanchal3654,,1,lG0PbUq4wkg,2,0,2021-01-14T01:47:42Z,Thanks Nate for a good solution overview and just a quick query was ORDER BY Date necessary in the subquery or we can avoid it?,2021-01-14T01:47:42Z
UgxQW3MLfuXoM7dAQJN4AaABAg.9ITz_1dWNOG9IU7Uh7IwEm,@stratascratch,UgxQW3MLfuXoM7dAQJN4AaABAg,2,lG0PbUq4wkg,0,1,2021-01-14T03:05:37Z,You can avoid the ORDER BY in the subquery in the final solution. It was just there to help explain a concept.,2021-01-14T03:05:37Z
UgxQW3MLfuXoM7dAQJN4AaABAg.9ITz_1dWNOG9IUOlmoU3sW,@chinmaypanchal3654,UgxQW3MLfuXoM7dAQJN4AaABAg,2,lG0PbUq4wkg,0,0,2021-01-14T05:36:38Z,@@stratascratch Thanks a lot Nate for your quick reply :),2021-01-14T05:36:38Z
UgyFwDyzGazX6s56Nvt4AaABAg,@mysteriousbd3743,,1,lG0PbUq4wkg,1,1,2021-01-13T19:58:44Z,"Sir, Thank You So Much",2021-01-13T19:58:44Z
UgyFwDyzGazX6s56Nvt4AaABAg.9ITMd1u-1FM9ITtIsHWLVD,@stratascratch,UgyFwDyzGazX6s56Nvt4AaABAg,2,lG0PbUq4wkg,0,0,2021-01-14T00:52:56Z,Thanks for watching. Let me know if you have any topic ideas.,2021-01-14T00:52:56Z
UgwIkMOX-evOqH1jLsF4AaABAg,@ramumandava8813,,1,lG0PbUq4wkg,1,1,2021-01-13T19:11:51Z,Thank you Nate. üôÇ,2021-01-13T19:11:51Z
UgwIkMOX-evOqH1jLsF4AaABAg.9ITHGf0q7Gv9ITtHa-Zhgf,@stratascratch,UgwIkMOX-evOqH1jLsF4AaABAg,2,lG0PbUq4wkg,0,0,2021-01-14T00:52:45Z,Thanks for watching! Let me know if you have any feedback.,2021-01-14T00:52:45Z
UgwwfcKnnS3XpxtCyQF4AaABAg,@user-fn5rx3gp2t,,1,eC7MdwKCCOE,1,0,2023-08-12T14:27:41Z,what If I used in python outer join and fill navalues  with zero)<br>??,2023-08-12T14:27:41Z
UgwwfcKnnS3XpxtCyQF4AaABAg.9tJllnwWIK-9tNEnW4nBJt,@stratascratch,UgwwfcKnnS3XpxtCyQF4AaABAg,2,eC7MdwKCCOE,0,0,2023-08-13T22:47:47Z,give it a try on the platform and see if your solution validates.,2023-08-13T22:47:47Z
UgxVjr6NX313Zd9q_K14AaABAg,@ahmedshehata9522,,1,eC7MdwKCCOE,1,1,2022-09-07T22:04:53Z,"I really appreciate your videos, specially those kind of videos of getting actual production scenarios. Also, I am really happy that I had similar approach of your answer.<br><br> The issue is that I am super un confident person. I was always on top of my class but i always get paranoia since i was student and felt that this issue ruined my career. Do you have any videos of this? i feel like i have to study everything online since i do not have much professional experience from my jobs to be able be confident with interviews with good companies :)",2022-09-07T22:04:53Z
UgxVjr6NX313Zd9q_K14AaABAg.9fggbnUSmYH9fhmhkrMS-C,@stratascratch,UgxVjr6NX313Zd9q_K14AaABAg,2,eC7MdwKCCOE,0,0,2022-09-08T08:17:22Z,"I think you have conquered your first obstacle and that is acknowledging that there is a problem.  From here on now, it will be easier to identify a solution for you.  It is great that you continue to study and take online courses.  I shared about creating projects with real business applications to practice and hone your skills and add to your portfolio.  I also shared in my blogs some tips in an interview.  Hope this helps you too.  Hope through practice you can build up your confidence.  <a href=""https://www.stratascratch.com/blog/what-not-to-do-during-a-technical-interview/"">https://www.stratascratch.com/blog/what-not-to-do-during-a-technical-interview/</a>",2022-09-08T08:17:22Z
UgzMuwfAcoSOsec_4xh4AaABAg,@dwaipayansaha4443,,1,eC7MdwKCCOE,0,0,2022-08-21T14:15:31Z,"My solution:-<br>with t1 as (select date, sum(consumption) max_consumption1 from fb_eu_energy<br>group by date<br>order by max_consumption1 desc),<br>t2 as (select date, sum(consumption) max_consumption2 from fb_asia_energy<br>group by date<br>order by max_consumption2 desc),<br>t3 as (select date, sum(consumption) max_consumption3 from fb_na_energy<br>group by date<br>order by max_consumption3 desc),<br>t4 as (select <a href=""http://t1.date/"">t1.date</a>, t1.max_consumption1,t2.max_consumption2,t3.max_consumption3<br>from t1<br>join t2 on<br>t1.date=<a href=""http://t2.date/"">t2.date</a><br>join t3 on<br>t2.date=t3.date)<br>select date ,(max_consumption1+max_consumption2+max_consumption3) total_energy from t4<br>order by total_energy desc<br>limit 1",2022-08-21T14:15:31Z
UgwjqnqBPclafE9aPud4AaABAg,@annachan6706,,1,eC7MdwKCCOE,0,0,2022-06-08T23:40:02Z,"with all_consumed as (<br>select * from fb_eu_energy<br>UNION <br>SELECT * FROM fb_asia_energy<br>UNION<br>SELECT * FROM fb_na_energy<br>)<br><br>SELECT<br>sum(consumption),<br>date<br>FROM all_consumed<br>group by 2<br>order by sum(consumption) desc<br>limit 1",2022-06-08T23:40:02Z
UgzFr-6rGeE2bks8G2J4AaABAg,@kritiverma1342,,1,eC7MdwKCCOE,2,0,2022-03-23T22:36:35Z,"which one is more optimized if given an option - union all or full join. I am thinking of having all the records using an outer join and then colasce to get the first non-null date, and adding up all the three consumption. Then next approach can be using rank or using a join on max energy. <br><br>select COALSCE(<a href=""http://eu.date/"">eu.date</a>, <a href=""http://asia.date/"">asia.date</a>, na.consumption), <br>    (ISNULL(eu.consumption, 0) + ISNULL(asia.consumption, 0) + ISNULL(na.consumption, 0)) as summ<br>from fb_eu_energy eu<br>    FULL JOIN fb_asia_energy asia <br>ON <a href=""http://eu.date/"">eu.date</a> = <a href=""http://asia.date/"">asia.date</a><br>    FULL JOIN fb_na_energy na <br>ON <a href=""http://eu.date/"">eu.date</a> = <a href=""http://na.date/"">na.date</a> ;",2022-03-23T22:36:35Z
UgzFr-6rGeE2bks8G2J4AaABAg.9Zw9h-tY1FT9Zxy3qytSkM,@stratascratch,UgzFr-6rGeE2bks8G2J4AaABAg,2,eC7MdwKCCOE,0,1,2022-03-24T15:24:44Z,My opinion is that UNION is more optimized because I&#39;ve found that it can handle bigger datasets. Doing a full join will blow up the rows and you might run out of memory. It&#39;s less of a risk (but still a risk) using a UNION,2022-03-24T15:24:44Z
UgzFr-6rGeE2bks8G2J4AaABAg.9Zw9h-tY1FT9ZyAcNSQGM1,@kritiverma1342,UgzFr-6rGeE2bks8G2J4AaABAg,2,eC7MdwKCCOE,0,0,2022-03-24T17:23:10Z,@@stratascratch Thanks for the quick reply.,2022-03-24T17:23:10Z
UgwDRAm2Bj01mExEOYt4AaABAg,@austinocampo2410,,1,eC7MdwKCCOE,1,1,2022-03-17T04:17:37Z,this is a very non efficient way to solve this problem. please fix this video with a better solution,2022-03-17T04:17:37Z
UgwDRAm2Bj01mExEOYt4AaABAg.9Zek9bCAftj9Zeko55ec7L,@stratascratch,UgwDRAm2Bj01mExEOYt4AaABAg,2,eC7MdwKCCOE,0,0,2022-03-17T04:23:16Z,"lol, yea you&#39;re probably right. I have ideas of other ways to solve it. Care to give me your solution?",2022-03-17T04:23:35Z
UgyeaPbV-lky1Gzwz4t4AaABAg,@02fireend45,,1,eC7MdwKCCOE,0,0,2022-01-22T07:48:23Z,gd,2022-01-22T07:48:23Z
UgzZ1UFk5l6VV2Zzy2t4AaABAg,@prash516,,1,eC7MdwKCCOE,0,0,2021-10-31T04:07:04Z,This was brilliant Nate !<br>Thanks :),2021-10-31T04:07:04Z
Ugxt8oOHrhERV16Wv8N4AaABAg,@joaopedroreissilva7075,,1,eC7MdwKCCOE,1,2,2021-10-26T12:09:59Z,"Really great, Nate.<br>I&#39;m not sure if it would be better based on SQL theory, but I&#39;d prefer to use dense_rank instead of join at the end of the code.<br>Because of organization and also if next will be necessary to find for example the 2¬∫ highest consumption or something like this, it&#39;s more practical.",2021-10-26T12:09:59Z
Ugxt8oOHrhERV16Wv8N4AaABAg.9TxxL74Lawb9TyTBL4GyxY,@stratascratch,Ugxt8oOHrhERV16Wv8N4AaABAg,2,eC7MdwKCCOE,0,1,2021-10-26T16:57:00Z,"If you can support your reasoning for using any function, I saw go for it! Not all questions have a black and white solution. But if you can support your approach then your approach is usually correct. Especially if a stakeholder accepts it.",2021-10-26T16:57:00Z
Ugz4Ldv5RGdQxtdFZTp4AaABAg,@shobhamourya8396,,1,eC7MdwKCCOE,1,0,2021-06-13T13:42:22Z,"Without Join:<br>with cet1<br>as<br>(<br>        select *<br>            from fb_eu_energy eu<br>        UNION ALL<br>        select * <br>            from fb_asia_energy asia<br>        UNION ALL<br>        select *<br>            from fb_na_energy na<br>        <br>),<br>cet2<br>as<br>(<br>    select date, sum(consumption) total_energy<br>    from cet1<br>    group by date<br>    order by total_energy desc<br>)<br><br>select date, total_energy <br>from cet2<br>where total_energy = (select max(total_energy) <br>                        from cet2)",2021-06-13T13:42:22Z
Ugz4Ldv5RGdQxtdFZTp4AaABAg.9OXVac9i0zW9OXjW010G7m,@stratascratch,Ugz4Ldv5RGdQxtdFZTp4AaABAg,2,eC7MdwKCCOE,0,1,2021-06-13T15:52:40Z,very cool!,2021-06-13T15:52:40Z
UgzPdfY-V-YpuwkXhWJ4AaABAg,@yashovardhan9841,,1,eC7MdwKCCOE,0,1,2021-05-20T05:07:28Z,"My approach - <br><br>WITH temp<br>     AS (SELECT *<br>         FROM   fb_eu_energy<br>         UNION ALL<br>         SELECT *<br>         FROM   fb_asia_energy<br>         UNION ALL<br>         SELECT *<br>         FROM   fb_na_energy),<br>     temp2<br>     AS (SELECT date,<br>                Sum(consumption) AS total,<br>                Rank()<br>                  OVER (<br>                    ORDER BY Sum(consumption) DESC) AS r<br>         FROM   temp<br>         GROUP  BY 1)<br>SELECT date,<br>       total<br>FROM   temp2<br>WHERE  r = 1",2021-05-20T05:07:28Z
Ugyeitqcq0kuFuIb2nN4AaABAg,@PATRICKCHUAD,,1,eC7MdwKCCOE,2,0,2021-02-14T12:24:36Z,Thanks for Sharing. Now just learn there is a command called CTE. very helpful tips.,2021-02-14T12:24:36Z
Ugyeitqcq0kuFuIb2nN4AaABAg.9Jjx5xsRc_I9JkXfmxo9Tj,@stratascratch,Ugyeitqcq0kuFuIb2nN4AaABAg,2,eC7MdwKCCOE,0,1,2021-02-14T17:52:56Z,"Yea, it&#39;s basically similar to creating a temp table but it can only be used for that query. Sort of like a subquery but at the top of your query.",2021-02-14T17:52:56Z
Ugyeitqcq0kuFuIb2nN4AaABAg.9Jjx5xsRc_I9Jl1dYmutUu,@PATRICKCHUAD,Ugyeitqcq0kuFuIb2nN4AaABAg,2,eC7MdwKCCOE,0,0,2021-02-14T22:32:15Z,@@stratascratch I realized this is similar to a sub query I&#39;m doing in MS access where sometimes it took 4 to 5 times subquery before I got the final result. Same the previous query is used in the next query.,2021-02-14T22:32:54Z
UgxRDYWDUmV1BKMFvAd4AaABAg,@kayodewilliams9118,,1,eC7MdwKCCOE,1,2,2021-02-12T07:10:34Z,Super straight forward.. Thank you. I wouldn&#39;t have thought through using a union.,2021-02-12T07:10:34Z
UgxRDYWDUmV1BKMFvAd4AaABAg.9JeE_AiNfD09Jf99rTNJyS,@stratascratch,UgxRDYWDUmV1BKMFvAd4AaABAg,2,eC7MdwKCCOE,0,0,2021-02-12T15:42:32Z,Thanks for watching! I did say it was a tricky problem =) Thanks for following along.,2021-02-12T15:42:32Z
UgwOU2ANUjKkcXaYC2x4AaABAg,@portiseremacunix,,1,eC7MdwKCCOE,1,0,2021-01-05T21:20:20Z,Happy to learn something new today!,2021-01-05T21:20:20Z
UgwOU2ANUjKkcXaYC2x4AaABAg.9I8ubgMAA7L9I967VzgQkt,@stratascratch,UgwOU2ANUjKkcXaYC2x4AaABAg,2,eC7MdwKCCOE,0,0,2021-01-05T23:09:40Z,Thanks! Glad you learned something new. let me know if you have any feedback or topic ideas.,2021-01-05T23:09:40Z
UgySef-9E9F-moWkTdB4AaABAg,@followmycrafts8811,,1,eC7MdwKCCOE,1,0,2020-12-29T16:59:17Z,Thanks a lot for your Tutorial,2020-12-29T16:59:17Z
UgySef-9E9F-moWkTdB4AaABAg.9HrQAcxB_Br9HrVlutEBR3,@stratascratch,UgySef-9E9F-moWkTdB4AaABAg,2,eC7MdwKCCOE,0,0,2020-12-29T17:48:12Z,Thank you for watching! Feel free to let me know if you have any feedback or ideas for more topics.,2020-12-29T17:48:12Z
Ugy8Tyc3iTBK9Au-S0R4AaABAg,@rakibraihanrimon8784,,1,eC7MdwKCCOE,1,0,2020-12-25T13:10:11Z,Thanks,2020-12-25T13:10:11Z
Ugy8Tyc3iTBK9Au-S0R4AaABAg.9HghmCp5Nvm9Hh7QvkvFBe,@stratascratch,Ugy8Tyc3iTBK9Au-S0R4AaABAg,2,eC7MdwKCCOE,0,0,2020-12-25T17:03:04Z,Glad you liked it. Let me know if you have any requests or feedback for me!,2020-12-25T17:03:04Z
Ugwlre_0rx-INIMlfBZ4AaABAg,@NotFound-iu8wx,,1,eC7MdwKCCOE,5,4,2020-12-25T09:40:05Z,"Hello Nate, can we use rank window function and Union all to get the result?",2020-12-25T09:40:05Z
Ugwlre_0rx-INIMlfBZ4AaABAg.9HgKjLK-ix49Hh7bxePg7D,@stratascratch,Ugwlre_0rx-INIMlfBZ4AaABAg,2,eC7MdwKCCOE,0,2,2020-12-25T17:04:43Z,Yes you can do that. The UNION ALL would basically add up all the energy values by date. The rank window function would be used to find the max value I&#39;m assuming? You can try it out on the platform and see if you get the same result as me. The link to the question is in the description. Let me know if you are able to get the same answer!,2020-12-25T17:04:43Z
Ugwlre_0rx-INIMlfBZ4AaABAg.9HgKjLK-ix49HhGgs1oWcZ,@ds_yat5918,Ugwlre_0rx-INIMlfBZ4AaABAg,2,eC7MdwKCCOE,0,0,2020-12-25T18:24:02Z,@@stratascratch Got the same by using rank window function ! Thanks for such a great video!! Merry Christmas,2020-12-25T18:24:02Z
Ugwlre_0rx-INIMlfBZ4AaABAg.9HgKjLK-ix49HjhRBJm5A_,@stratascratch,Ugwlre_0rx-INIMlfBZ4AaABAg,2,eC7MdwKCCOE,0,0,2020-12-26T17:04:54Z,@@ds_yat5918 Merry Christmas and happy holidays!,2020-12-26T17:04:54Z
Ugwlre_0rx-INIMlfBZ4AaABAg.9HgKjLK-ix49KAM-riapZv,@KamagongTree,Ugwlre_0rx-INIMlfBZ4AaABAg,2,eC7MdwKCCOE,0,1,2021-02-25T03:50:27Z,"If the interviewer is a stickler about performance, they may ask you which would be less expensive. Window functions are usually seen as more efficient than joining more data.",2021-02-25T03:51:37Z
Ugwlre_0rx-INIMlfBZ4AaABAg.9HgKjLK-ix49KAPz2CN9rl,@stratascratch,Ugwlre_0rx-INIMlfBZ4AaABAg,2,eC7MdwKCCOE,0,0,2021-02-25T04:25:10Z,"@@KamagongTree very very true. Especially when working large amounts of data. Joining more data, running subqueries and CTEs, can take up a lot of time and slow things down.",2021-02-25T04:25:10Z
UgwgdwZjEQzHEYuC5PR4AaABAg,@TheShrayansh,,1,eC7MdwKCCOE,1,0,2020-12-22T15:06:12Z,Wow‚ù§Ô∏è,2020-12-22T15:06:12Z
UgwgdwZjEQzHEYuC5PR4AaABAg.9H_BfHXT4hq9H_LK8COy8L,@stratascratch,UgwgdwZjEQzHEYuC5PR4AaABAg,2,eC7MdwKCCOE,0,0,2020-12-22T16:30:33Z,Glad you enjoyed it. Let me know if you have any feedback or requests for other topics.,2020-12-22T16:30:33Z
Ugy48MiDweOI1CKO2R14AaABAg,@bananaboydan3642,,1,XBE09l-UYTE,1,0,2023-06-06T00:47:14Z,Why cant you use distinct for unique salaries in the twitter employees example?,2023-06-06T00:47:14Z
Ugy48MiDweOI1CKO2R14AaABAg.9qambwtDBog9qaqFiNtACA,@stratascratch,Ugy48MiDweOI1CKO2R14AaABAg,2,XBE09l-UYTE,0,0,2023-06-06T01:19:00Z,You lose information if you use a distinct for unique salaries. Give it a try. The link to the question is in the description. You can code and see the output with the free tier.,2023-06-06T01:19:00Z
UgxOrmt-INe331I_tGt4AaABAg,@bananaboydan3642,,1,XBE09l-UYTE,0,0,2023-06-06T00:39:40Z,"Amazing video man, exactly what i was looking for",2023-06-06T00:39:40Z
Ugwb_jPt6sAiuvPhD7x4AaABAg,@VloggingMemories,,1,XBE09l-UYTE,0,0,2023-03-10T12:06:38Z,"My Solution:<br>1. Aggregate Function-<br>select <br>    request_date,<br>    round(abs(distance_to_travel/monetary_cost - avg(distance_to_travel/monetary_cost) over (partition by to_char(request_date, &#39;YYYY-MM&#39;)))::decimal, 2) dpd_year<br>from uber_request_logs;<br>2. Ranking Function-<br>select *<br>from <br>(select department, <br>salary,<br>dense_rank() over (partition by department order by salary desc) as rank<br>from <br>(select distinct department, <br>salary from twitter_employee)x)y<br>where rank&lt;=3<br>order by 1, 2 desc;<br>3. Lead/Lag Function-<br>select <br>    airbnb_year,<br>    curr_host_cnt,<br>    prev_host_cnt,<br>    round((curr_host_cnt-prev_host_cnt)/prev_host_cnt::numeric, 2)*100 growth_rate<br>from<br>(select <br>    to_char(host_since , &#39;YYYY&#39;) airbnb_year,<br>    count(distinct id) curr_host_cnt,<br>    lag(count(distinct id)) over (order by to_char(host_since , &#39;YYYY&#39;)) prev_host_cnt<br>from airbnb_search_details<br>group by 1)t1;",2023-03-10T12:06:38Z
UgyzfrXI9kZeR3VYYZx4AaABAg,@womilojuoluwatobi1015,,1,XBE09l-UYTE,0,1,2023-01-28T03:00:06Z,Your teachings are horrifying for beginners. You are just rushing. We are not seeing how the window function takes efffect on the table ..we all of a sudden just approached at a result set. Chill na,2023-01-28T03:02:20Z
UgylgqJYsTVB7iMEwhR4AaABAg,@diftq8191,,1,XBE09l-UYTE,1,1,2022-10-19T23:54:45Z,Is there an operational definition you could give for when to use each type of window function? For partition by I&#39;ve been going with: &quot;use partition by when you want to group by something but not lose any rows&quot; but that doesn&#39;t help me with most partition by questions. Any help here?,2022-10-19T23:54:45Z
UgylgqJYsTVB7iMEwhR4AaABAg.9hO1_AieE1Y9hQ5k5d9EVX,@stratascratch,UgylgqJYsTVB7iMEwhR4AaABAg,2,XBE09l-UYTE,0,0,2022-10-20T19:09:40Z,"I think of it more like...&quot;what do I need to do in order to answer the question?&quot;. If it means that I need to group by something but not lose any rows, then I&#39;ll use a partition by. I just start with envisioning the output and then I pick appropriate functions and implement the right logic to get me there.",2022-10-20T19:09:40Z
UgzagUExpJimfNnpabN4AaABAg,@davidallantodd7999,,1,XBE09l-UYTE,0,0,2022-10-12T17:14:06Z,"Huge thumbs up for being so crisp and concise, which imho projects a more positive character than content with fluff and frills.",2022-10-12T17:14:06Z
UgwonXyTHjUuLlKdPX94AaABAg,@kshitijdhakal,,1,XBE09l-UYTE,1,0,2022-07-25T17:40:11Z,Love this tutorial. It&#39;s very easy to grasp everything.,2022-07-25T17:40:11Z
UgwonXyTHjUuLlKdPX94AaABAg.9duvLYefSF99dwDzsGDqu0,@stratascratch,UgwonXyTHjUuLlKdPX94AaABAg,2,XBE09l-UYTE,0,1,2022-07-26T05:51:05Z,"Thank you, glad you like it.",2022-07-26T05:51:05Z
Ugyi6HKRPqbRagsDpIl4AaABAg,@emiliogarza6446,,1,XBE09l-UYTE,1,0,2022-07-24T02:39:34Z,"Really useful, thanks a lot",2022-07-24T02:39:34Z
Ugyi6HKRPqbRagsDpIl4AaABAg.9dqjUK9JmwN9dqpQnNUY2Y,@stratascratch,Ugyi6HKRPqbRagsDpIl4AaABAg,2,XBE09l-UYTE,0,1,2022-07-24T03:31:30Z,Glad to hear that!,2022-07-24T03:31:30Z
Ugx_JrnkYmN3BBG7Tcx4AaABAg,@tamago_roe,,1,XBE09l-UYTE,1,0,2022-07-13T04:08:25Z,This is THE ONE video that I review before every single one of my interviews. Thanks for the video!,2022-07-13T04:08:25Z
Ugx_JrnkYmN3BBG7Tcx4AaABAg.9dPZuSdsSCI9dR4tqkTg0b,@stratascratch,Ugx_JrnkYmN3BBG7Tcx4AaABAg,2,XBE09l-UYTE,0,0,2022-07-13T18:15:55Z,Glad it was helpful!,2022-07-13T18:15:55Z
UgziP4wAfSz1WuQHAEl4AaABAg,@kareemmajzoub4426,,1,XBE09l-UYTE,1,0,2022-07-12T19:30:10Z,Why did you use a group by request date in the first problem?,2022-07-12T19:30:10Z
UgziP4wAfSz1WuQHAEl4AaABAg.9dOdan4pM7m9dWabRYyfyb,@stratascratch,UgziP4wAfSz1WuQHAEl4AaABAg,2,XBE09l-UYTE,0,0,2022-07-15T21:37:58Z,"This is a syntax rule. All the columns appearing in the SELECT statement have to appear in the GROUP BY clause. The question requires column request_date to appear in the solution, so it also must be included in the GROUP BY clause.",2022-07-15T21:37:58Z
UgzB8xV_Z_a70FYDAO54AaABAg,@Saboor-Hamedi,,1,XBE09l-UYTE,0,0,2022-07-09T04:10:32Z,"It would be good to write query instead of skipping it, that way we can see the errors and the way you fix it",2022-07-09T04:10:32Z
UgzP1emavEnizWvN3VN4AaABAg,@larryhatcher8927,,1,XBE09l-UYTE,3,0,2022-06-28T07:30:44Z,I&#39;ve been working in Stratascratch for about three month doing hundreds of the problems and I feel like I have a pretty good grasp of coding in MYSQL......even in the windows functions that this video covers.  I&#39;m probably not going to get a job in a FAANG company but my question is:  The coding that I have learned  in Stratascratch.....Is that all I need when I see a job that requires a year of SQL?  Is there more to it than just the coding?,2022-06-28T07:30:44Z
UgzP1emavEnizWvN3VN4AaABAg.9coJ8BtKhUJ9cvNn6DH-hd,@stratascratch,UgzP1emavEnizWvN3VN4AaABAg,2,XBE09l-UYTE,0,0,2022-07-01T01:26:05Z,"For a job that requires a year of SQL, coding using some advance techniques like window functions and rankings is all you probably really need. Having a good grasp of how to breakdown problems and code up solutions is a bonus at that level (which I&#39;m guessing is sort of entry level analytics if it&#39;s requiring a year of experience). Understanding the business context and best practices would probably be taught on the job. Good luck!",2022-07-01T01:26:05Z
UgzP1emavEnizWvN3VN4AaABAg.9coJ8BtKhUJ9cvehPLXyP_,@larryhatcher8927,UgzP1emavEnizWvN3VN4AaABAg,2,XBE09l-UYTE,0,0,2022-07-01T04:02:36Z,@@stratascratch Thanks......one thing that prompted me to ask this question was several weeks ago I was on the linkedin site and just on a whim I took a skills test for MYSQL.  I thought considering how many problems I done on StrataScratch I should do well.  There was a lot of stuff on that test that I had not seen before.  A few things about downloading data and a few other things I had not seen.  These questions didn&#39;t seem to have very much to do with queries and it just crossed my mind.....is there more I should be learning.....Of course you are right...a lot of stuff would be taught on the job,2022-07-01T04:02:36Z
UgzP1emavEnizWvN3VN4AaABAg.9coJ8BtKhUJ9cvew-JouZt,@larryhatcher8927,UgzP1emavEnizWvN3VN4AaABAg,2,XBE09l-UYTE,0,0,2022-07-01T04:04:35Z,"@@stratascratch BTW my biggest bug-a-boo in coding are the REGEXP functions.  The windows functions, once understand, have not been a problem.",2022-07-01T04:04:35Z
UgzJEdCkSrZzYt9PTJl4AaABAg,@VarunSood0109,,1,XBE09l-UYTE,0,0,2022-06-11T05:19:58Z,"I think a more efficient solution to the Unique Salaries question would be as below:<br><br>select distinct x.department, x.salary from<br>(select *, dense_rank() over (partition by department order by salary desc) as salary_rank<br>from twitter_employee) x<br>where x.salary_rank &lt; 4<br>order by department , salary desc;<br><br><br>Let me know if I am correct. Thanks! :)",2022-06-11T05:19:58Z
UgywtNWGCFb-LKtSAX54AaABAg,@VarunSood0109,,1,XBE09l-UYTE,0,0,2022-06-11T03:20:25Z,"Thanks so much for such an informative video! I have a question: Why do we even need to do a Group by here? For me, the solution ended where we selected the date and the abs column from the subquery. Kinda confused here!",2022-06-11T03:20:25Z
UgwtFgr93blleTpoF_d4AaABAg,@evazhong4419,,1,XBE09l-UYTE,1,0,2022-05-24T03:26:32Z,"Hi, I have a question for the first question, You use a avg() window function, I am kinda confused that  can we just use normal avg() function and group by after the from statement in this case",2022-05-24T03:26:32Z
UgwtFgr93blleTpoF_d4AaABAg.9bOkN2Gzbdg9bQYqakqX4-,@stratascratch,UgwtFgr93blleTpoF_d4AaABAg,2,XBE09l-UYTE,0,0,2022-05-24T20:15:36Z,"It&#39;s because I want to find the avg of the month. So I could probably use the avg() function with a group by, however, the a.dist_to_cost column would get in the way. So if you remove that, then you can go with your approach. But if you want all 3 columns, you&#39;ll need to take the average of the month&#39;s partition.",2022-05-24T20:15:36Z
UgyzvjmVgTOPl-qk3K54AaABAg,@swiiph,,1,XBE09l-UYTE,0,0,2022-05-08T22:57:38Z,"heres a solution for the rankings() problem:<br><br>WITH result AS (<br>SELECT<br>    salary,<br>    department,<br>    dense_rank() OVER(partition by department order by salary desc) as rank<br>from twitter_employee<br>group by 1, 2<br>order by 2, 1 desc<br>)<br><br>SElECT department, salary<br>FROM RESULT <br>WHERE RANK &lt;= 3",2022-05-08T22:57:38Z
UgzXrVfFa2vqTJ85CTV4AaABAg,@swiiph,,1,XBE09l-UYTE,0,0,2022-04-26T23:08:01Z,"I think that you can do this in just one subquery without the groupbuys:<br><br>select abs(round(dpd_month.dpd - avg(dpd_month.dpd) over(partition by dpd_month.request_month)::DECIMAL,2)) as difference, <br>dpd_month.request_date <br><br>from (<br>select, *<br>to_char(request_date::date, &#39;YYYY-MM&#39;) as request_month,<br>distance_to_travel/monetary_cost as dpd<br>from uber_request_logs<br>) dpd_month<br><br><br><br>order by dpd_month.request_date asc",2022-04-26T23:08:01Z
Ugzpm_AcCMdoZQtYaex4AaABAg,@callravik,,1,XBE09l-UYTE,0,0,2022-04-16T00:45:47Z,I wish you included sliding window options in these tutorials.,2022-04-16T00:45:47Z
UgwOxyEngC0CObpBVTR4AaABAg,@sabaamanollahi5901,,1,XBE09l-UYTE,0,0,2022-04-08T04:29:22Z,"very useful, thank you",2022-04-08T04:29:22Z
UgyU4jBiLHeIKr_lYEZ4AaABAg,@ravindrakdr,,1,XBE09l-UYTE,0,0,2022-03-28T07:23:36Z,"Hi All,<br><br>Need to get like this in SQL<br><br>a  1<br>a  2<br>a  3<br>a  4<br>a  5<br>.......<br>.......<br>a  900<br>a  1<br>a  2<br>a  3<br><br>Please help me here",2022-03-28T07:23:36Z
UgyGGrMBUehTFJTpLaF4AaABAg,@RaviTheVlogger,,1,XBE09l-UYTE,0,0,2022-03-24T17:41:52Z,Excellent way of explanation. thank you for the video.,2022-03-24T17:41:52Z
Ugz_UQAPHui9fU_XwQp4AaABAg,@granand,,1,XBE09l-UYTE,0,0,2022-03-23T14:11:11Z,Really awesome mate. I subscribed and happy to know if you have a awesome courses you teach which is as unique and productive as these.  Thank you,2022-03-23T14:11:11Z
UgxeOwu1G8g6hYrGIfJ4AaABAg,@alinazem6662,,1,XBE09l-UYTE,0,0,2022-03-03T22:51:00Z,Loving the way you lay out the problem and break it down to bits. That &quot;VISION&quot; is gold. Thank you for every second of your time you spend educating people.,2022-03-03T22:51:00Z
UgzSF3Hes_CMIHgy_gZ4AaABAg,@BellTollsForThee,,1,XBE09l-UYTE,0,0,2022-03-03T20:33:16Z,Great content I love your videos and clarity of speech! <br>Thanks,2022-03-03T20:33:16Z
Ugwr2rb-RQ1xUsCvLo14AaABAg,@swapnilrana3080,,1,XBE09l-UYTE,0,0,2022-02-08T19:31:21Z,"Thanks a lot for creating this awesome channel, its amazingly well organized and makes things easy.",2022-02-08T19:31:21Z
UgzOAgKYybDhQFFodRh4AaABAg,@SandeepKumar-nk2ru,,1,XBE09l-UYTE,0,0,2022-01-04T04:35:23Z,why can&#39;t you use rank instead of ntile in the scenario you explained?,2022-01-04T04:35:23Z
UgxNOMK9JkOeC4zjXIB4AaABAg,@stutidey5902,,1,XBE09l-UYTE,0,0,2021-12-23T03:15:09Z,This video has become my go-to refresher before any interview,2021-12-23T03:15:09Z
Ugx0_aAUUJMvkpepydJ4AaABAg,@eugeniosp3,,1,XBE09l-UYTE,0,0,2021-12-14T22:47:06Z,such an underrated channel.,2021-12-14T22:47:06Z
UgwZTW_S42ziWm_5ELV4AaABAg,@warnercooler4488,,1,XBE09l-UYTE,0,0,2021-12-11T19:16:30Z,"Awesome content, well presented. Thank you so much!",2021-12-11T19:16:30Z
UgxxBO5yP_-PmXJUPc14AaABAg,@user-vw9dn2rz9s,,1,XBE09l-UYTE,1,0,2021-12-06T06:20:36Z,Thank you so much for this!,2021-12-06T06:20:36Z
UgxxBO5yP_-PmXJUPc14AaABAg.9Vatx1Lf_f79VcvNXYeNwJ,@stratascratch,UgxxBO5yP_-PmXJUPc14AaABAg,2,XBE09l-UYTE,0,0,2021-12-07T01:11:34Z,"Thanks, Jermaine! Appreciate it",2021-12-07T01:11:34Z
UgyJ8Vs_3fRtLhtgDyF4AaABAg,@iamyuvi009,,1,XBE09l-UYTE,1,0,2021-11-25T04:32:56Z,"Hey Nate, thank you for the video.. do you see anything wrong with this code?<br><br>select department, salary from<br>(select department, salary, dense_rank() over(partition by department order by sal desc) drnk from twitter_employee<br>group by 1,2<br>order by 1) a<br>where drnk&lt;=3;",2021-11-25T04:32:56Z
UgyJ8Vs_3fRtLhtgDyF4AaABAg.9V9NssRaf6D9VAmQwrblG3,@stratascratch,UgyJ8Vs_3fRtLhtgDyF4AaABAg,2,XBE09l-UYTE,0,0,2021-11-25T17:35:25Z,I don&#39;t see anything wrong with it but give it a try on the platform to see if you see any syntax errors!,2021-11-25T17:35:25Z
Ugzqmpn_MQHJj0PS6Qh4AaABAg,@CeruleanAnthracite,,1,XBE09l-UYTE,0,0,2021-11-10T00:33:32Z,"Hi, I&#39;m confused about why you had to group by all columns in the aggregate functions section for the final query? once you got the table after doing the over(partition by...) bit, why can&#39;t we just calculate the difference b/w the columns directly? (i&#39;m referring to the very first question you discussed in the video)",2021-11-10T00:40:35Z
Ugy76SqZD9iwWRzbQ4V4AaABAg,@kritiverma1342,,1,XBE09l-UYTE,1,0,2021-10-30T02:12:10Z,"Why do we need to group by in the very first example? I would appreciate it if you can point out a mistake in the below query<br><br><br>select a.request_month_yr, ROUND(ABS((a.dist_cost  - AVG(a.dist_cost) OVER (partition by a.request_month_yr ))), 2)  AS absolute_diff <br>from (<br>select format (request_date, ‚ÄòYYYY-MM‚Äô) AS request_month_yr, (distance_to_travel/monetary_cost) as dist_cost<br>FROM uber_request_logs) a",2021-10-30T02:12:10Z
Ugy76SqZD9iwWRzbQ4V4AaABAg.9U6B64CrdGu9UAF2QkiyVs,@stratascratch,Ugy76SqZD9iwWRzbQ4V4AaABAg,2,XBE09l-UYTE,0,0,2021-10-31T16:03:35Z,Great try! I would suggest posting this code and asking for help on our forum on the platform. Someone from my team will analyze your code and get back to you about any errors they see. This would be the fastest way to an answer!,2021-10-31T16:03:35Z
UgwoW3RI5yu-5dhFGEB4AaABAg,@joaopedroreissilva7075,,1,XBE09l-UYTE,1,0,2021-10-29T13:28:00Z,"Another really good content, thanks.<br><br>But Nate, on the second question (ranking), this method you used to have the unique values could be done with &#39;DISTINCT&#39; too, right?<br>About the third question(NTILE), instead of NTILE, dense_rank would works by exactly the same logic, right?",2021-10-29T13:28:00Z
UgwoW3RI5yu-5dhFGEB4AaABAg.9U4oe8WQWL49U54xZ7om-j,@stratascratch,UgwoW3RI5yu-5dhFGEB4AaABAg,2,XBE09l-UYTE,0,1,2021-10-29T15:59:12Z,"In theory yes, you could use DISTINCT too for the 2nd question. Give it a try on the platform and see if the output is the same. NTILE and dense_rank aren&#39;t the same. You might be able to use the 2 functions interchangeably due to the dataset and question, but I wouldn&#39;t think of the 2 functions as the same. The use of them solely depends on the dataset and business context.",2021-10-29T15:59:12Z
UgwQfZ21983nJ76luSZ4AaABAg,@shalinigarg859,,1,XBE09l-UYTE,0,0,2021-10-28T17:27:58Z,Thank you so much. Your videos are really informative and helpful. Much appreciated üëç,2021-10-28T17:27:58Z
Ugwe6t0x1H1F8_ps9qN4AaABAg,@shobhamourya8396,,1,XBE09l-UYTE,1,0,2021-10-25T08:45:41Z,"Here&#39;s my solution using CTE:<br><br>with cte1 <br>as<br>(<br>-- calculate the distance per dollar, and extract the yyyy-mm from request date<br>    select  request_date, <br>        avg(distance_to_travel*1.0/monetary_cost*1.0)  over(partition by request_date order by request_date) distance_per_dollar,<br>        to_char(cast(request_date as date), &#39;YYYY-MM&#39;) as request_year_month<br>    from uber_request_logs<br><br>),<br>cte2<br>as(<br>-- use the yyyy-mm to find monthly distance_per_cost<br>select request_date, distance_per_dollar,<br>       avg(distance_per_dollar) over(partition by request_year_month order by request_year_month) as year_month_avg<br>from cte1<br>)<br>-- calculate the difference between monthly and daily distance_per_cost<br>select distinct request_date, round(abs(year_month_avg - distance_per_dollar)::DECIMAL,2)  diff<br>from cte2<br>order by request_date",2021-10-25T08:45:41Z
Ugwe6t0x1H1F8_ps9qN4AaABAg.9Tv09ucPtp09TwFPi9ofBI,@stratascratch,Ugwe6t0x1H1F8_ps9qN4AaABAg,2,XBE09l-UYTE,0,1,2021-10-25T20:18:09Z,Great solution! Thank you for adding it to the comments.,2021-10-25T20:18:09Z
UgxkJxYsxXHvB0ogltV4AaABAg,@yingqixu7002,,1,XBE09l-UYTE,1,0,2021-10-12T01:47:07Z,"Thanks for patiently going through this problem! I have a question regarding the Distance Per Dollar case. If there are multiple rides per day, should we calculate the total distance and total cost first for each day and then generate the distance-per-dollar matrics? Thanks!",2021-10-12T01:47:07Z
UgxkJxYsxXHvB0ogltV4AaABAg.9TNmvn7BDnj9TO5bsATNzw,@stratascratch,UgxkJxYsxXHvB0ogltV4AaABAg,2,XBE09l-UYTE,0,0,2021-10-12T04:39:09Z,Yes of course. You should definitely try the problem on the platform and then post your comments/questions on the discussion forum. Someone from my team help guide you through the problem and answer any specific questions. We try to get to it in a few days. That&#39;ll be much faster than waiting for me!,2021-10-12T04:39:09Z
UgyTdMRByE2rYsXVJnN4AaABAg,@pralineblues,,1,XBE09l-UYTE,0,0,2021-10-11T21:45:52Z,can someone explain why we are interested in (distance per dollar) and (the absolute difference bw that and the average distance per dollar for the month) or how they would be used as metrics,2021-10-11T22:31:17Z
UgwS6ulfggDmqqY6u0x4AaABAg,@nadavnavon2140,,1,XBE09l-UYTE,1,0,2021-10-08T11:14:24Z,"Superb! <br>Quick question regarding the first Uber task, how about dropping the subquery and just use:<br><br>SELECT <br>               request_date, <br>               distance_to_travel / monetary_cost as dist_to _cost<br>               avg( distance_to_travel / monetary_cost) over (partition by extract(month from date_request))  as avg_dist_to_cost <br>FROM <br>               uber_request_logs",2021-10-08T11:15:18Z
UgwS6ulfggDmqqY6u0x4AaABAg.9TEVfI3wOqp9TFPaVHvy4R,@stratascratch,UgwS6ulfggDmqqY6u0x4AaABAg,2,XBE09l-UYTE,0,1,2021-10-08T19:40:34Z,Sounds like it would work in theory and I like the optimization. You can also test your code on the platform (links to the questions are in the description) to see if you get the same output.,2021-10-08T19:40:34Z
UgyXQnsl4AlKmSuzP9t4AaABAg,@ZhensongRen,,1,XBE09l-UYTE,0,0,2021-10-06T14:54:51Z,"--Distance Per Dollar<br><br>SELECT DISTINCT request_mnth,<br>    ROUND(abs(m.dis_to_cost - m.monthly_avg)::DECIMAL, 2) AS difference<br>FROM (SELECT <br>    to_char(request_date, &#39;YYYY-MM&#39;) AS request_mnth,<br>    distance_to_travel/monetary_cost AS dis_to_cost,<br>    AVG(distance_to_travel/monetary_cost) OVER (PARTITION BY to_char(request_date, &#39;YYYY-MM&#39;)) as monthly_avg<br>    FROM uber_request_logs) m<br>    <br>ORDER BY request_mnth",2021-10-06T14:54:51Z
UgxsFp1jPs8DnBad2o14AaABAg,@glin96,,1,XBE09l-UYTE,1,2,2021-10-01T03:41:29Z,I love how you make every question so easy to understand after you break it down! Thank you for sharing your knowledge!,2021-10-01T03:41:29Z
UgxsFp1jPs8DnBad2o14AaABAg.9SwfGuhHf9M9SxndODW9Y1,@stratascratch,UgxsFp1jPs8DnBad2o14AaABAg,2,XBE09l-UYTE,0,0,2021-10-01T14:13:50Z,"Glad you appreciate it! Thanks for watching, Gabby! üòé",2021-10-01T14:13:50Z
Ugw6Sn4WLJAS0Vb4Xfl4AaABAg,@tarekelias463,,1,XBE09l-UYTE,1,0,2021-09-29T02:37:42Z,"In the subquery you‚Äôve used to remove duplicates, couldn‚Äôt a select distinct do the job?",2021-09-29T02:37:42Z
Ugw6Sn4WLJAS0Vb4Xfl4AaABAg.9SrPNuGaNOC9St_0uWMNSz,@stratascratch,Ugw6Sn4WLJAS0Vb4Xfl4AaABAg,2,XBE09l-UYTE,0,0,2021-09-29T22:49:09Z,"But you have a count function as the 2nd column, so you have to add a group by in the subquery. It does remove the duplicates but the main purpose is to allow the count function to operate. You&#39;ll get an error if you used a distinct instead of a group by. Hope that helps!",2021-09-29T22:49:09Z
UgxJt0szEpWzFaLBOll4AaABAg,@arpan8076,,1,XBE09l-UYTE,0,0,2021-09-14T14:36:51Z,"Hello<br>I work in a supermarket and I normally do some query in SQL Server as my basic day to day work<br>I want to upgrade my selft and learn more into this and get a high payable job<br>From where can I learn medium and advance SQL and What PL language will I need to know to be able to get a job in DA,DS  ?? Can i get some useful links to study and get certified ..<br><br>Thank you",2021-09-14T14:36:51Z
Ugxawr00mAUI5bvWoHh4AaABAg,@MaxKashirin,,1,XBE09l-UYTE,0,0,2021-09-13T20:26:19Z,"Hi Nate, I have a question.<br>Why in the first task you count avg(dist_to_cost) partition by month, not SUM(distance_to_travel)/SUM(monetary_cost) partition by month ?",2021-09-13T20:26:19Z
UgxgOwnB1IGPKoR2xhl4AaABAg,@arielbarbosa7337,,1,XBE09l-UYTE,1,0,2021-09-03T19:56:11Z,Do you recommend any books to learn advanced sql? Thanks.,2021-09-03T19:56:11Z
UgxgOwnB1IGPKoR2xhl4AaABAg.9RqJZHfnne39RsKhsZ3RCh,@stratascratch,UgxgOwnB1IGPKoR2xhl4AaABAg,2,XBE09l-UYTE,0,1,2021-09-04T14:44:43Z,"I haven&#39;t found any books that I liked for advanced SQL. My favorite tutorial is Mode Analytics SQL (<a href=""https://mode.com/sql-tutorial/introduction-to-sql/)"">https://mode.com/sql-tutorial/introduction-to-sql/)</a>. They have everything you need to learn the functions and theory. The rest is practice!",2021-09-04T14:44:43Z
UgwWh07auvXYMhq6a4F4AaABAg,@Zzzz-hk5ft,,1,XBE09l-UYTE,1,0,2021-08-26T05:20:04Z,"Great stuff here, I went in having only used windowed functions by copy and pasting from stackoverflow, but after watching this I really feel like I understand what is going on. Hope I get to the next round of my interview tomorrow.. wish me luck!",2021-08-26T05:20:04Z
UgwWh07auvXYMhq6a4F4AaABAg.9RV8wPUCNK89RWGEAkUW7h,@stratascratch,UgwWh07auvXYMhq6a4F4AaABAg,2,XBE09l-UYTE,0,0,2021-08-26T15:43:02Z,Glad I could help! And good luck on your interview.,2021-08-26T15:43:02Z
Ugx3LBAqpz3tUp4kmKJ4AaABAg,@jameshizon4861,,1,XBE09l-UYTE,3,0,2021-08-24T18:41:43Z,Wow I found you on LinkedIn. I am also a UCD grad with DS background.,2021-08-24T18:41:43Z
Ugx3LBAqpz3tUp4kmKJ4AaABAg.9RRR58qfscO9RRjYNYk1xF,@stratascratch,Ugx3LBAqpz3tUp4kmKJ4AaABAg,2,XBE09l-UYTE,0,1,2021-08-24T21:31:44Z,"Oh nice! I graduated with a degree in BME. I got your previous message about reminding you of WongFu. Interestingly, we&#39;re friends. We went to the same high school so maybe our personalities rubbed off one another =)",2021-08-24T21:31:44Z
Ugx3LBAqpz3tUp4kmKJ4AaABAg.9RRR58qfscO9RRjz1XModl,@jameshizon4861,Ugx3LBAqpz3tUp4kmKJ4AaABAg,2,XBE09l-UYTE,0,0,2021-08-24T21:35:31Z,@@stratascratch Lol WOW,2021-08-24T21:35:31Z
Ugx3LBAqpz3tUp4kmKJ4AaABAg.9RRR58qfscO9RRk47aQkM4,@jameshizon4861,Ugx3LBAqpz3tUp4kmKJ4AaABAg,2,XBE09l-UYTE,0,0,2021-08-24T21:36:21Z,Also helped with developing YT channel :),2021-08-24T21:36:21Z
Ugy7ZUyYvebnIbPQeQh4AaABAg,@snigdho642,,1,XBE09l-UYTE,1,1,2021-08-19T14:37:48Z,"Very good explanation of the problems. Really enjoyed the video. Just one thought - on the first question about averaging the fare/km for the month, what we did here is basically an average of the ratio. Should we instead calculate it as the SUM(distance for the month)/SUM(fare for the month)?",2021-08-19T14:37:48Z
Ugy7ZUyYvebnIbPQeQh4AaABAg.9RE7CIRCS5H9neRKAvn5UX,@terryh558,Ugy7ZUyYvebnIbPQeQh4AaABAg,2,XBE09l-UYTE,0,0,2023-03-24T21:15:12Z,I agree with you.,2023-03-24T21:15:12Z
Ugw14ahSFArB8z5TUmZ4AaABAg,@niveditakumari701,,1,XBE09l-UYTE,2,0,2021-08-12T19:18:25Z,For the Twitter  Question can we deduplicate by using DISTINCT keyword instead of GROUP BY?,2021-08-12T19:18:25Z
Ugw14ahSFArB8z5TUmZ4AaABAg.9QxakTrX9Yy9QxwbGwW5Aa,@stratascratch,Ugw14ahSFArB8z5TUmZ4AaABAg,2,XBE09l-UYTE,0,1,2021-08-12T22:29:24Z,"Yes, I believe that should work.",2021-08-12T22:29:24Z
Ugw14ahSFArB8z5TUmZ4AaABAg.9QxakTrX9Yy9R-O3L0Og3w,@niveditakumari701,Ugw14ahSFArB8z5TUmZ4AaABAg,2,XBE09l-UYTE,0,0,2021-08-13T21:16:31Z,@@stratascratch Thanks!,2021-08-13T21:16:31Z
UgxTjq61941Oh2WP6z54AaABAg,@niveditakumari701,,1,XBE09l-UYTE,3,0,2021-08-12T19:11:14Z,"Hey, quick question, GROUP BY should be a.request_date as not b. as we are doing AVG() and have a.request_date and a.dist_to_cost there?",2021-08-12T19:11:14Z
UgxTjq61941Oh2WP6z54AaABAg.9Qx_vkoBOSi9Qxa1I7tqsZ,@niveditakumari701,UgxTjq61941Oh2WP6z54AaABAg,2,XBE09l-UYTE,0,0,2021-08-12T19:12:07Z,For the 1st example here (Uber),2021-08-12T19:12:07Z
UgxTjq61941Oh2WP6z54AaABAg.9Qx_vkoBOSi9QxwriSUsww,@stratascratch,UgxTjq61941Oh2WP6z54AaABAg,2,XBE09l-UYTE,0,0,2021-08-12T22:31:39Z,Not sure if it makes a difference here. I don&#39;t think we&#39;ll get a different solution.,2021-08-12T22:31:39Z
UgxTjq61941Oh2WP6z54AaABAg.9Qx_vkoBOSi9R-O2CF-7Fp,@niveditakumari701,UgxTjq61941Oh2WP6z54AaABAg,2,XBE09l-UYTE,0,0,2021-08-13T21:16:22Z,"@@stratascratch  thanks, I just wanted to verify my understanding!",2021-08-13T21:16:22Z
UgyRGEJJ7OZNlgnVW954AaABAg,@zomgitsnarbe28,,1,XBE09l-UYTE,1,5,2021-08-02T03:23:24Z,"I wanted to thank you for such quality material, it is much appreciated.<br><br>I just got a doubt I cannot clear.<br>On the LAG example, I originally did it without the Cast as numeric in the dividend of the growth formula and the results basically showed only for the rows with 3 digits.<br>What is the casting to numeric is doing there?",2021-08-02T03:23:24Z
UgyRGEJJ7OZNlgnVW954AaABAg.9QX8Vt7cLFV9QYN_S7gqsW,@stratascratch,UgyRGEJJ7OZNlgnVW954AaABAg,2,XBE09l-UYTE,0,0,2021-08-02T14:54:20Z,"In our platform, seems like it might have already been a numeric or float, if it returned 3 digits? In our platform, we cut off the digits and limit to only 3 to make the solution checker work for all data types.",2021-08-02T14:54:20Z
UgwAykWc1hhZydWmeuN4AaABAg,@niveditakumari701,,1,XBE09l-UYTE,0,0,2021-07-31T17:26:08Z,"Really awesome to clarify the concepts and their usage, loved it",2021-07-31T17:26:08Z
UgwUwag56ruxK3A8Klp4AaABAg,@anirudhvallabh419,,1,XBE09l-UYTE,1,0,2021-07-18T21:10:03Z,"For the NTILE question I think it would make more sense to ORDER BY ASC on fraud score in the window function. Currently your percentile calculation takes the highest fraud score to be 1st percentile but it should actually be 100th percentile.<br>So 90th percentile would mean, your score is higher than 90% of all the scores out there.",2021-07-18T21:10:03Z
UgwUwag56ruxK3A8Klp4AaABAg.9PxQeDxxDZH9gvqmEEL2zZ,@timmytesla9655,UgwUwag56ruxK3A8Klp4AaABAg,2,XBE09l-UYTE,0,0,2022-10-08T15:53:42Z,I had the same thought,2022-10-08T15:53:42Z
UgyfM0Bq-6awVdBdDlp4AaABAg,@hendrywaldochendrawan6065,,1,XBE09l-UYTE,0,0,2021-07-13T07:13:51Z,"For the second question - <a href=""https://www.youtube.com/watch?v=XBE09l-UYTE&amp;t=11m24s"">11:24</a><br>why do we need to create query = a ? why not just use one with rank ID as the subquery. is there a flaw by doing so?<br><br>SELECT DEPARTMENT, SALARY, RANK_ID<br>FROM (<br>             SELECT DEPARTMENT, SALARY, RANK() OVER (PARTITION BY DEPARTMENT ORDER BY SALARY DESC) AS RANK_ID<br>             FROM TWITTER_EMPLOYEE<br>             GROUP BY 1, 2<br>             ORDER BY 1, 2 desc) A<br>WHERE RANK_ID &lt; 4",2021-07-13T07:13:51Z
Ugw9MEfHR13-DgNSQ_V4AaABAg,@sugandhtiwari1667,,1,XBE09l-UYTE,0,0,2021-07-13T06:27:32Z,"You are really good at this , best explained!!!",2021-07-13T06:27:32Z
UgxwkDYZHbBjZ0CFT9F4AaABAg,@yuye2941,,1,XBE09l-UYTE,1,0,2021-07-11T23:26:33Z,"May I also ask what does &quot;t1) t2&quot; means in <a href=""https://www.youtube.com/watch?v=XBE09l-UYTE&amp;t=20m18s"">20:18</a>? Thanks so much",2021-07-11T23:26:33Z
UgxwkDYZHbBjZ0CFT9F4AaABAg.9Pfdhp9q1QL9Pg3hjBqrvj,@stratascratch,UgxwkDYZHbBjZ0CFT9F4AaABAg,2,XBE09l-UYTE,0,0,2021-07-12T03:22:28Z,"Hi, it&#39;s the same thing as your last question. We&#39;re aliasing the subqueries and giving them a name.",2021-07-12T03:22:28Z
UgygzXyatsokYMUL9qh4AaABAg,@yuye2941,,1,XBE09l-UYTE,1,0,2021-07-11T23:16:45Z,"Hi Nate,<br><br>Thanks for the video. May I ask why do we put &quot;a&quot; after order by year in <a href=""https://www.youtube.com/watch?v=XBE09l-UYTE&amp;t=19m25s"">19:25</a>? Especially we are not referring &quot;a&quot; in the select statement.",2021-07-11T23:16:45Z
UgygzXyatsokYMUL9qh4AaABAg.9Pfca4h3ISl9Pg3d57mm9-,@stratascratch,UgygzXyatsokYMUL9qh4AaABAg,2,XBE09l-UYTE,0,0,2021-07-12T03:21:50Z,It&#39;s an alias given to the subquery. Sometimes it&#39;s required when writing a SQL query. You&#39;re basically &quot;naming&quot; the table.,2021-07-12T03:21:50Z
UgzsMv9PGu2lH8R3hVt4AaABAg,@ranpan2861,,1,XBE09l-UYTE,1,0,2021-07-08T02:57:56Z,"For the NTILE question, if i&#39;m understanding this function correctly, it just divides the data into 100 bins. So what if I want to find a percentile when the data is less than 100, say 80? Then the top 5 percentile isn&#39;t the top 5 bins anymore. Thanks",2021-07-08T02:57:56Z
UgzsMv9PGu2lH8R3hVt4AaABAg.9PWiiWFoOi_9PYBUA1EglW,@stratascratch,UgzsMv9PGu2lH8R3hVt4AaABAg,2,XBE09l-UYTE,0,0,2021-07-08T16:37:13Z,"Yes, that&#39;s right. It wouldn&#39;t necessarily be the top 5 bigs. It may just be the top 4 bins that represent the top 5 percent since you&#39;ll only have 80 bins.",2021-07-08T16:37:13Z
UgyfExV5bYlySlEFq7t4AaABAg,@arthuramanyire526,,1,XBE09l-UYTE,1,0,2021-07-05T07:19:52Z,Super helpful. Could you please share the dataset?,2021-07-05T07:19:52Z
UgyfExV5bYlySlEFq7t4AaABAg.9PPTJIYI1cA9PRlTl7n7O7,@stratascratch,UgyfExV5bYlySlEFq7t4AaABAg,2,XBE09l-UYTE,0,0,2021-07-06T04:45:47Z,It&#39;s on the platform! Most people will just copy and paste the output to an excel,2021-07-06T04:45:47Z
Ugw6UKfFMoNa7Wgc4kd4AaABAg,@Quenchedfooty,,1,XBE09l-UYTE,1,1,2021-06-28T11:52:39Z,"Thanks SQL part is clear but a non SQL question about this example is bothering me, to compute average distance to cost - should we consider average of row averages (distance_to_travel/monetary_cost) or consider Sum(distance_to_travel) / Sum(monetary_cost ). Kindly help.",2021-06-28T11:52:39Z
Ugw6UKfFMoNa7Wgc4kd4AaABAg.9P7vy6zWeYX9P9f6-sv7MT,@stratascratch,Ugw6UKfFMoNa7Wgc4kd4AaABAg,2,XBE09l-UYTE,0,0,2021-06-29T04:03:47Z,"&quot;averages (distance_to_travel/monetary_cost) &quot; I think should work on this. If you have a specific question like this, feel free to ask the question on the platform. Someone on my team will help you within a few days.",2021-06-29T04:03:47Z
UgxS5inYWweZbXADOdN4AaABAg,@lanasedayanoch,,1,XBE09l-UYTE,1,0,2021-06-28T11:42:56Z,is this only for PostgreSQL?,2021-06-28T11:42:56Z
UgxS5inYWweZbXADOdN4AaABAg.9P7uqwTIwHf9P9epAzUkX7,@stratascratch,UgxS5inYWweZbXADOdN4AaABAg,2,XBE09l-UYTE,0,0,2021-06-29T04:01:21Z,"Yes this is postgres. By the end of the year, the platform should be able to handle most db engines. What db engine are you looking for?",2021-06-29T04:01:21Z
UgxJZ2cVNlnDnyRfUQh4AaABAg,@wenlizhang5283,,1,XBE09l-UYTE,1,0,2021-06-23T18:58:52Z,"Hi Nate, thank you for the video. For the distance per dollar question. I don&#39;t think we need group by in the end. Because we just need difference per day.",2021-06-23T18:58:52Z
Ugyr7CkYvXKs1zmyRHF4AaABAg,@bananaboydan3642,,1,C0hk-dZ9XSE,1,0,2023-07-04T00:37:59Z,why is just checking if our solution is correct a premium feature? or am i doing something wrong on the site,2023-07-04T00:37:59Z
Ugyr7CkYvXKs1zmyRHF4AaABAg.9rirohGd1i39rl-0Ml6MLD,@stratascratch,Ugyr7CkYvXKs1zmyRHF4AaABAg,2,C0hk-dZ9XSE,0,0,2023-07-04T20:28:05Z,Checking your solution is a premium feature. Most platforms will lock the question completely if it&#39;s premium. We allow you to execute code and manually check the output of your solution. But the solution itself is a premium feature.,2023-07-04T20:28:05Z
UgymSom2sHhG5WqtIZ14AaABAg,@VloggingMemories,,1,C0hk-dZ9XSE,0,0,2023-03-15T09:34:13Z,"My Solution:<br>select <br>    airbnb_year,<br>    curr_host_cnt,<br>    prev_host_cnt,<br>    round((curr_host_cnt-prev_host_cnt)/prev_host_cnt::numeric, 2)*100 growth_rate<br>from<br>(select <br>    to_char(host_since , &#39;YYYY&#39;) airbnb_year,<br>    count(distinct id) curr_host_cnt,<br>    lag(count(distinct id)) over (order by to_char(host_since , &#39;YYYY&#39;)) prev_host_cnt<br>from airbnb_search_details<br>group by 1)t1;",2023-03-15T09:34:13Z
Ugxa9r20hrvW5lK6N-l4AaABAg,@dwaipayansaha4443,,1,C0hk-dZ9XSE,1,1,2022-08-21T13:45:42Z,"My solution,<br>with t1 as (select year(host_since) yearly,count(id) pres_year,lag(count(id),1,0) over(order by year(host_since)) prev_year from airbnb_search_details<br>group by yearly)<br>select yearly,pres_year,prev_year,round((pres_year-prev_year)/prev_year,2)*100 per_growth from t1<br>order by yearly",2022-08-21T13:45:42Z
Ugxa9r20hrvW5lK6N-l4AaABAg.9f-0ym5p3YR9f-dWNm8BFZ,@stratascratch,Ugxa9r20hrvW5lK6N-l4AaABAg,2,C0hk-dZ9XSE,0,0,2022-08-21T19:31:12Z,That is awesome!  Thank you for sharing.,2022-08-21T19:31:12Z
UgwDZrWEqPHkYD8lU3Z4AaABAg,@pramodyadav4422,,1,C0hk-dZ9XSE,2,0,2022-04-07T07:53:33Z,"Hello Nate,<br>Thanks for your videos, Each video brings a new learning.<br>I&#39;m little bit confuse regarding ::FLOAT. Sometime it works, sometime it don&#39;t.<br>I started to use CAST function to do this.<br>Can you please explain when ::FLOAT works?<br><br>I was using MySQL(Beta) as interpreter. I remember few days back it worked for me with ::FLOAT.<br>But for this question it gave an error.",2022-04-07T07:53:33Z
UgwDZrWEqPHkYD8lU3Z4AaABAg.9_WCZW4-Mit9_XQN08gG8i,@stratascratch,UgwDZrWEqPHkYD8lU3Z4AaABAg,2,C0hk-dZ9XSE,0,1,2022-04-07T19:13:25Z,::float is a postgres notation for cast(column as float). You can use cast() for either postgres or mysql. But ::float is a postgres syntax so it only works for postgres.,2022-04-07T19:13:25Z
UgwDZrWEqPHkYD8lU3Z4AaABAg.9_WCZW4-Mit9_XSK06uz7E,@pramodyadav4422,UgwDZrWEqPHkYD8lU3Z4AaABAg,2,C0hk-dZ9XSE,0,0,2022-04-07T19:30:28Z,@@stratascratch thank you for clarifying.<br>I didn&#39;t know I learnt postgres as well while watching your videos üòä,2022-04-07T19:30:28Z
Ugwplf0q-HAvhtpBqAl4AaABAg,@kritiverma1342,,1,C0hk-dZ9XSE,1,1,2022-03-31T18:37:44Z,"Same method:<br><br>Select year, current, prev, ROUND(((current-prev)/prev::FLOAT)*100) as perc_growth <br>FROM(<br>    (select extract(year from host_since::DATE) as year, count(id) as current, <br>    LAG(count(id) , 1) Over (Order by extract(year from host_since::DATE)) as prev<br>    from airbnb_search_details<br>    WHERE host_since IS NOT NULL <br>    GROUP BY extract(year from host_since::DATE))) sub",2022-03-31T18:37:44Z
Ugwplf0q-HAvhtpBqAl4AaABAg.9_FKia5ydxd9_fJjRVLpzg,@anggipermanaharianja6122,Ugwplf0q-HAvhtpBqAl4AaABAg,2,C0hk-dZ9XSE,0,0,2022-04-11T06:08:36Z,"yours is basically true, for readability I think it would be better if you put it like this:<br><br>with cte as (<br>    select extract(year from host_since::date) as curr_year,<br>        count(id) as curr_year_count<br>    from airbnb_search_details<br>    group by 1<br>    order by 1 asc<br>)<br><br>select cte.curr_year as current_year,<br>    cte.curr_year - 1 as previous_year,<br>    coalesce(100 * (cte.curr_year_count - lag(curr_year_count, 1) over (order by curr_year asc)) / lag(curr_year_count, 1) over (order by curr_year asc),0) as growth_pct<br>from cte",2022-04-11T06:08:36Z
Ugyp8ky2Gl9Vq1B5GOp4AaABAg,@elfridhasman4181,,1,C0hk-dZ9XSE,0,0,2021-12-07T10:48:21Z,Thank you very much Nateü§óü§óü§ó,2021-12-07T10:48:21Z
Ugz1UIVIw2Ok1-5xAHp4AaABAg,@shobhamourya8396,,1,C0hk-dZ9XSE,2,0,2021-11-28T16:20:34Z,"Would it affect the performance if I used count(*) instead of count(id) ?<br><br>Here&#39;s my solution using CTE:<br>with cte1<br>as<br>(<br>    select id, EXTRACT(YEAR from host_since) curr_year<br>    from airbnb_search_details<br>    order by curr_year<br>),<br>cte2<br>as<br>(<br>    select curr_year, count(id) hosts_curr_year<br>    from cte1<br>    group by curr_year<br>    order by curr_year<br>),<br>cte3<br>as<br>(<br>    select curr_year, hosts_curr_year,<br>       lag(hosts_curr_year, 1) over(order by curr_year) hosts_prev_year<br>    from cte2<br>)<br>select curr_year, hosts_curr_year, hosts_prev_year, <br>       round(((hosts_curr_year - hosts_prev_year)*1.0/hosts_prev_year*1.0)*100.00::Decimal) as growth_rate<br>      from cte3",2021-11-28T16:20:34Z
Ugz1UIVIw2Ok1-5xAHp4AaABAg.9VINFJG04Sp9VIm748oi9_,@stratascratch,Ugz1UIVIw2Ok1-5xAHp4AaABAg,2,C0hk-dZ9XSE,0,0,2021-11-28T20:06:38Z,It won&#39;t effect performance. But you should ask yourself you&#39;re interested in counting the rows or counting non-null rows!,2021-11-28T20:06:38Z
Ugz1UIVIw2Ok1-5xAHp4AaABAg.9VINFJG04Sp9VJoLWsFcZ6,@shobhamourya8396,Ugz1UIVIw2Ok1-5xAHp4AaABAg,2,C0hk-dZ9XSE,0,0,2021-11-29T05:45:19Z,"@@stratascratch count(*) will count all rows, including Null values of the aggregating column whereas count(id) will count only not-Null values of column id for the aggregating column. In this dataset, host ids are unique and not-Null so it won&#39;t make a difference.",2021-11-29T05:45:19Z
UgyhMKWYuMvJn4VXAw14AaABAg,@nasos00,,1,C0hk-dZ9XSE,1,0,2021-11-27T22:24:01Z,"The date field mentions &quot;host since&quot;. Doesn&#39;t that mean that we need to count that host for every subsequent year as well? So before using the lag function, shouldn&#39;t we compute the cumulative sum of current_year_host? Of course this assumes that once they registered as a host they remained a host and didn&#39;t change their status. Anyway, curious about your thoughts on this.",2021-11-27T22:24:01Z
UgyhMKWYuMvJn4VXAw14AaABAg.9VGS2PEEoD79VIluOGppcp,@stratascratch,UgyhMKWYuMvJn4VXAw14AaABAg,2,C0hk-dZ9XSE,0,1,2021-11-28T20:04:46Z,"That&#39;s a good assumption and something I would consider. However, when I interpret the question, it&#39;s taking the # of hosts registered in the current year vs previous year. So I&#39;m not caring about hosts from other years and thus I don&#39;t take the cum sum. That&#39;s my way of thinking about the problem and interpreting it. But with most data science questions, there are grey areas and it&#39;s always about answering the question given the business context. This is a question I would talk through with the interviewer to get a better understanding of what he/she wants.",2021-11-28T20:04:46Z
UgwTv0vfffsjpf_b8kl4AaABAg,@AdnanKashemOxe,,1,C0hk-dZ9XSE,0,0,2021-07-17T09:31:17Z,Your contents are awesome. Keep up the good work.. Best Wishes,2021-07-17T09:31:17Z
UgxYQLHXNCs1dCA3L3R4AaABAg,@bobbybarnes4249,,1,C0hk-dZ9XSE,1,0,2021-03-16T16:48:45Z,Thank you for breaking down each step. Love the videos!,2021-03-16T16:48:45Z
UgxYQLHXNCs1dCA3L3R4AaABAg.9KxfArUmDjz9Kxhk7Hkbap,@stratascratch,UgxYQLHXNCs1dCA3L3R4AaABAg,2,C0hk-dZ9XSE,0,0,2021-03-16T17:11:11Z,Thanks for watching! more videos to come,2021-03-16T17:11:11Z
UgxxyfY6WmGDq8Wqsjt4AaABAg,@arijitchakraborty266,,1,C0hk-dZ9XSE,2,0,2021-03-04T19:50:48Z,"If you kindly post the test data, it will be much helpful. Thanks much !!",2021-03-04T19:50:48Z
UgxxyfY6WmGDq8Wqsjt4AaABAg.9KU5TivlVZ39KU60rK0iMD,@stratascratch,UgxxyfY6WmGDq8Wqsjt4AaABAg,2,C0hk-dZ9XSE,0,1,2021-03-04T19:55:36Z,"There&#39;s a link to the test data in the description (<a href=""https://platform.stratascratch.com/coding-question?id=9637&amp;python=)"">https://platform.stratascratch.com/coding-question?id=9637&amp;python=)</a>. You can code on the platform. It&#39;s totally free to use the data and execute the code.",2021-03-04T19:55:36Z
UgxxyfY6WmGDq8Wqsjt4AaABAg.9KU5TivlVZ39KUF4R2Q4aJ,@arijitchakraborty266,UgxxyfY6WmGDq8Wqsjt4AaABAg,2,C0hk-dZ9XSE,0,0,2021-03-04T21:14:43Z,@@stratascratch Take love Sir ! What you are doing just awesome. God bless you.,2021-03-04T21:14:43Z
UgyecZjfl3Byukqm73B4AaABAg,@MadSKWIREL,,1,C0hk-dZ9XSE,1,0,2021-02-17T09:27:43Z,"Not the best showcase of the window function, because self &quot;join on t1.year = t2.year +1&quot; is more simple and intuitive solution.",2021-02-17T09:27:43Z
UgyecZjfl3Byukqm73B4AaABAg.9JrMF-iuLE89JrxHpC6OIU,@stratascratch,UgyecZjfl3Byukqm73B4AaABAg,2,C0hk-dZ9XSE,0,0,2021-02-17T15:00:09Z,That&#39;s true. A self join with a year = year+1 is another great way of solving the problem. It would save you from actually implementing the lag(). Most things would stay about the same as the calculations use both the current year and last year values. Thanks for actually watching the coding part of the video.,2021-02-17T15:00:09Z
UgyQGAD6dWrxZsePbqp4AaABAg,@manavsaxena5579,,1,C0hk-dZ9XSE,1,0,2021-01-15T13:27:34Z,"Came across your channel by accident. It took me One Video to Subscribe! Great Content, please keep it coming.",2021-01-15T13:27:34Z
UgyQGAD6dWrxZsePbqp4AaABAg.9IXoSqWGqpM9IY4aRZB5Nc,@stratascratch,UgyQGAD6dWrxZsePbqp4AaABAg,2,C0hk-dZ9XSE,0,0,2021-01-15T15:57:17Z,Thanks so much. Let me know if you have any topic requests or have any feedback!,2021-01-15T15:57:17Z
UgxF-RhWNA0OSkhVbil4AaABAg,@RedShipsofSpainAgain,,1,C0hk-dZ9XSE,1,3,2020-12-13T15:32:00Z,Really great Nate.  Curious how you would implement this in Python using Pandas?,2020-12-13T15:32:00Z
UgxF-RhWNA0OSkhVbil4AaABAg.9HD3TINjoPH9I3Av_13FYk,@stratascratch,UgxF-RhWNA0OSkhVbil4AaABAg,2,C0hk-dZ9XSE,0,1,2021-01-03T15:56:09Z,"If you go to the link in the description, I have the python solution in the platform =)",2021-01-03T15:56:09Z
UgxWi1XBmiRUHtSS5Qh4AaABAg,@ashimneupane1234,,1,C0hk-dZ9XSE,1,1,2020-12-10T15:04:15Z,Thank you,2020-12-10T15:04:15Z
UgxWi1XBmiRUHtSS5Qh4AaABAg.9H5HuR30BNK9H5LewtPacw,@stratascratch,UgxWi1XBmiRUHtSS5Qh4AaABAg,2,C0hk-dZ9XSE,0,0,2020-12-10T15:37:05Z,"Thanks for watching. Feel free to provide comments, new ideas, and feedback.",2020-12-10T15:37:05Z
UgyWA1RcPU0XK3T9fEJ4AaABAg,@poesaste,,1,C0hk-dZ9XSE,1,0,2020-12-10T13:35:24Z,This was great Nate üëç,2020-12-10T13:35:24Z
UgyWA1RcPU0XK3T9fEJ4AaABAg.9H57jl3xHaN9H5LcvXJub_,@stratascratch,UgyWA1RcPU0XK3T9fEJ4AaABAg,2,C0hk-dZ9XSE,0,2,2020-12-10T15:36:48Z,Appreciate the kind words. I wrap up window functions next week with a video on the 4 different types of window functions you&#39;ll see on a coding interview and examples of each. Hope you watch that one too.,2020-12-10T15:36:48Z
Ugxjb0KXUIedLD7KY254AaABAg,@stratascratch,,1,EPUayNC5ku4,0,4,2020-11-19T15:04:26Z,"Timestamps:<br><br>Intro: (<a href=""https://www.youtube.com/watch?v=EPUayNC5ku4&amp;t=0m00s"">0:00</a>)<br>Interview Question: (<a href=""https://www.youtube.com/watch?v=EPUayNC5ku4&amp;t=0m11s"">0:11</a>)<br>Explore the Dataset: (<a href=""https://www.youtube.com/watch?v=EPUayNC5ku4&amp;t=0m38s"">0:38</a>)<br>Solution Approach: (<a href=""https://www.youtube.com/watch?v=EPUayNC5ku4&amp;t=1m27s"">1:27</a>)<br>Coding - JOIN The Tables: (<a href=""https://www.youtube.com/watch?v=EPUayNC5ku4&amp;t=3m05s"">3:05</a>)<br>Coding - Explanation of the SQL Window Function: (<a href=""https://www.youtube.com/watch?v=EPUayNC5ku4&amp;t=4m22s"">4:22</a>)<br>Coding - Finishing up the Solution: (<a href=""https://www.youtube.com/watch?v=EPUayNC5ku4&amp;t=6m37s"">6:37</a>)",2020-11-19T15:04:26Z
UgzvCz1-u4cz5Z3_Zdh4AaABAg,@Reemo3,,1,EPUayNC5ku4,2,0,2023-06-17T18:52:49Z,What is this using? :: does not convert the data type,2023-06-17T18:52:49Z
UgzvCz1-u4cz5Z3_Zdh4AaABAg.9r42acLlxfD9r4WciX_9dO,@stratascratch,UgzvCz1-u4cz5Z3_Zdh4AaABAg,2,EPUayNC5ku4,0,1,2023-06-17T23:15:14Z,":: is a cast function in postgres. If you&#39;re using other sql engines, the function is usually cast(column_name as data_type)",2023-06-17T23:15:14Z
UgzvCz1-u4cz5Z3_Zdh4AaABAg.9r42acLlxfD9rCZs23AxXj,@Reemo3,UgzvCz1-u4cz5Z3_Zdh4AaABAg,2,EPUayNC5ku4,0,0,2023-06-21T02:17:28Z,"@@stratascratch very cool and simple, seen it in Python. Hopefully will have some Postgres exposure soon",2023-06-21T02:17:28Z
UgzRTyaUfjlS9ju9qCB4AaABAg,@aashishmalhotra,,1,EPUayNC5ku4,1,2,2022-08-20T10:44:43Z,Ok i write steps on my own by breaking down question. But how to implement it helped me. Also converting to float is learnt thanku,2022-08-20T10:44:43Z
UgzRTyaUfjlS9ju9qCB4AaABAg.9ex7TLRRkPx9eyF_0k1B-b,@stratascratch,UgzRTyaUfjlS9ju9qCB4AaABAg,2,EPUayNC5ku4,0,0,2022-08-20T21:14:46Z,That is great.  Keep moving forward.,2022-08-20T21:14:46Z
UgxiSgv59cIyZY0RT8t4AaABAg,@capybaraRed,,1,EPUayNC5ku4,1,0,2022-08-04T21:54:02Z,"I was less sophisticated than you on my attempt (btw the parameters of the challenged have changed a bit). Guess I just love subqueries üòÖ. Sum over partition was genius.<br><br>SELECT c.first_name<br>, o.order_details<br>, round(total_order_cost/sub.total_spend,2) as perc_total_spend<br>from orders o<br>join <br>	(select cust_id, sum(total_order_cost) as total_spend<br>		from orders group by 1) sub<br>    on o.cust_id = sub.cust_id<br>join customers c<br>    on o.cust_id = <a href=""http://c.id/"">c.id</a><br>order by 1,2,3",2022-08-04T22:00:51Z
UgxiSgv59cIyZY0RT8t4AaABAg.9eK7LaD1dn29eMDsokkGpE,@stratascratch,UgxiSgv59cIyZY0RT8t4AaABAg,2,EPUayNC5ku4,0,1,2022-08-05T17:29:37Z,Amazing!  Thank you.,2022-08-05T17:29:37Z
UgyyzdgvO0KX30jVBO94AaABAg,@mangoout8683,,1,EPUayNC5ku4,0,0,2022-08-04T18:32:59Z,Why we didn‚Äôt take order_quantity into calculations?<br>Apart from this solution is really easy if you know how to use partition by function,2022-08-04T18:32:59Z
Ugx7rirDWh_TcsWV47d4AaABAg,@SandyCoco1,,1,EPUayNC5ku4,3,0,2022-08-02T20:03:39Z,The way you explain these sql queries are a üíØ<br>Please what&#39;s the symbol used in converting int to float? Is it colon (:),2022-08-02T20:03:39Z
Ugx7rirDWh_TcsWV47d4AaABAg.9eEm7MJ4Z5j9eEnXjaoIWx,@stratascratch,Ugx7rirDWh_TcsWV47d4AaABAg,2,EPUayNC5ku4,0,1,2022-08-02T20:15:59Z,"It&#39;s a double colon (::) for postgres. But for other db engines like MySQL it&#39;s cast() -- e.g., cast(column_name as int). You can use cast() for postgres too but most just use :: to save time.",2022-08-02T20:15:59Z
Ugx7rirDWh_TcsWV47d4AaABAg.9eEm7MJ4Z5j9eEnZIuk1CV,@stratascratch,Ugx7rirDWh_TcsWV47d4AaABAg,2,EPUayNC5ku4,0,1,2022-08-02T20:16:12Z,Also thanks for the kind words!,2022-08-02T20:16:12Z
Ugx7rirDWh_TcsWV47d4AaABAg.9eEm7MJ4Z5j9eFc_NnB5FT,@SandyCoco1,Ugx7rirDWh_TcsWV47d4AaABAg,2,EPUayNC5ku4,0,0,2022-08-03T03:59:28Z,@@stratascratch Thank you for the extra tip. I&#39;m learning on MySQL ‚ù§,2022-08-03T03:59:28Z
UgxBhlyUSQOb2f-7JO54AaABAg,@halildurmaz7827,,1,EPUayNC5ku4,1,1,2022-07-31T12:06:17Z,"Do interviewers allow us to create example tables (2 rows + Vertical), on paper? Because, it really helps me to think about the solution.",2022-07-31T12:06:17Z
UgxBhlyUSQOb2f-7JO54AaABAg.9e8luHbNsXG9e9cOYrsm00,@stratascratch,UgxBhlyUSQOb2f-7JO54AaABAg,2,EPUayNC5ku4,0,0,2022-07-31T20:02:25Z,I think that is okay.  Everyone has their own process of coming up with a solution.  Talk to your interviewer.,2022-07-31T20:02:25Z
Ugwl8f7T38Hi1gGfUkh4AaABAg,@emiliogarza6446,,1,EPUayNC5ku4,1,1,2022-07-23T22:37:05Z,"Edit: Just saw the data changed, mine appears as 20, 125, 60 and yours appears as 20, 100, 60. <br><br>I tried solving this with a correlated subquery but it gave me different percentages than yours.<br><br>For example, for Eva your percentages are 56, 11, and 33. Mine appear as 60.98, 9.76, 29.27. If you do manual verification, you&#39;ll see that Eva spent $205 total, so the percentages (125, 20, 60) should be 60.98, 9.76, 29.27. Weird thing is that I&#39;m getting the same percentages as you in Farida, what could be the mistake here?<br> <br>This is my solution:<br><br>SELECT c.first_name, <a href=""http://c.id/"">c.id</a>, o1.order_details, ROUND(o1.total_order_cost / (SELECT SUM(total_order_cost)<br>                                    FROM orders o2<br>                                    WHERE o2.cust_id = o1.cust_id)*100, 2) AS per_total_spent<br>FROM orders o1<br>JOIN customers c ON <a href=""http://c.id/"">c.id</a> = o1.cust_id<br>ORDER BY c.first_name, 100",2022-07-23T22:41:41Z
Ugwl8f7T38Hi1gGfUkh4AaABAg.9dqIjKx1M149g-QmNzmkhe,@stratascratch,Ugwl8f7T38Hi1gGfUkh4AaABAg,2,EPUayNC5ku4,0,0,2022-09-15T14:02:35Z,"You&#39;re right; the question and the datasets have been updated in the meantime. There&#39;s no error; the data for Farida stayed the same. From what I see, your code should work OK and return the correct solution on the platform.",2022-09-15T14:02:35Z
UgyWQx-DZhUGFJD0Fm54AaABAg,@jmmmmmmmmmk,,1,EPUayNC5ku4,1,0,2022-07-06T04:23:53Z,Very very good video and well explained. Thank you,2022-07-06T04:23:53Z
UgyWQx-DZhUGFJD0Fm54AaABAg.9d7_6iSavqc9dQtQ1bRUXp,@stratascratch,UgyWQx-DZhUGFJD0Fm54AaABAg,2,EPUayNC5ku4,0,0,2022-07-13T16:26:51Z,Thank you.  Glad you find it helpful.,2022-07-13T16:26:51Z
UgxrbX_vfpbkqADV-gt4AaABAg,@Holu54,,1,EPUayNC5ku4,1,0,2022-07-04T13:56:02Z,"@ Nate, thanks for the effort in putting these stuffs together. I observed that we were silent on an assumption that was made in the question &quot;Assume each customer has a unique first name (i.e., there is only 1 customer named Karen in the dataset) and that customers place at most only 1 order a day&quot;. Don&#39;t you think this assumption would change the dynamics of the solution? Thanks",2022-07-04T13:56:02Z
UgxrbX_vfpbkqADV-gt4AaABAg.9d3S-QM_0fe9d46zbR-_n1,@stratascratch,UgxrbX_vfpbkqADV-gt4AaABAg,2,EPUayNC5ku4,0,0,2022-07-04T20:11:39Z,"Yes, those are assumptions that would definitely change the solution. We were silent in the video but on an interview, I would definitely mention those assumptions. Thanks for watching!",2022-07-04T20:11:39Z
Ugw76J45_T9mPOMEAet4AaABAg,@tarushikhasharma5328,,1,EPUayNC5ku4,1,0,2022-06-27T05:24:51Z,where can we practice  SQL?,2022-06-27T05:24:51Z
Ugw76J45_T9mPOMEAet4AaABAg.9clVwNAbYpZ9cmY2vZILkf,@stratascratch,Ugw76J45_T9mPOMEAet4AaABAg,2,EPUayNC5ku4,0,0,2022-06-27T15:02:36Z,"<a href=""http://stratascratch.com/"">stratascratch.com</a> is what you see in this video. Give it a try!",2022-06-27T15:02:36Z
UgxyAFlY5tv-bRfNitJ4AaABAg,@thulanidendere3986,,1,EPUayNC5ku4,0,0,2022-05-24T08:11:12Z,"You have explained this well sir,",2022-05-24T08:11:12Z
UgzKY3fCPpcbFwpyGfZ4AaABAg,@VishalKumar-iv8gl,,1,EPUayNC5ku4,2,0,2022-05-11T21:26:17Z,Which platform you are using?,2022-05-11T21:26:17Z
UgzKY3fCPpcbFwpyGfZ4AaABAg.9auCayrx-zt9auDMcFLCHV,@stratascratch,UgzKY3fCPpcbFwpyGfZ4AaABAg,2,EPUayNC5ku4,0,1,2022-05-11T21:32:56Z,This is StrataScratch. The coding is in postgres but MySQL is also available. Python is also available on the platform.,2022-05-11T21:32:56Z
UgzKY3fCPpcbFwpyGfZ4AaABAg.9auCayrx-zt9auDZVxcqvw,@VishalKumar-iv8gl,UgzKY3fCPpcbFwpyGfZ4AaABAg,2,EPUayNC5ku4,0,0,2022-05-11T21:34:41Z,@@stratascratch Thanks..I will start practice on your site..üòÖ,2022-05-11T21:34:41Z
Ugxc6ocVjg2psBgjynh4AaABAg,@keifer7813,,1,EPUayNC5ku4,2,1,2022-04-19T04:50:41Z,I&#39;m really starting to like window functions. Question though - could this have been done using a group by?,2022-04-19T04:50:41Z
Ugxc6ocVjg2psBgjynh4AaABAg.9_zmAfTjnKx9a-xCfT21mh,@stratascratch,Ugxc6ocVjg2psBgjynh4AaABAg,2,EPUayNC5ku4,0,1,2022-04-19T15:46:19Z,"Yeah, you could use a group by but window functions are a bit more flexible. Depending on the question, sometimes it&#39;s just more clear to use a window function rather than a group by. It really depends on the question!",2022-04-19T15:46:19Z
Ugxc6ocVjg2psBgjynh4AaABAg.9_zmAfTjnKx9a06stEL8ri,@keifer7813,Ugxc6ocVjg2psBgjynh4AaABAg,2,EPUayNC5ku4,0,0,2022-04-19T17:19:35Z,@@stratascratch Ahh got it. Thank you. Keep the videos coming!,2022-04-19T17:19:35Z
UgyVZyb6dHhYX1nzgCl4AaABAg,@jaychopra8932,,1,EPUayNC5ku4,1,0,2022-04-12T08:47:34Z,what is the syntax for :: in mysql,2022-04-12T08:47:34Z
UgyVZyb6dHhYX1nzgCl4AaABAg.9_iAiYf5D0-9_isFy-ciD7,@stratascratch,UgyVZyb6dHhYX1nzgCl4AaABAg,2,EPUayNC5ku4,0,1,2022-04-12T15:16:44Z,It&#39;s a cast function in mysql. So something like cast(data_column as datatype),2022-04-12T15:16:44Z
Ugx7tJJvdGG0I1STB0R4AaABAg,@yusufbas035,,1,EPUayNC5ku4,1,0,2022-04-05T09:34:40Z,waow <br>that was so nice<br>nice work dude,2022-04-05T09:34:40Z
Ugx7tJJvdGG0I1STB0R4AaABAg.9_REYSWJ4Kv9_REaiAeiU-,@yusufbas035,Ugx7tJJvdGG0I1STB0R4AaABAg,2,EPUayNC5ku4,0,0,2022-04-05T09:35:07Z,it looks like simple but it is helpful,2022-04-05T09:35:07Z
UgzdmINrcyWXi-URgF94AaABAg,@EminentJade,,1,EPUayNC5ku4,0,0,2022-04-05T06:49:55Z,"lifesaver before interview, tyvm.",2022-04-05T06:49:55Z
UgyPGdb0yT_TS9Er_R54AaABAg,@mercantilism954,,1,EPUayNC5ku4,0,0,2022-04-01T22:42:26Z,"This is amazing intro for windows function. Love the video.<br><br>For those who want to replicate his code, &quot;order_cost&quot; has been replaced with &quot;total_rder_cost&quot;",2022-04-01T22:42:26Z
UgwfqEGOkhrTe1Yxgdp4AaABAg,@blacknerd6481,,1,EPUayNC5ku4,0,0,2022-03-31T21:01:36Z,You made this look so easy man. Thanks alot,2022-03-31T21:01:36Z
UgwnUXvQSty3aqtgQLJ4AaABAg,@mohitupadhayay1439,,1,EPUayNC5ku4,0,2,2022-03-26T16:29:10Z,2 months ago I was so afraid to even walk through an SQL video on Youtube. But look at me now! I am diving deeper and learning so much every day. Thankfully i didn&#39;t gave up!,2022-03-26T16:29:10Z
UgxB2p2SNxIJAQrE8JV4AaABAg,@derartudejene780,,1,EPUayNC5ku4,0,0,2022-03-24T22:12:11Z,"<a href=""https://youtu.be/JkzkrPnOp4w"">https://youtu.be/JkzkrPnOp4w</a>. SQL common interview question",2022-03-24T22:12:11Z
UgwGtOMqi-S4pr-Sxux4AaABAg,@pratikparbhane8677,,1,EPUayNC5ku4,0,0,2022-03-21T06:05:00Z,Thanks A Lot !!!!!!!!!!!!!!,2022-03-21T06:05:00Z
UgwYifMz27m1zZO0GuF4AaABAg,@alexandersfetcu1121,,1,EPUayNC5ku4,1,0,2022-03-16T15:35:07Z,CAST(order_cost AS NUMERIC) / SUM(order_cost) OVER( PARTITION BY first_name) * 100 AS Ratio,2022-03-16T15:35:07Z
UgwYifMz27m1zZO0GuF4AaABAg.9ZdNts_jhxx9dqpn0m76HI,@alexandersfetcu1121,UgwYifMz27m1zZO0GuF4AaABAg,2,EPUayNC5ku4,0,0,2022-07-24T03:34:41Z,Easy,2022-07-24T03:34:41Z
UgxtxSBbSWweWCTwfoN4AaABAg,@DaniMrtini,,1,EPUayNC5ku4,0,5,2022-03-15T06:11:10Z,I get the overall gist of solving questions like this but breaking jt down so nicely like you is something I&#39;m strugglong with. Got final interviews with Amazon coming up and just looking around,2022-03-15T06:11:10Z
Ugz72bW8BQjibrQ3oT14AaABAg,@chriskeo392,,1,EPUayNC5ku4,0,0,2022-03-14T06:22:03Z,Content üî•,2022-03-14T06:22:03Z
Ugyjxv7sCNZPdapK41l4AaABAg,@gursimarkaur9340,,1,EPUayNC5ku4,1,0,2022-03-14T04:46:42Z,Can this be done using a simple group by?,2022-03-14T04:46:42Z
Ugyjxv7sCNZPdapK41l4AaABAg.9ZY463Phzf79ZZLZ5AVWPg,@stratascratch,Ugyjxv7sCNZPdapK41l4AaABAg,2,EPUayNC5ku4,0,0,2022-03-14T16:38:27Z,the edge here is that a group by will de-dup while a partition by will preserve all rows. So it might work on this dataset but a group by might fail with another dataset.,2022-03-14T16:38:27Z
Ugwf60rTLTsCr1yNCBB4AaABAg,@jtanakala2976,,1,EPUayNC5ku4,0,0,2022-03-09T11:15:30Z,"Hi,<br><br>This is the table called comments_and_translations<br><br>id	comment  	translation<br><br>1	very good	<br><br>2	good	<br><br>3	bad	<br><br>4	ordinary	<br><br>5	cdcdcdcd	very bad<br><br>6	excellent	<br><br>7	ababab	    not satisfied<br><br>8	satisfied	<br><br>9	aabbaabb	extraordinary<br><br>10	ccddccbb	medium<br><br><br><br><br><br><br><br>The output should be <br><br>comment<br><br>very good<br><br>good<br><br>bad<br><br>ordinary<br><br>very bad<br><br>excellent<br><br>not satisfied<br><br>satisfied<br><br>extraordinary<br><br>medium<br><br><br><br>please help me out with the query.",2022-03-09T11:15:30Z
Ugwh-O05RvDZ_NF-X5R4AaABAg,@GoktugAsc123,,1,EPUayNC5ku4,0,0,2022-03-08T20:04:49Z,"That is a really clear explanation but aren&#39;t we supposed to use the order_quantity column to calculate the total cost per person, too?",2022-03-08T20:04:49Z
UgwKd1rknl3qTmaR3XZ4AaABAg,@oscarherediamiranda616,,1,EPUayNC5ku4,0,0,2022-03-01T04:11:03Z,Hi guy. Thanks you for this video. You have a new subscriber now. I am studying software engineering here in peru and I would like to have a master at data engineering or data scientist.,2022-03-01T04:11:03Z
Ugw7-7FhL-b-yORgKwN4AaABAg,@malice112,,1,EPUayNC5ku4,1,0,2022-02-24T18:34:02Z,can this be solved with a GROUP BY instead?,2022-02-24T18:34:02Z
Ugw7-7FhL-b-yORgKwN4AaABAg.9YqCTotoPi09YqPpXHhum1,@stratascratch,Ugw7-7FhL-b-yORgKwN4AaABAg,2,EPUayNC5ku4,0,1,2022-02-24T20:30:43Z,Yes you can but there might be duplicate rows that effect the outcome. That&#39;s just something to keep top of mind while coding it up,2022-02-24T20:30:43Z
UgwMVK2fMce9iG4D6mt4AaABAg,@thepogchamp7783,,1,EPUayNC5ku4,1,0,2022-02-18T14:15:30Z,"So I did the same thing without a window function by just having a nested select in the from... is that better or worse? And if I can do that, when do I really need a window function?",2022-02-18T14:15:30Z
UgwMVK2fMce9iG4D6mt4AaABAg.9YaI73UXXlH9YaqLx0vY08,@stratascratch,UgwMVK2fMce9iG4D6mt4AaABAg,2,EPUayNC5ku4,0,0,2022-02-18T19:23:22Z,You can choose when to apply a window function vs another method. The nested select is slower because it&#39;s another operation inside a bigger operation that SQL will need to perform. It&#39;s more of an optimization and performance question when you compare the 2,2022-02-18T19:23:22Z
Ugz2uw0dTJsr8w87REN4AaABAg,@HowardLinca,,1,EPUayNC5ku4,0,0,2022-02-17T18:04:21Z,GENIUS...,2022-02-17T18:04:21Z
UgylC_GnHRgXsbTTjRZ4AaABAg,@suchismitachaudhuri4946,,1,EPUayNC5ku4,0,0,2022-02-09T17:00:17Z,the explanation is as easy as it gets &lt;3,2022-02-09T17:00:17Z
Ugyy_s_sXtQcMO8M3wJ4AaABAg,@fabio336ful,,1,EPUayNC5ku4,0,0,2022-01-25T18:17:13Z,Why didnt you use group by?,2022-01-25T18:17:13Z
UgyU_qJiosZJmnvOv6x4AaABAg,@mashiro2598,,1,EPUayNC5ku4,1,0,2022-01-20T03:42:11Z,I think the hard part is understanding the data and what they mean by total customer spend. If someone misreads this part it would be difficult.,2022-01-20T03:42:11Z
UgyU_qJiosZJmnvOv6x4AaABAg.9XPUa5-HBZ69XPX9m1o_dc,@stratascratch,UgyU_qJiosZJmnvOv6x4AaABAg,2,EPUayNC5ku4,0,1,2022-01-20T04:04:40Z,"Agreed and it&#39;s hard to write the question in a clear manner without giving away too much of the solution. On an interview, you can at least ask clarifying questions.",2022-01-20T04:04:40Z
Ugwq-uNA96-Xvh81VKF4AaABAg,@adhirajmajumder,,1,EPUayNC5ku4,1,0,2022-01-14T05:36:54Z,You&#39;re the best SQL/Data science Youtuber.,2022-01-14T05:36:54Z
Ugwq-uNA96-Xvh81VKF4AaABAg.9XAEx8eoOjm9XBb8DVRUIV,@stratascratch,Ugwq-uNA96-Xvh81VKF4AaABAg,2,EPUayNC5ku4,0,1,2022-01-14T18:18:46Z,Thank you! And thanks for watching my videos!,2022-01-14T18:18:46Z
UgwYAzcCheeNWxRhNQ14AaABAg,@asholkkumar4685,,1,EPUayNC5ku4,0,0,2021-12-27T03:18:18Z,Very good explanation .thanks for that.<br>if u don&#39;t mind share the tables source code ..,2021-12-27T03:18:18Z
UgxuPqlxGAyS4tJEqX94AaABAg,@alfredama,,1,EPUayNC5ku4,0,1,2021-11-28T01:09:18Z,"That was really neat , i love the clear and concise train of thought before delving into code",2021-11-28T01:09:18Z
UgzvLLe1XGh7g61tluh4AaABAg,@Vintagetube310,,1,EPUayNC5ku4,1,0,2021-10-26T05:17:29Z,"Thank you for that clear explanation. For someone who wouldn‚Äôt know to use the partition clause, would there be another way to solve this?",2021-10-26T05:17:29Z
UgzvLLe1XGh7g61tluh4AaABAg.9TxD7usQCeq9TyT3M6zwoH,@stratascratch,UgzvLLe1XGh7g61tluh4AaABAg,2,EPUayNC5ku4,0,1,2021-10-26T16:55:55Z,The only other way I know is to create CTEs that would create views like the one the partition clause creates. Then you can join or use the CTE view so that it gets you the output you want.,2021-10-26T16:55:55Z
UgwBqHT_2B-uKDKOeAl4AaABAg,@subhashgn1775,,1,EPUayNC5ku4,0,0,2021-09-19T07:28:02Z,Very well explained.<br>Thank you for providing the great resource.,2021-09-19T07:28:02Z
UgytvT18-5Xd_w_xjGh4AaABAg,@dollajihukum,,1,EPUayNC5ku4,0,0,2021-09-14T17:32:28Z,so nice üôè....,2021-09-14T17:32:28Z
UgziUGntOHK07ETO_oJ4AaABAg,@cabaymau5132,,1,EPUayNC5ku4,0,0,2021-09-07T09:26:38Z,"very well-explained, Thank you and big subcribed",2021-09-07T09:27:30Z
UgzXAyMmG5v8Z0fiwMh4AaABAg,@angelomartinez9843,,1,EPUayNC5ku4,0,0,2021-09-06T21:37:37Z,Great explanation of partition by. I was so lost before watching this video! Thank you!,2021-09-06T21:37:37Z
Ugx2Di0qdlyeAHyu9eB4AaABAg,@IrakliChitishvili,,1,EPUayNC5ku4,2,3,2021-09-04T22:01:12Z,"Great. One question: Using the same window function, how to show total spent by customer? Like Eva: 180, Farida 260 etc without repeating sum over each order_cost?",2021-09-04T22:01:12Z
Ugx2Di0qdlyeAHyu9eB4AaABAg.9Rt6elJraFq9XFUP2aoOV2,@observer698,Ugx2Di0qdlyeAHyu9eB4AaABAg,2,EPUayNC5ku4,0,0,2022-01-16T06:28:08Z,I think we need to use group by then over name field using the SUM(order_cost),2022-01-16T06:37:07Z
Ugx2Di0qdlyeAHyu9eB4AaABAg.9Rt6elJraFq9_zmI5X59FN,@keifer7813,Ugx2Di0qdlyeAHyu9eB4AaABAg,2,EPUayNC5ku4,0,0,2022-04-19T04:51:41Z,@@observer698 Would there even be a point in the window function if we just used a group by?,2022-04-19T04:51:41Z
Ugx25JEJQncddTnqVU14AaABAg,@marinamondadorigessinger6390,,1,EPUayNC5ku4,0,2,2021-09-04T18:17:14Z,"thanks a million, it helped me a lot with the understanding of Partition by clause!",2021-09-04T18:17:14Z
Ugz66Ibgp5Ck6ugrFHN4AaABAg,@user-xt5gu3pt8n,,1,EPUayNC5ku4,1,0,2021-08-15T15:28:33Z,Why PARTITION BY instead of GROUP BY?,2021-08-15T15:28:33Z
Ugz66Ibgp5Ck6ugrFHN4AaABAg.9R3upWXfyJ59R4QAA5CBNw,@stratascratch,Ugz66Ibgp5Ck6ugrFHN4AaABAg,2,EPUayNC5ku4,0,0,2021-08-15T20:11:08Z,Mainly because I wanted to sum all the orders by the user&#39;s name with still preserving the information in order details and order cost. Window partition was the only way to do it. Great question.,2021-08-15T20:11:08Z
UgwBgBTWklUpRgb_kF54AaABAg,@sophiema1433,,1,EPUayNC5ku4,1,0,2021-08-14T22:19:54Z,"Just thinking, what if there are customers with the same first name, is it better to partition by customer id?",2021-08-14T22:19:54Z
UgwBgBTWklUpRgb_kF54AaABAg.9R246QtDtAC9R4OhIMiJfh,@stratascratch,UgwBgBTWklUpRgb_kF54AaABAg,2,EPUayNC5ku4,0,0,2021-08-15T19:58:19Z,"Yup it is better. I think in a later version of the question, I had an assumption that all names are unique. In reality, partitioning by id is the best practice.",2021-08-15T19:58:19Z
UgxNCh5vVMRdDXEPqVN4AaABAg,@okygy2125,,1,EPUayNC5ku4,0,2,2021-08-13T01:58:28Z,Very clean explanation. Short and sweet :-) Thanks a lot!,2021-08-13T01:58:28Z
UgzDk2EBV0uyOtO-7tJ4AaABAg,@sharanchakradhar,,1,EPUayNC5ku4,1,0,2021-08-11T13:50:10Z,Where can I find such questions more to crack ??,2021-08-11T13:50:10Z
UgzDk2EBV0uyOtO-7tJ4AaABAg.9QuROO8x9Z89Quf9A0Pzx9,@stratascratch,UgzDk2EBV0uyOtO-7tJ4AaABAg,2,EPUayNC5ku4,0,0,2021-08-11T15:59:10Z,There&#39;s over 600+ coding questions on StrataScratch!,2021-08-11T15:59:10Z
UgxdQyEsIyfvru8LDht4AaABAg,@anmolchawla616,,1,EPUayNC5ku4,0,0,2021-08-10T05:43:54Z,Thank you so much.,2021-08-10T05:43:54Z
Ugw3VcxBMHe9TFQSl794AaABAg,@mohit4902,,1,EPUayNC5ku4,1,0,2021-08-07T16:53:52Z,There are 2 kinds of people - 1) That use rank over partition by 2) That create a subquery,2021-08-07T16:53:52Z
Ugw3VcxBMHe9TFQSl794AaABAg.9QkTEMJD7O59Qkmw4gPam7,@stratascratch,Ugw3VcxBMHe9TFQSl794AaABAg,2,EPUayNC5ku4,0,0,2021-08-07T19:54:44Z,Ranking is sometimes faster so if someone asks you about query optimization...use ranking &gt; subquery =),2021-08-07T19:54:44Z
UgxZhu-IoX4ZveIi0Tp4AaABAg,@meghasyam427,,1,EPUayNC5ku4,0,0,2021-08-07T12:30:12Z,That&#39;s a cool function,2021-08-07T12:30:12Z
Ugzclxr-laJ3NOm2ZzZ4AaABAg,@nikilkvn,,1,EPUayNC5ku4,1,1,2021-07-30T16:37:00Z,"I am curious. why is that &#39;order_quantity&#39; not included. It would not change the relative percentages. But, definitely changes the percentage value individually.  The code when executed was a success by the system. can someone please explain. :( - Nice explanation BTW. Thanks Nate. :)",2021-07-30T16:37:00Z
Ugzclxr-laJ3NOm2ZzZ4AaABAg.9QQpwpnogRW9QRIjEvRK9C,@stratascratch,Ugzclxr-laJ3NOm2ZzZ4AaABAg,2,EPUayNC5ku4,0,1,2021-07-30T20:57:17Z,"Thanks for watching! Would you be able to ask this in the discussion forum in the platform? Someone from my team or myself will help to answer it! Sorry about that, it&#39;s easier to handle these questions if they&#39;re logged on our side!",2021-07-30T20:57:17Z
Ugx0t3sWJVaFy1F6xIZ4AaABAg,@AdvikSSC2023,,1,EPUayNC5ku4,0,1,2021-07-21T11:29:12Z,Really heart touching .<br>With few seconds you will soled this query with beter understanding,2021-07-21T11:29:12Z
UgzQwfbo_WBYwI7ACJp4AaABAg,@96merluzzo,,1,EPUayNC5ku4,3,0,2021-07-20T22:19:51Z,Why did I discover your channel just today? I have an interview tomorrow! :( :&#39;(,2021-07-20T22:19:51Z
UgzQwfbo_WBYwI7ACJp4AaABAg.9Q1hE7NeBpS9Q1i9R49CJA,@96merluzzo,UgzQwfbo_WBYwI7ACJp4AaABAg,2,EPUayNC5ku4,0,0,2021-07-20T22:27:56Z,Great content by the way,2021-07-20T22:27:56Z
UgzQwfbo_WBYwI7ACJp4AaABAg.9Q1hE7NeBpS9Q1khsUGdzn,@stratascratch,UgzQwfbo_WBYwI7ACJp4AaABAg,2,EPUayNC5ku4,0,1,2021-07-20T22:50:15Z,Good luck! You should binge watch all of this! Try out the platform too to get an idea to see if you&#39;re ready for the coding portion!,2021-07-20T22:50:15Z
UgzQwfbo_WBYwI7ACJp4AaABAg.9Q1hE7NeBpS9Q1lq_aa8gw,@96merluzzo,UgzQwfbo_WBYwI7ACJp4AaABAg,2,EPUayNC5ku4,0,0,2021-07-20T23:00:11Z,@@stratascratch Thanks,2021-07-20T23:00:11Z
UgzkQCy0vRMCb9ZYLmp4AaABAg,@saivarunkolluru,,1,EPUayNC5ku4,1,0,2021-07-18T06:52:07Z,"Which one is better and faster, group by solution or window func?",2021-07-18T06:52:07Z
UgzkQCy0vRMCb9ZYLmp4AaABAg.9PvtTURD64C9Py2KrzOi_y,@stratascratch,UgzkQCy0vRMCb9ZYLmp4AaABAg,2,EPUayNC5ku4,0,0,2021-07-19T02:56:48Z,Probably the group by I would imagine. The window function is a bit like a group by but it might be more complicated if you have functions and partitions in there and would take more time to execute compared to a simple group by.,2021-07-19T02:56:48Z
UgxdiQ3vIPqx4siQt8F4AaABAg,@yasoram8007,,1,EPUayNC5ku4,0,1,2021-07-14T20:13:16Z,Simple and excellent explanation Nate! . Thank you,2021-07-14T20:13:16Z
UgwtnDlwcJfDZX25CAx4AaABAg,@kli9005,,1,EPUayNC5ku4,0,0,2021-07-11T16:01:11Z,Nice explanation!!  Thank you.  However I was a little distracted by ‚ÄúeVa‚Äù.,2021-07-11T16:01:11Z
UgxDg7k3-h4YEzYQBUZ4AaABAg,@atharnayab3547,,1,EPUayNC5ku4,0,0,2021-07-11T07:43:38Z,why not use group by first_name?,2021-07-11T07:43:38Z
Ugw0zuTHRlWLOKQloXV4AaABAg,@vinay2237,,1,EPUayNC5ku4,1,0,2021-07-11T05:53:26Z,"Clear explanation but I used to think order by was mandatory, partition was optional when using over clause, so can you please explain in which scenario order by is mandatory when using the over clause?",2021-07-11T05:53:53Z
Ugw0zuTHRlWLOKQloXV4AaABAg.9PdlBZH7MKm9e-yJ_zj1IN,@localmartian9047,Ugw0zuTHRlWLOKQloXV4AaABAg,2,EPUayNC5ku4,0,0,2022-07-28T02:01:34Z,"Order by is mandatory when using ranking functions like rank dense_rank row_number lead lag, but for normal aggregation it can be skipped",2022-07-28T02:01:34Z
UgxQg13ZkmGhclX7C5x4AaABAg,@bachlam9841,,1,EPUayNC5ku4,0,0,2021-07-06T04:43:25Z,"Thank for your vid, I didn‚Äôt think about partitioning method, and intend to use sub query with group by id for calculate the percentage before selecting it",2021-07-06T04:43:25Z
Ugy-9AsPZ8R66VMHPD54AaABAg,@ashutoshanand4040,,1,EPUayNC5ku4,0,0,2021-06-25T14:31:03Z,You got a new subscriber üôÇ,2021-06-25T14:31:03Z
UgwphbcezNi_Lt1KrfZ4AaABAg,@manuelsojan9093,,1,EPUayNC5ku4,2,0,2021-06-20T00:21:56Z,"Hi Nate...I did it like this, but I&#39;m getting different values for the percentages for Eva:<br><br>SELECT first_name, order_details, ROUND((ORDER_COST/TOTAL_PER_CUSTOMER) * 100,0) AS Percent_of_total_spend FROM <br>(select *, SUM(order_cost) OVER(PARTITION BY cust_id) AS TOTAL_PER_CUSTOMER from orders <br>INNER JOIN CUSTOMERS<br>ON ORDERS.CUST_ID = CUSTOMERS.ID) AS D1<br>ORDER BY First_name",2021-06-20T00:21:56Z
UgwXZKc2-Wlni7MN7lJ4AaABAg,@VloggingMemories,,1,IKw0lkmBHNI,0,0,2023-03-15T10:21:08Z,"My Solution:<br>select distinct from_user, nationality<br>from <br>(select *,<br>dense_rank() over (partition by from_user order by review_score desc) rank<br>from airbnb_reviews<br>where from_type=&#39;guest&#39;)t1<br>join airbnb_hosts t2 on t1.to_user=t2.host_id<br>where rank=1<br>order by 1;",2023-03-15T10:21:08Z
UgxuPq6j60kwAY8jDuV4AaABAg,@swayankashanu4542,,1,IKw0lkmBHNI,0,0,2022-10-01T12:24:16Z,"Hi,<br>I do not have the premium feature to check whether my output is correct. Can anyone suggest my code is corrector not?  Here&#39;s my code :<br>with summary_data as <br>(<br>select <br>    ar.from_user , <br>    ah.host_id,<br>    ar.review_score,<br>    ah.nationality,<br>    row_number() over (partition by ar.from_user order by ar.review_score desc) as rnk<br>from <br>    airbnb_reviews ar inner join airbnb_hosts ah<br>on ar.to_user = ah.host_id<br>where ar.from_type =&#39;guest&#39;<br>)<br>select <br>    distinct from_user , nationality<br>from summary_data where rnk=1 ;",2022-10-01T12:24:16Z
UgxXlOmgK3wmiOmQSFR4AaABAg,@mohammadshahfaishal2747,,1,IKw0lkmBHNI,1,1,2022-07-30T23:10:33Z,Why are you not doing it by window function max(rating) over (partition by  guess_id)<br>And then put a condition where max_rating = rating,2022-07-30T23:10:33Z
UgxXlOmgK3wmiOmQSFR4AaABAg.9e7O7fZdNlZ9ff-9C-u2Dj,@stratascratch,UgxXlOmgK3wmiOmQSFR4AaABAg,2,IKw0lkmBHNI,0,0,2022-09-07T06:17:07Z,"Because you can&#39;t use the aliases in the WHERE or HAVING clause. And if you tried to use the whole window function expression, it still wouldn&#39;t work. The window functions are not allowed in WHERE and HAVING.",2022-09-07T06:17:07Z
Ugwu3brwxwfQp4PsNFd4AaABAg,@hassamkafeel,,1,IKw0lkmBHNI,0,0,2022-04-16T23:03:35Z,"with a as (select from_user, review_score, h.nat as natt, to_user,<br>dense_rank() over (partition by from_user order by review_score desc) as rank_ed<br>from airbnb_reviews<br>inner join <br>(select distinct(host_id) as id, nationality as nat from airbnb_hosts) as h<br>on <a href=""http://h.id/"">h.id</a> = to_user<br>where from_type = &#39;guest&#39; and to_type = &#39;host&#39;)<br>select distinct(natt),from_user, rank_ed, review_score from a <br>where rank_ed = 1<br>order by 2",2022-04-16T23:03:35Z
UgxNAILs3gGYdIXWwM54AaABAg,@shobhamourya8396,,1,IKw0lkmBHNI,4,5,2021-10-20T04:45:57Z,"Here&#39;s my solution using CTE<br><br>with cte1<br>as<br>(<br>    select *,<br>        dense_rank() over(partition by from_user order by review_score desc) host_rank<br>    from airbnb_reviews<br>    where from_type = &#39;guest&#39; and<br>      to_type = &#39;host&#39;<br>)<br>select distinct from_user, h.nationality fav_host_nationality<br>from cte1 c,<br>     airbnb_hosts h<br>where c.host_rank = 1 and c.to_user = h.host_id<br>order by from_user",2021-10-20T04:45:57Z
UgxNAILs3gGYdIXWwM54AaABAg.9Thhkhtl8Yj9YFcfWebYBP,@elohim58,UgxNAILs3gGYdIXWwM54AaABAg,2,IKw0lkmBHNI,0,0,2022-02-10T04:20:33Z,+1 to above on both readability and run time efficiency,2022-02-10T04:20:33Z
UgxNAILs3gGYdIXWwM54AaABAg.9Thhkhtl8Yj9YFe9CvtJok,@shobhamourya8396,UgxNAILs3gGYdIXWwM54AaABAg,2,IKw0lkmBHNI,0,0,2022-02-10T04:33:29Z,@@elohim58 Thank you!,2022-02-10T04:33:29Z
UgxNAILs3gGYdIXWwM54AaABAg.9Thhkhtl8Yj9_dh2nPgmHm,@anggipermanaharianja6122,UgxNAILs3gGYdIXWwM54AaABAg,2,IKw0lkmBHNI,0,0,2022-04-10T15:02:37Z,"you are indeed correct, your SQL could be even easier to read like this one below:<br><br>with cte as (select ar.from_user as user_id,<br>    ar.to_user, <br>    ar.review_score, <br>    ah.nationality as nationality,<br>    rank() over(partition by ar.from_user order by ar.review_score desc) as rank_<br>from airbnb_reviews as ar<br>left join airbnb_hosts as ah<br>    on ar.to_user = ah.host_id<br>where from_type=&#39;guest&#39;)<br>    <br>select  distinct user_id, nationality from cte where rank_ = 1",2022-04-10T15:02:37Z
UgxNAILs3gGYdIXWwM54AaABAg.9Thhkhtl8Yj9_u04uetBmz,@hassamkafeel,UgxNAILs3gGYdIXWwM54AaABAg,2,IKw0lkmBHNI,0,0,2022-04-16T23:05:31Z,"@@anggipermanaharianja6122 with a as (select from_user, review_score, h.nat as natt, to_user,<br>dense_rank() over (partition by from_user order by review_score desc) as rank_ed<br>from airbnb_reviews<br>inner join <br>(select distinct(host_id) as id, nationality as nat from airbnb_hosts) as h<br>on <a href=""http://h.id/"">h.id</a> = to_user<br>where from_type = &#39;guest&#39; and to_type = &#39;host&#39;)<br>select distinct(natt),from_user, rank_ed, review_score from a <br>where rank_ed = 1<br>order by 2",2022-04-16T23:05:31Z
UgxP4TNySg24e4GjoFt4AaABAg,@miradizrakhmatov2159,,1,IKw0lkmBHNI,1,9,2021-09-26T06:27:07Z,"Thank you Nate, you have a great platform<br><br>1)Using correlated subquery:<br><br>SELECT DISTINCT a.from_user, h.nationality<br>FROM airbnb_reviews a <br>JOIN airbnb_hosts h ON a.to_user = h.host_id<br><br>WHERE a.from_type = &#39;guest&#39; <br>AND a.review_score = (SELECT MAX(b.review_score)<br>                        FROM airbnb_reviews b<br>                            WHERE   a.from_user = b.from_user)<br><br>2) Using window function:<br><br>SELECT DISTINCT a.from_user, b.nationality<br>FROM<br>(SELECT from_user, to_user, review_score,<br>DENSE_RANK() OVER(PARTITION BY from_user ORDER BY review_score DESC) rk<br>FROM airbnb_reviews<br>WHERE from_type = &#39;guest&#39;) a<br>JOIN airbnb_hosts b ON a.to_user = b.host_id<br>WHERE rk = 1",2021-09-26T06:28:04Z
UgxP4TNySg24e4GjoFt4AaABAg.9Sk5FehNXsR9voHY8_KMer,@AjayBhaskar,UgxP4TNySg24e4GjoFt4AaABAg,2,IKw0lkmBHNI,0,0,2023-10-13T13:13:17Z,"Hi, I have a question. Shouldn‚Äôt the John in the correlated us query be a left join ? If you do inner join then you are reducing the number of users who have fav hosts ? There could be some missed users and nationality ?",2023-10-13T13:13:17Z
UgytBO6YtAG_0YyPSQp4AaABAg,@emadhussain1776,,1,IKw0lkmBHNI,1,0,2021-08-28T06:49:40Z,"I observed one anomaly in the data where host and guest user id is same in airbnb_reviews table, which means host himself acted as a guest and gave a max or higher rating. We should remove this in filter where from_user and to_user is not equal. Please comment",2021-08-28T06:50:33Z
UgytBO6YtAG_0YyPSQp4AaABAg.9R_SmNKw5XO9Raszbj9YSj,@stratascratch,UgytBO6YtAG_0YyPSQp4AaABAg,2,IKw0lkmBHNI,0,0,2021-08-28T20:06:38Z,Yea you&#39;re right. I didn&#39;t even notice that. Thanks!,2021-08-28T20:06:38Z
UgzQuiimDMQTtIBAZM54AaABAg,@zarynooi5669,,1,IKw0lkmBHNI,0,0,2021-07-29T07:49:01Z,Appreciate content like this !!,2021-07-29T07:49:01Z
UgylpCuoTC0pOplSKiZ4AaABAg,@miraskhabibulla1345,,1,IKw0lkmBHNI,0,0,2021-07-18T11:29:06Z,"beautiful!!! Nate, you are awesome!",2021-07-18T11:29:06Z
UgxjP0M5jJPYaDddG0F4AaABAg,@umakanta7,,1,IKw0lkmBHNI,0,1,2021-07-18T04:19:23Z,This is again another piece of mind blowing example..keep up the good work.,2021-07-18T04:19:23Z
Ugyx5kxJvLQkvEd4Q6x4AaABAg,@RaviSingh-xx2wq,,1,IKw0lkmBHNI,1,1,2021-06-19T13:11:24Z,"I feel my solution is a bit longer üòÇ <br><br>SELECT b.from_user,<br>       b.nationality<br>FROM   (SELECT from_user,<br>               Dense_rank()<br>                 OVER(<br>                   partition BY from_user<br>                   ORDER BY score DESC) AS rnk,<br>               score,<br>               nationality<br>        FROM   (SELECT from_user,<br>                       Max(review_score) AS score,<br>                       nationality<br>                FROM   airbnb_reviews r<br>                       INNER JOIN airbnb_hosts h<br>                               ON r.to_user = h.host_id<br>                WHERE  from_type = &#39;guest&#39;<br>                GROUP  BY 1,<br>                          3) a)b<br>WHERE  rnk = 1",2021-06-19T13:11:24Z
Ugyx5kxJvLQkvEd4Q6x4AaABAg.9Oltp22-klR9Omatjv00UL,@stratascratch,Ugyx5kxJvLQkvEd4Q6x4AaABAg,2,IKw0lkmBHNI,0,0,2021-06-19T19:45:15Z,Still looks good to me!,2021-06-19T19:45:15Z
UgxWdXgfTY0oFKVxRMt4AaABAg,@vigneshiyer2424,,1,IKw0lkmBHNI,2,0,2021-02-22T17:11:00Z,Great question! Any alternative methods to solve this?,2021-02-22T17:11:00Z
UgxWdXgfTY0oFKVxRMt4AaABAg.9K43E_1dZV39K4_lRZ0HfG,@stratascratch,UgxWdXgfTY0oFKVxRMt4AaABAg,2,IKw0lkmBHNI,0,0,2021-02-22T22:03:59Z,"Yeah, probably a better method is to use a CASE statement. It&#39;ll make the code cleaner. Try it in the link to the question in the description. Thanks for watching!",2021-02-22T22:03:59Z
UgxWdXgfTY0oFKVxRMt4AaABAg.9K43E_1dZV39OltvPd1tVM,@RaviSingh-xx2wq,UgxWdXgfTY0oFKVxRMt4AaABAg,2,IKw0lkmBHNI,0,0,2021-06-19T13:12:16Z,"You could try my solution but I think Nate&#39;s is more efficient:<br><br>SELECT b.from_user,<br>       b.nationality<br>FROM   (SELECT from_user,<br>               Dense_rank()<br>                 OVER(<br>                   partition BY from_user<br>                   ORDER BY score DESC) AS rnk,<br>               score,<br>               nationality<br>        FROM   (SELECT from_user,<br>                       Max(review_score) AS score,<br>                       nationality<br>                FROM   airbnb_reviews r<br>                       INNER JOIN airbnb_hosts h<br>                               ON r.to_user = h.host_id<br>                WHERE  from_type = &#39;guest&#39;<br>                GROUP  BY 1,<br>                          3) a)b<br>WHERE  rnk = 1",2021-06-19T13:12:16Z
UgyCGWY8Rxf3gN7hOTV4AaABAg,@saraali-ws9uv,,1,IKw0lkmBHNI,1,1,2020-11-27T15:16:46Z,Your question selection is quite good..!! Keep going..!! I have a suggestion though...To keep the whole thing interesting you can maybe give a similar question as take-home question and answer it in the next video..! What say..? Good work..!!üëèüëè,2020-11-27T15:16:46Z
UgyCGWY8Rxf3gN7hOTV4AaABAg.9GZq-j-cxCk9GZxfsdLl-Z,@stratascratch,UgyCGWY8Rxf3gN7hOTV4AaABAg,2,IKw0lkmBHNI,0,3,2020-11-27T16:23:49Z,"Thanks for watching. Yes, I can certainly do that. I can give a take-home question to do on your own and also give a preview of the next question I&#39;ll cover in the series. It won&#39;t work for every week since I change topics from time to time but for the weeks where I do 2 SQL questions in a row, I can pass on more questions. Thanks for the suggestion.",2020-11-27T16:23:49Z
Ugw0DgsQJbCSuw65brB4AaABAg,@faisalbargi5903,,1,_gy1o9UH2dQ,0,0,2023-04-04T18:25:15Z,"my solution<br>with cte as <br>    (select  user1,user2<br>    from facebook_friends<br>    union<br>    select  user2,user1<br>    from facebook_friends),<br>cte2 as <br>    (select distinct user1,count(user2) over(partition by user1) as total_friends<br>    from cte)<br>select user1,total_friends/9.0*100 as perc_pop<br>from cte2;",2023-04-04T18:25:15Z
UgyOQRlZ8ZCfF_pHnk54AaABAg,@VloggingMemories,,1,_gy1o9UH2dQ,0,0,2023-03-28T18:47:23Z,"My Solution:<br>with total_population(pop_cnt) as<br>(select count(user)<br>from<br>(select distinct user1 as user from facebook_friends<br>union <br>select distinct user2 as user from facebook_friends)x),<br>total_pairs as<br>(select distinct user1 as frnd1, user2 as frnd2 from facebook_friends<br>union<br>select distinct user2 as frnd1, user1 as frnd2 from facebook_friends)<br>select frnd1, <br>count(distinct frnd2)/max(pop_cnt)::float*100 as popularity_percentage<br>from total_pairs, total_population<br>group by 1;",2023-03-28T18:47:23Z
Ugxw4r77HxC1B6N-Q_14AaABAg,@gagansingh3481,,1,_gy1o9UH2dQ,1,1,2022-08-29T02:11:34Z,Awesome dear you did a great explanation kindly make more videos like this üòÄ,2022-08-29T02:11:34Z
Ugxw4r77HxC1B6N-Q_14AaABAg.9fINtnhHK8h9fK5uoxK2NT,@stratascratch,Ugxw4r77HxC1B6N-Q_14AaABAg,2,_gy1o9UH2dQ,0,0,2022-08-29T18:12:54Z,Thank you.  I am glad you find it very helpful.,2022-08-29T18:12:54Z
Ugznwvk5pO6GfgJnH6l4AaABAg,@dwaipayansaha4443,,1,_gy1o9UH2dQ,0,1,2022-08-21T09:59:50Z,"This is one of the best union combined with cte problem I have come across<br>My solution:-<br>with t1 as (select * from facebook_friends<br>union all<br>select user2,user1 from facebook_friends),<br>t2 as (select user1, count(user2) no_friends from t1<br>group by user1<br>order by user1),<br>t3 as (select user1, no_friends,count(*) over() total from t2)<br>select user1,(no_friends/total)*100.0 pop_per from t3",2022-08-21T09:59:50Z
Ugz_xWDu6knaemO1zap4AaABAg,@Hotobu,,1,_gy1o9UH2dQ,1,0,2022-07-27T02:34:21Z,This is a &quot;hard&quot; question? This is SUPER easy!,2022-07-27T02:34:21Z
Ugz_xWDu6knaemO1zap4AaABAg.9dySGk8Nt5s9e1PO1cuZ2X,@stratascratch,Ugz_xWDu6knaemO1zap4AaABAg,2,_gy1o9UH2dQ,0,0,2022-07-28T15:26:05Z,You are a cut above the rest.,2022-07-28T15:26:05Z
Ugy_4XiX31RnGhUrC9h4AaABAg,@danielxing1034,,1,_gy1o9UH2dQ,0,0,2022-06-09T16:19:29Z,Great walkthrough! But still think the number of friend of each user should not be computed using union because A friends with B does not mean B friends with A. Those who only show up in the second column should be considered having no friends.,2022-06-09T16:19:29Z
UgypXHREzwCZT6qv8aR4AaABAg,@96satenik,,1,_gy1o9UH2dQ,0,0,2022-04-30T10:35:46Z,Can total number of users be defined as Max(users1) ?,2022-04-30T10:35:46Z
UgwAyacYPkaQ78PVq_t4AaABAg,@chandraprakash0211,,1,_gy1o9UH2dQ,1,0,2022-04-06T10:23:46Z,Hey Nate ! Can we use self join and not union for this problem ?,2022-04-06T10:24:26Z
UgwAyacYPkaQ78PVq_t4AaABAg.9_TtxxO3KLr9_UW5p5CPEe,@stratascratch,UgwAyacYPkaQ78PVq_t4AaABAg,2,_gy1o9UH2dQ,0,0,2022-04-06T16:05:46Z,Yes! There are many ways to solve the problem. You can even try your self join approach on the platfrom. There&#39;s a link in the description.,2022-04-06T16:05:46Z
UgwhpClChwy_ZsnGDQB4AaABAg,@derartudejene780,,1,_gy1o9UH2dQ,0,0,2022-03-24T21:01:03Z,"<a href=""https://youtu.be/JkzkrPnOp4w"">https://youtu.be/JkzkrPnOp4w</a>. SQL INTERVIEW",2022-03-24T21:01:03Z
Ugw_AGlue45iPCEbDxR4AaABAg,@SudhirKumar-rl4wt,,1,_gy1o9UH2dQ,0,0,2022-02-28T07:28:37Z,I am not sure if we need to use DISTINCT if we are using union already,2022-02-28T07:28:37Z
UgxAN18qIjZE82xfGpl4AaABAg,@finvestomate,,1,_gy1o9UH2dQ,0,0,2022-02-23T07:35:38Z,"Thank you for the insights. I also tried doing it in similar manner, with a shorter query<br><br>select user1, Round((Friends/max(users_id) over())*100,3) as PP <br>from<br>(select *, <br>    count(user1) as Friends, <br>    row_number() over(order by user1) as users_id from <br>        (select * <br>            from facebook_friends<br>            UNION ALL<br>            select user2,user1 from facebook_friends) as A <br>    group by user1 <br>    order by user1) as B;",2022-02-23T07:35:38Z
UgyoL7s9RWoyMLQDvZl4AaABAg,@CalconUs2,,1,_gy1o9UH2dQ,0,0,2022-02-21T05:40:21Z,"Here&#39;s my own spin on this. I like the Union that was used, so I kept it, followed by CTE and subquery. Did not use window function. I checked it with the website and it was correct :) <br><br>WITH<br>allFriends AS<br>(<br>  SELECT<br>   user1, user2<br>  FROM facebook_friends<br>  UNION<br>  SELECT<br>   user2 AS user1, user1 AS user2<br>  FROM facebook_friends<br>)<br>SELECT<br> user1,<br> (num_friends/total_users) * 100 AS popularity_percentage<br>FROM<br>(<br>  SELECT<br>   user1,<br>   CAST(COUNT(*) AS float) AS num_friends,<br>   CAST((SELECT COUNT(DISTINCT user1) FROM allFriends) AS float) AS total_users<br>  FROM allFriends<br>  GROUP BY user1<br>) a<br>ORDER BY user1 ASC",2022-02-21T05:40:21Z
UgyKVPIdti1YE23rOCp4AaABAg,@thepogchamp7783,,1,_gy1o9UH2dQ,1,1,2022-02-20T14:37:06Z,"Hey Nate! Thanks for the video - they are really insightful, and helps a lot  when I do the questions! Quick question - is there a reason to join the tuu value? What I did was just to put it as the denominator in the select statement, which I thought should save some time since you don&#39;t have to join. Thanks!",2022-02-20T14:37:06Z
UgyKVPIdti1YE23rOCp4AaABAg.9YfUB26qQNJ9Yia1AupkML,@stratascratch,UgyKVPIdti1YE23rOCp4AaABAg,2,_gy1o9UH2dQ,0,0,2022-02-21T19:34:39Z,"I think it was just to ensure that I&#39;ve captured all users since there is a user1 and user2. I would validate your approach with the platform. We capture most use cases/edge cases on the platform, so if your thinking is right, your solution should validate correctly.",2022-02-21T19:34:39Z
UgwvQx2Ngy-Jd84pPLp4AaABAg,@caiyu538,,1,_gy1o9UH2dQ,0,0,2022-01-27T05:07:39Z,"Great solution,  without your explanation, it is hard to figure it out. Thank you.",2022-01-27T05:07:39Z
UgwK8Y-jrKtuD8abmkh4AaABAg,@avanichheda,,1,_gy1o9UH2dQ,0,2,2022-01-25T04:11:34Z,Great video and content! I was able to use count(user2) OVER (Partition by user1) after doing the union to get to the friends of the users. <br>Was able to confirm by submitting on the platform too :),2022-01-25T04:11:34Z
UgwhCRAfBP-IqbIjRhJ4AaABAg,@TheRaju991,,1,_gy1o9UH2dQ,0,0,2022-01-19T19:26:15Z,This was uniquely tough to think through.,2022-01-19T19:26:15Z
UgwvKs6KClCBLB9cUKV4AaABAg,@StanleySI,,1,_gy1o9UH2dQ,1,0,2022-01-10T22:10:59Z,"I found this platform is very helpful because it allows me to run code and let me see result step by step. In this way, I can better understand what I&#39;m doing and where I got wrong.",2022-01-10T22:10:59Z
UgwvKs6KClCBLB9cUKV4AaABAg.9X1iXPfjBUF9X2DPZBSCPz,@stratascratch,UgwvKs6KClCBLB9cUKV4AaABAg,2,_gy1o9UH2dQ,0,0,2022-01-11T02:49:31Z,That&#39;s for a great testimonial =),2022-01-11T02:49:31Z
UgzuL-bZvR41trfsN3l4AaABAg,@gourd1990,,1,_gy1o9UH2dQ,0,0,2021-12-17T23:03:42Z,"I solved this like this:<br><br>with all_users as(<br>    select user1 &quot;user&quot;<br>    from facebook_friends<br>    union <br>    select user2<br>    from facebook_friends<br>    order by 1<br>)<br>select all_users.user, l.count / (select count(*) from all_users)::float *100<br>from all_users<br>cross join lateral(<br>    select count(*) from facebook_friends<br>    where all_users.user = facebook_friends.user1 or all_users.user = facebook_friends.user2<br>) l",2021-12-17T23:03:42Z
UgwMeXZPeNW9ZRJ7-kx4AaABAg,@joaopedroreissilva7075,,1,_gy1o9UH2dQ,0,0,2021-11-01T21:56:41Z,"Thank you, Nate.<br>Really good approach.",2021-11-01T21:56:41Z
UgysrEBgs27fk0-2XWJ4AaABAg,@poopah4497,,1,_gy1o9UH2dQ,1,1,2021-10-24T18:40:33Z,"Hey Nate , I must over-simplify it. Why I cannot use a sub query to calculate total user and window function to calculate friends per user.<br>Select user1, (select count(distinct user2) from xx) as totalususer, count(distinct user2 ) over (partition by user1) from ‚Ä¶",2021-10-24T18:40:33Z
UgysrEBgs27fk0-2XWJ4AaABAg.9TtVRzF5Xjy9TtqJbpMNt5,@stratascratch,UgysrEBgs27fk0-2XWJ4AaABAg,2,_gy1o9UH2dQ,0,0,2021-10-24T21:51:39Z,You can probably use a subquery to solve it. Give it a try on the platform to see if you get the output you want,2021-10-24T21:51:39Z
Ugy8sQuKe-c6k8Ieaad4AaABAg,@iceindia007,,1,_gy1o9UH2dQ,0,0,2021-10-01T02:42:11Z,"u can simply use something like this:<br>select a.* , a.i/(sum(a.i) over()) as aasd<br>from (<br><br>select requester_id as id, count(accepter_id ) as i from request_accepted<br>group by 1<br><br>union<br>select accepter_id  as id , count(requester_id) as i from request_accepted<br><br>group by 1<br>) a<br><br>order by 3 desc",2021-10-01T02:42:11Z
UgyHZfUOZuK3Ic_BOAp4AaABAg,@okygy2125,,1,_gy1o9UH2dQ,0,0,2021-09-21T07:12:58Z,Excellent!,2021-09-21T07:12:58Z
UgzyeN9Y4mN2OqQB34l4AaABAg,@tanvimehta7960,,1,_gy1o9UH2dQ,0,25,2021-09-07T17:25:41Z,"That&#39;s insightful. Using a CTE helped me shorten the query. Also when using UNION, you get rid of duplicates with the individual tables as well. So using distinct is not required.<br><br>with total as (<br>    select user1, user2 from facebook_friends<br>    union<br>    select user2, user1 from facebook_friends)<br><br>select distinct user1, cast(count(user2) over (partition by user1) as float)*100/(select count(distinct user1) from total) from total<br>order by user1;",2021-09-07T17:25:41Z
UgyVsEiyHo22VESyTqF4AaABAg,@radhikashroff2643,,1,_gy1o9UH2dQ,2,1,2021-08-22T21:10:58Z,"Hi Nate, <br><br>To calculate number of friends each user has, can we do this ?<br><br>SELECT user1, COUNT(user2)<br>FROM facebookfriends<br>GROUP BY user1 <br><br>so this would group all the friends of each user in user1 since right column are the friends right ?",2021-08-22T21:10:58Z
UgyVsEiyHo22VESyTqF4AaABAg.9RMY_Uj_acp9RNAqj_Vr8m,@stratascratch,UgyVsEiyHo22VESyTqF4AaABAg,2,_gy1o9UH2dQ,0,0,2021-08-23T03:02:50Z,"Yes, so long as all users are found in the user1 column. But our assumption is that not all users are found in user1. So the question is slightly more complicated.",2021-08-23T03:02:50Z
UgyVsEiyHo22VESyTqF4AaABAg.9RMY_Uj_acp9cRCmffclwc,@GouthamSudini,UgyVsEiyHo22VESyTqF4AaABAg,2,_gy1o9UH2dQ,0,1,2022-06-18T22:53:27Z,@@stratascratch wouldn&#39;t user1 carry all users based on the table structure? How can a user2 be a friend without being a user in 1st place?,2022-06-18T22:53:27Z
UgxyblMwtKq-jnJLoIV4AaABAg,@vijayjayaraman5990,,1,_gy1o9UH2dQ,1,1,2021-08-11T03:48:43Z,"While I agree with the solution, why wouldn&#39;t Bob be a user1 in the original table? That doesn&#39;t sound intuitive at all.. I would have done the SELECT user1, count(distinct user2) right away because user1 was supposed to have all the users on Facebook..",2021-08-11T03:48:43Z
UgxyblMwtKq-jnJLoIV4AaABAg.9QtMZFFnBgz9QvQAXm1_Te,@stratascratch,UgxyblMwtKq-jnJLoIV4AaABAg,2,_gy1o9UH2dQ,0,1,2021-08-11T22:58:47Z,"I didn&#39;t have the assumption that user1 should have all the users on FB. I could make that case if it was a user table (like a copy from production) but if it&#39;s a mapping table or some other type of table used for analytics, I wouldn&#39;t feel comfortable making that assumption without asking the interviewer first (or checking it if I was at work).",2021-08-11T22:58:47Z
UgzIZW4BQuvikUfy8cd4AaABAg,@nanditha4477,,1,_gy1o9UH2dQ,0,0,2021-08-10T19:33:28Z,Hi Nate ..Can u please have a Sql session once. Can u please ping me if thats possible,2021-08-10T19:33:28Z
UgyIPO1_-9yBIyErdRZ4AaABAg,@meghasyam427,,1,_gy1o9UH2dQ,2,2,2021-08-07T13:24:57Z,"Tricky question but in reality all the users even in the user2 column should be present in the user1 column too. This is the basis of a user table. This question is all about what if all the users are not present in user1 column, then your code is the ultimate solution.<br>Hatsoff to your thinking which considers the worst case i didn&#39;t expect.",2021-08-07T13:26:11Z
UgyIPO1_-9yBIyErdRZ4AaABAg.9Qk5KB094Hw9QkmpkzgkRh,@stratascratch,UgyIPO1_-9yBIyErdRZ4AaABAg,2,_gy1o9UH2dQ,0,2,2021-08-07T19:53:52Z,I agree that if this was a user table then you&#39;d have all the users in the user1 column. But what if this was a mapping table? That assumption might not be valid. It&#39;s always good to ask the interview to clarify any assumptions. I always assume the worse case =) Thanks for watching and for the great detailed comment.,2021-08-07T19:53:52Z
UgyIPO1_-9yBIyErdRZ4AaABAg.9Qk5KB094Hw9jRGL7yNLA6,@shyamsundark1556,UgyIPO1_-9yBIyErdRZ4AaABAg,2,_gy1o9UH2dQ,0,0,2022-12-09T23:04:16Z,Exactly ! the only catch in this Q is the fact that they dont explicitly state the structure of the data.,2022-12-09T23:04:16Z
Ugxr1nqbVnZ1qgCIjEV4AaABAg,@ahmedshaalan339,,1,_gy1o9UH2dQ,0,0,2021-08-03T09:20:50Z,"with dim_user as<br>(<br>select user1 from facebook_friends<br>union<br>select user2 from facebook_friends<br>),<br>cte as<br>(<br>select user1, (sum(friends)/(select Count(user1) from dim_user) ) *100<br>from (<br>select user1, count(user2) friends from facebook_friends <br>group by user1<br>union<br>select user2, count(user1) from facebook_friends <br>group by user2<br>)tmp<br>group by user1<br>)<br>select * from  cte<br>order by 1",2021-08-03T09:20:50Z
UgzdNXvUnN5BHYFHSUp4AaABAg,@wenlizhang5283,,1,_gy1o9UH2dQ,1,0,2021-06-23T06:51:32Z,"Hi, why do you use MAX() to get all _users number? Can we just divide by tuu.all_users?",2021-06-23T06:51:32Z
UgzdNXvUnN5BHYFHSUp4AaABAg.9OvWXHmU65G9Owl-H2XDjV,@stratascratch,UgzdNXvUnN5BHYFHSUp4AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-23T18:25:56Z,"Actually, yes you could just use all_users since it&#39;s the count() and you&#39;ll just be given 1 number anyways. good catch.",2021-06-23T18:25:56Z
Ugw55i4T37TwHbqLLOt4AaABAg,@philintheblank4360,,1,_gy1o9UH2dQ,1,1,2021-06-20T16:30:31Z,"it seems there&#39;s an assumtption that person a can be friends with person b, but person b is not friends with person a... I don&#39;t think that&#39;s how this platform or friendship works.  Clarifying a simplifying assumption with the interviewer would streamline a lot of this.",2021-06-20T16:30:31Z
Ugw55i4T37TwHbqLLOt4AaABAg.9OopPLYcVR99OpIt9hHhMZ,@stratascratch,Ugw55i4T37TwHbqLLOt4AaABAg,2,_gy1o9UH2dQ,0,1,2021-06-20T20:56:52Z,Yea you&#39;re right. There are some assumptions that are needed to solve the question the right way. And you should talk to the interviewer to get the assumptions clarified. Even the question here has changed a bit since I created this video. Thanks for pointing this out.,2021-06-20T20:56:52Z
UgxZiqmJbNEAG6BSiKt4AaABAg,@sandeepvenkatasairam9922,,1,_gy1o9UH2dQ,1,0,2021-06-17T10:47:37Z,"Thanks Nate for keeping us educated ! Just a logical Question I had , Since you are saying user1 is the user and user2 is their friend list , I hope even the friend should be a user on Platform , So can we cant consider select distinct user1 from facebook_friends as total users , Correct me if I am missing something .",2021-06-17T10:47:37Z
UgxZiqmJbNEAG6BSiKt4AaABAg.9OgUmEFfyhZ9Oh3klP0KB5,@stratascratch,UgxZiqmJbNEAG6BSiKt4AaABAg,2,_gy1o9UH2dQ,0,1,2021-06-17T16:10:44Z,That&#39;s a great assumption. That&#39;s something you&#39;d want to talk to the interviewer about to see if your assumption makes sense. Usually when it&#39;s captured on a db table then the user&#39;s friend should also be on the platform. Why is it that you can&#39;t consider distinct user1 from the table?,2021-06-17T16:10:44Z
UgxiolEJYXQZfRfZjbp4AaABAg,@InnocentDike,,1,_gy1o9UH2dQ,1,0,2021-06-14T19:41:08Z,Hi Nate at Strata Scratch pls can you RECOMMEND a platform where I can study ADVANCE SQL. Thanks in advance.,2021-06-14T19:41:08Z
UgxiolEJYXQZfRfZjbp4AaABAg.9O_iS7Wa8Pb9O_yddxbX1d,@stratascratch,UgxiolEJYXQZfRfZjbp4AaABAg,2,_gy1o9UH2dQ,0,3,2021-06-14T22:02:39Z,"For advanced level SQL, I would recommend leetcode, stratascratch, interviewquery, <a href=""http://sqlpad.io/"">sqlpad.io</a> as great platforms that will give you some advanced level skills. If you need something even more difficult then I would suggest doing real projects to improved. Real projects tend to have much longer sql scripts that are necessary to be written.",2021-06-14T22:02:39Z
UgyVCBu1CRpJb7YYoUF4AaABAg,@shobhamourya8396,,1,_gy1o9UH2dQ,2,0,2021-06-13T19:39:00Z,"Without Join<br>with cet1<br>as<br>(<br>  -- to select unique users from user1 and user2<br>  select count(*) max_users<br>  from (<br>            select user1 from facebook_friends<br>            UNION<br>            select user2 from facebook_friends<br>        ) a<br><br>),<br>cet2<br>as<br>(<br>-- To select unique friend pairs, i.e user1-user2 and user2-user1 pairs<br>    select user1 as fb_user, user2 as friend<br>    from facebook_friends<br>    UNION<br>    select user2 as fb_user , user1 as friend<br>    from facebook_friends<br>),<br>cet3<br>as<br>(<br>-- To count the number of friends for each user from user-friend pairs<br>    select fb_user , count(*) as num_friends <br>    from cet2 <br>    group by fb_user<br>    order by fb_user<br>)<br>select fb_user as user1, (num_friends*1.00/max_users*1.00)*100 as popularity_percent<br>from cet3 , cet1",2021-06-13T19:39:00Z
UgyVCBu1CRpJb7YYoUF4AaABAg.9OY8Pjlfleu9OY9-6syNu2,@stratascratch,UgyVCBu1CRpJb7YYoUF4AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-13T19:44:06Z,"That looks great! But isn‚Äôt a cet3, cet1 an implied JOIN?",2021-06-13T19:44:06Z
UgyVCBu1CRpJb7YYoUF4AaABAg.9OY8Pjlfleu9OY9z8yd4Xu,@shobhamourya8396,UgyVCBu1CRpJb7YYoUF4AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-13T19:52:43Z,@@stratascratch cet3 and cet1 don&#39;t have a common column,2021-06-13T19:52:43Z
UgzeUnEWSWEp8fTa8Rx4AaABAg,@akshaytalekar6751,,1,_gy1o9UH2dQ,5,0,2021-06-11T02:24:42Z,"Hey Nate, the Stratascratch website isn&#39;t working. Is it down?",2021-06-11T02:24:42Z
UgzeUnEWSWEp8fTa8Rx4AaABAg.9OR8SV4fXbO9OR9UMMfQ2X,@stratascratch,UgzeUnEWSWEp8fTa8Rx4AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-11T02:33:41Z,"No it&#39;s working! There&#39;s a bug right now that I&#39;ll fix soon. But you&#39;ll need to register/login before you can view the question. Again, I&#39;ll fix it soon but that&#39;s what you need to do to start coding on a question.",2021-06-11T02:33:41Z
UgzeUnEWSWEp8fTa8Rx4AaABAg.9OR8SV4fXbO9ORAO4F1tXn,@akshaytalekar6751,UgzeUnEWSWEp8fTa8Rx4AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-11T02:41:34Z,Thanks for the quick response! When I open a question it just keeps on loading and the question is never displayed,2021-06-11T02:41:34Z
UgzeUnEWSWEp8fTa8Rx4AaABAg.9OR8SV4fXbO9ORBEjzv5qs,@stratascratch,UgzeUnEWSWEp8fTa8Rx4AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-11T02:49:02Z,"@@akshaytalekar6751 email me at nate@<a href=""http://stratascratch.com/"">stratascratch.com</a> and I can help. Can you also try to clear your browser cache?",2021-06-11T02:49:02Z
UgzeUnEWSWEp8fTa8Rx4AaABAg.9OR8SV4fXbO9ORC8JEs8yj,@akshaytalekar6751,UgzeUnEWSWEp8fTa8Rx4AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-11T02:56:54Z,"Yes, I did try clearing the cache but no results. Just dropped you an email with a screenshot",2021-06-11T02:56:54Z
UgzeUnEWSWEp8fTa8Rx4AaABAg.9OR8SV4fXbO9ORH7BRPm2h,@stratascratch,UgzeUnEWSWEp8fTa8Rx4AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-11T03:40:26Z,@@akshaytalekar6751 Cool man...I responded to your email.,2021-06-11T03:40:26Z
Ugx_vf69RTh6LDEw7O14AaABAg,@shikharftc,,1,_gy1o9UH2dQ,1,0,2021-06-06T16:37:26Z,"I tried to do a similar thing without a join. The solutions works on the portal<br><br>with tb1 as(select count(*)<br>from(<br>select user1 from facebook_friends<br>union<br>select user2 from facebook_friends)a),<br>tb2 as(<br>select user1,user2 from facebook_friends<br>union <br>select user2 as user1, user1 as user2 from facebook_friends<br>)<br>select user1,<br>(count(*) / (select * from tb1)::float)*100 as popularity_percentage<br>from tb2<br>group by user1<br>order by user1",2021-06-06T16:37:26Z
Ugx_vf69RTh6LDEw7O14AaABAg.9OFn3hvZFCM9OJ5-YXNAPS,@stratascratch,Ugx_vf69RTh6LDEw7O14AaABAg,2,_gy1o9UH2dQ,0,0,2021-06-07T23:20:36Z,That&#39;s great! Thanks for letting us know about another approach.,2021-06-07T23:20:36Z
UgwpE4fzVcXDtBeNg9p4AaABAg,@saikatdasgupta2006,,1,_gy1o9UH2dQ,0,0,2021-06-01T07:42:38Z,"Another solution<br><br>select user1, (b.sum1/c.cnt)*100 as popularity_percent from<br>(select count(distinct user1) as cnt from (select distinct user1 from facebook_friends union select distinct user2 from facebook_friends) d) c,<br>(<br>select user1, sum(cnt) sum1 from<br>(select user1,count(*) cnt from facebook_friends group by user1<br>union<br>select user2,count(*) from facebook_friends group by user2) a<br>group by user1) b<br>order by user1",2021-06-01T07:42:38Z
UgxfUhyvkjj1QAos12B4AaABAg,@yashovardhan9841,,1,_gy1o9UH2dQ,0,1,2021-05-20T04:12:26Z,"My approach - <br><br>with temp as (<br>select user1 as u, count(*) as tot_friends from facebook_friends group by 1<br>union all<br>select user2 as u, count(*) as tot_friends from facebook_friends group by 1<br>)<br>select u, (sum(tot_friends)/(select count(distinct u) from temp)::float)*100 as popularity_percentage <br>from<br>temp<br>group by 1<br>order by 1",2021-05-20T04:12:26Z
UgzXAO1rK8k2kLO0jgF4AaABAg,@rithikgaur3830,,1,_gy1o9UH2dQ,1,0,2021-05-15T04:39:12Z,"Thank you so much for videos and stratscratch , i had general query , as in the above video unless we see tha data and if we only go with the schema mentioned we tend to believe that user 1 column has all users already , so in a real facebook or top company interviews do we actually have the data available for us or do we actually have to ask the interviewer on the limitations on data set .",2021-05-15T04:40:14Z
UgzXAO1rK8k2kLO0jgF4AaABAg.9NLrO0AoLil9NOH2m-epqB,@stratascratch,UgzXAO1rK8k2kLO0jgF4AaABAg,2,_gy1o9UH2dQ,0,1,2021-05-16T03:10:43Z,"In my experience, you won&#39;t have the data so you need to ask the interviewer what the data set looks like. Basically what are some of the limitations and/or example values. Then you can start to develop your solution and identify edge cases.",2021-05-16T03:10:43Z
Ugw7e4JYn1BnLz23h-N4AaABAg,@123admini2r,,1,_gy1o9UH2dQ,1,1,2021-05-12T11:07:28Z,"damn, understanding the data provided is the tough part",2021-05-12T11:07:28Z
Ugw7e4JYn1BnLz23h-N4AaABAg.9NEpR4V9oi19NFMVfAll4v,@stratascratch,Ugw7e4JYn1BnLz23h-N4AaABAg,2,_gy1o9UH2dQ,0,0,2021-05-12T16:05:11Z,"To me it&#39;s the data schema that&#39;s hard because there&#39;s so many assumptions you&#39;ll need to make based on the structure of the table. And then once you have an understanding of that, the values for each column can add complexity. So this part takes a lot of time when on an interview.",2021-05-12T16:05:11Z
UgzNNus_H7_z2-GkKnx4AaABAg,@celiahan3787,,1,_gy1o9UH2dQ,2,0,2021-04-10T23:41:47Z,since you are doing union to get all the users. So how do you think the user1 column did not contain all the users and we need to do the union?,2021-04-10T23:41:47Z
UgzNNus_H7_z2-GkKnx4AaABAg.9LymJqEanyM9M0fRrOIuhD,@stratascratch,UgzNNus_H7_z2-GkKnx4AaABAg,2,_gy1o9UH2dQ,0,2,2021-04-12T02:39:26Z,"I thought about edge cases where Friend A might be friends with Friend B, but Friend B might not be friends with A. Many platforms are like this (FB might not be one of them but I know IG is) so dong the UNION takes care of that. You can always ask the interviewer what they think. I don&#39;t think there&#39;s a right or wrong answer here since you have to assume certain things for you to end up coding a solution.",2021-04-12T02:39:26Z
UgzNNus_H7_z2-GkKnx4AaABAg.9LymJqEanyM9M0hLBA3TLF,@celiahan3787,UgzNNus_H7_z2-GkKnx4AaABAg,2,_gy1o9UH2dQ,0,0,2021-04-12T02:55:59Z,"@@stratascratch Thank you so much, I got you. You are talking about the followers on IG. Yes, if in that case, it makes more sense",2021-04-12T02:55:59Z
UgwIUX7qylj99pe0XTB4AaABAg,@MichaelBrown-wb6mo,,1,_gy1o9UH2dQ,1,0,2021-03-28T08:54:24Z,"How about:<br><br>SELECT ff.user1 AS [User]<br>, CONVERT(DECIMAL(10,2), COUNT(*) * 100.0 / (SELECT COUNT(*) FROM facebook_friends)) AS PopPercent<br><br><br><br>FROM facebook_friends AS ff<br><br><br><br>GROUP BY ff.user1<br><br><br><br>ORDER BY ff.user1 ASC",2021-03-28T08:54:24Z
UgwIUX7qylj99pe0XTB4AaABAg.9LQiR4Ez1Fp9LSccJPBNiZ,@stratascratch,UgwIUX7qylj99pe0XTB4AaABAg,2,_gy1o9UH2dQ,0,0,2021-03-29T02:42:07Z,Try it out on the platform and let me know if you get the same answer.,2021-03-29T02:42:07Z
UgxpFHFT32g3SpnRzOV4AaABAg,@hingemethod5938,,1,_gy1o9UH2dQ,1,0,2021-03-12T18:46:03Z,"Here is a much simpler solution using group by<br>select user_id, round(num_friends/(select count(*)::numeric from friends) * 100,2)||&#39;%&#39;<br>from<br>(<br>select distinct user_id, count(distinct user_friends) as num_friends<br>from friends<br>group by 1)a",2021-03-12T18:46:03Z
UgxpFHFT32g3SpnRzOV4AaABAg.9Kn_QWnhsVA9KnpUQAoEK-,@stratascratch,UgxpFHFT32g3SpnRzOV4AaABAg,2,_gy1o9UH2dQ,0,0,2021-03-12T21:06:24Z,Does that validate on the platform?,2021-03-12T21:06:24Z
UgxTTqg2TwafiVVksNV4AaABAg,@AmanSingh-od2ue,,1,_gy1o9UH2dQ,2,0,2021-02-06T21:04:43Z,"Hey Nate, <br>I do have concern from the top query where the FROM clause ends at total_unique_users. Could you please explain as to why it is ending there and why not at the end where the LEFT JOIN is completed?",2021-02-06T21:04:43Z
UgxTTqg2TwafiVVksNV4AaABAg.9JRHGPk863m9JRz0KiN4D9,@stratascratch,UgxTTqg2TwafiVVksNV4AaABAg,2,_gy1o9UH2dQ,0,0,2021-02-07T03:35:44Z,"It&#39;s because the total_unique_users is the 1st subquery and then I do another subquery called tuu that uses total_unique_users. I use tuu to join to user_friends. Once I join those 2 subqueries, I take the aggregate count. So the main reason why it ends at tuu is because I&#39;m doing a subquery within a subquery. Hope that makes sense. If you want to troubleshoot, I would try to see how the data is aggregated at each level of subquery. Then write more code and see how the data is aggregated again. Keep doing this until the entire query is built. That should help you understand what&#39;s going on.",2021-02-07T03:35:44Z
UgxTTqg2TwafiVVksNV4AaABAg.9JRHGPk863m9JWu_pFpbBi,@AmanSingh-od2ue,UgxTTqg2TwafiVVksNV4AaABAg,2,_gy1o9UH2dQ,0,0,2021-02-09T01:33:14Z,"@@stratascratch Thank you. I got your point, Nate.",2021-02-09T01:33:14Z
UgwqOoh7FQzkCX_bIg54AaABAg,@ghislaineamrani2775,,1,_gy1o9UH2dQ,1,2,2021-01-22T07:15:45Z,"Hi Nate, thank you !<br>This solution below seems to work also but I used  WITH instead of subqueries, is it an issue ? Thank you for your feedback !<br><br>WITH total_users as (<br>select distinct user as all_users from facebook_friends<br>union <br>select distinct friend from facebook_friends<br>),<br><br>consolidated_table as(<br>select all_users, count(user) as number_user, (select count(*) from total_users) as total_u<br>from total_users<br>cross join facebook_friends<br>where all_users = user<br>OR all_users = friend<br>group by 1<br>)<br><br>select all_users, (cast(number_user as float)/cast(total_u as float))*100 as percent from consolidated_table;",2021-01-22T07:15:45Z
UgwqOoh7FQzkCX_bIg54AaABAg.9IoATgJifNj9IpRdRcV2mz,@stratascratch,UgwqOoh7FQzkCX_bIg54AaABAg,2,_gy1o9UH2dQ,0,1,2021-01-22T19:05:00Z,"It&#39;s not at all an issue if you used a CTE (i.e., using a WITH) instead of a subquery. The reason why I used a subquery is because some SQL flavors don&#39;t allow using WITH so I wanted to write the solution so that the code can be used on any SQL flavor. Thanks for watching and more thanks for trying out this question!",2021-01-22T19:05:00Z
Ugy6M_-SukJykNffJDN4AaABAg,@ismafoot11,,1,_gy1o9UH2dQ,1,2,2021-01-12T22:02:17Z,if you use &quot;union&quot; you don&#39;t need distinct before user1 and user2. union will do that for you,2021-01-12T22:02:17Z
Ugy6M_-SukJykNffJDN4AaABAg.9IR-z9TpAKY9ISz8ATQUws,@stratascratch,Ugy6M_-SukJykNffJDN4AaABAg,2,_gy1o9UH2dQ,0,1,2021-01-13T16:24:39Z,Yup that&#39;s true! I just added the DISTINCT to be explicit.,2021-01-13T16:24:39Z
UgzSK-ogxnHjxOTHB1V4AaABAg,@kennag7469,,1,_gy1o9UH2dQ,1,0,2021-01-01T11:55:51Z,"What do you think about this solution my man<br><br>with cte as (<br>select *,count(user1) /count(distinct user(user1) *100 percentages<br>from users<br>group by user1 )<br>select percentages,*<br>from cte inner join users on cte.user1 = users.user1",2021-01-01T11:55:51Z
UgzSK-ogxnHjxOTHB1V4AaABAg.9Hyapo1q-IF9HzARpEr2q_,@stratascratch,UgzSK-ogxnHjxOTHB1V4AaABAg,2,_gy1o9UH2dQ,0,0,2021-01-01T17:15:45Z,Unfortunately it doesn&#39;t execute. Try going to the link in the description to execute the code and see if you get the same answer. Let me know if you need help!,2021-01-01T17:15:45Z
UgwegrYecohDIZxoTSt4AaABAg,@rashvinganesh,,1,_gy1o9UH2dQ,1,4,2020-12-27T09:52:54Z,Thank you so much Nate. Find it very easy to understand what‚Äôs the solution you‚Äôre taking. As a beginner in SQL this really helps me understand more clear.,2020-12-27T09:52:54Z
UgwegrYecohDIZxoTSt4AaABAg.9HlVmw42DrC9HmThGyeVTC,@stratascratch,UgwegrYecohDIZxoTSt4AaABAg,2,_gy1o9UH2dQ,0,3,2020-12-27T18:53:53Z,That&#39;s great. Hope you enjoy the entire series of SQL problems then. Let me know if you have any questions or feedback.,2020-12-27T18:53:53Z
UgwaJ_Lw2TRNUKWsx-x4AaABAg,@pedrol1000,,1,7Q-2DEwHW7w,0,0,2022-09-03T17:25:41Z,"Great analysis! Below is another way to do it with a small change in the filter:<br><br>SELECT COUNT(b.phone_number)::float / COUNT(a.phone_number) * 100 AS perc<br>FROM fb_sms_sends a<br>LEFT JOIN fb_confirmers b ON a.ds = <a href=""http://b.date/"">b.date</a><br>AND a.phone_number = b.phone_number<br>WHERE a.ds = &#39;08-04-2020&#39;<br>AND a.type = &#39;message&#39;;",2022-09-03T17:25:41Z
UgxcmC3JwygDIYtzmNF4AaABAg,@lingxu9697,,1,7Q-2DEwHW7w,0,1,2022-03-30T16:37:37Z,"I feel this questions is easy to code out, but hard to get your head around about the denominator and numerator...",2022-03-30T16:37:37Z
UgyCYGoNfMtBAevhYyN4AaABAg,@petertao287,,1,7Q-2DEwHW7w,2,0,2022-03-12T09:39:06Z,Thank you so much!¬†Great resource!<br>Would you please making some interview videos for potential data analysts?,2022-03-12T09:39:06Z
UgyCYGoNfMtBAevhYyN4AaABAg.9ZTRypsQ9dr9ZXD2uAB1vo,@stratascratch,UgyCYGoNfMtBAevhYyN4AaABAg,2,7Q-2DEwHW7w,0,0,2022-03-13T20:45:40Z,Yup! Most of these questions can be applied for data analysts also! i&#39;ll point out some specific DA questions in the future.,2022-03-13T20:45:40Z
UgyCYGoNfMtBAevhYyN4AaABAg.9ZTRypsQ9dr9_7duCecVMq,@petertao287,UgyCYGoNfMtBAevhYyN4AaABAg,2,7Q-2DEwHW7w,0,0,2022-03-28T19:00:09Z,@@stratascratch Thank you!,2022-03-28T19:00:09Z
UgwZ5ofirESo3tVLhYh4AaABAg,@aashi9781,,1,7Q-2DEwHW7w,1,0,2021-12-27T02:02:19Z,"I got confused with Invalid confirmation. I was thinking that there are mix of valid and invalid confirmation types. particularly this line -&#39;Confirmation texts are only valid on the date they were sent.&#39; (means message+valid confirmation(after join) will be valid) .This will give the result(11.14 %).I guess, I get confused(or overthink) with understanding the language of these question. How to avoid this trap! Mostly happens for many Facebook questions.",2021-12-27T02:02:19Z
UgwZ5ofirESo3tVLhYh4AaABAg.9WRW4cSHoJG9WTAwiwn7lS,@stratascratch,UgwZ5ofirESo3tVLhYh4AaABAg,2,7Q-2DEwHW7w,0,0,2021-12-27T17:36:04Z,That‚Äôs the hardest part of interviews! My advice is to always ask the interviewer for clarity on how you‚Äôre interpreting the question. Don‚Äôt write any code until it‚Äôs totally clear to you.,2021-12-27T17:36:04Z
UgzAUIaAh2CjayXtd_J4AaABAg,@Adam-mw3hu,,1,7Q-2DEwHW7w,2,0,2021-12-10T03:10:39Z,"Nate,<br><br>Why would you not include &quot;confirmation&quot; message type?  It seems like those are exactly the ones we&#39;re interested in.  Confirmation is the first step, the second step is for the user to confirm the confirmation.  Calculate the percentage where the confirmation is confirmed.<br><br>The other types of communication (&quot;message&quot; and &quot;friend request&quot;) would be something like &quot;you have a friend request&quot; or &quot;Janie posted on your wall&quot; and should be filtered out.",2021-12-10T03:10:39Z
UgzAUIaAh2CjayXtd_J4AaABAg.9VkrOH693ew9VkyV8sMaQp,@stratascratch,UgzAUIaAh2CjayXtd_J4AaABAg,2,7Q-2DEwHW7w,0,1,2021-12-10T04:12:45Z,Check the problem description. There&#39;s dirty data. The other table contains the real confirmations.,2021-12-10T04:12:45Z
UgzAUIaAh2CjayXtd_J4AaABAg.9VkrOH693ew9Zm9Weknkxi,@kaiyao5135,UgzAUIaAh2CjayXtd_J4AaABAg,2,7Q-2DEwHW7w,0,0,2022-03-20T01:22:38Z,"I have the same situation as Adam at the beginning that made me go with the incorrect solution. The truth is the real confirmed messages are not those marked as &#39; confirmation&#39; explicitly, and we need to link the tables and match the date and phone name to see which messages are the real confirmed messages.",2022-03-20T01:22:38Z
Ugy19xXUKKEVGfXsbeV4AaABAg,@hiovanycubillosgomez5901,,1,7Q-2DEwHW7w,0,1,2021-09-14T12:27:53Z,"Your initial analysis  before you to start to write t-sql is excellent, because sometimes we never reading well the statement, the besth teacher Nate, thank you so much for this.",2021-09-14T12:29:23Z
UgzRyZk0eQE8ia9eipl4AaABAg,@wenlizhang5283,,1,7Q-2DEwHW7w,1,3,2021-06-24T06:49:32Z,Problem like this always needs a long time to understand the meaning. Even though it is not hard at all,2021-06-24T06:49:32Z
UgzRyZk0eQE8ia9eipl4AaABAg.9Oy55_unvH19OzmOxyM-44,@stratascratch,UgzRyZk0eQE8ia9eipl4AaABAg,2,7Q-2DEwHW7w,0,0,2021-06-24T22:35:54Z,"There&#39;s so many assumptions that need to be aligned. If you don&#39;t get clarity about the assumptions, you could go down the wrong path. So definitely know what you mean. Thanks for your comment.",2021-06-24T22:35:54Z
UgzFsN2-Rg_eOA0ACGF4AaABAg,@kloveinn,,1,7Q-2DEwHW7w,1,0,2021-04-06T15:40:46Z,why did you filter out freind_request? shouldnt that be part of &quot;total texts&quot; sent on that day? Its a small thing but just curious.,2021-04-06T15:40:46Z
UgzFsN2-Rg_eOA0ACGF4AaABAg.9Lnc5IO9neH9Lnd2s6aCHR,@stratascratch,UgzFsN2-Rg_eOA0ACGF4AaABAg,2,7Q-2DEwHW7w,0,2,2021-04-06T15:49:10Z,In the problem statement it said that friend_requests and invalid confirmation text messages were inserted in the table on accident so that&#39;s why I filtered them out.,2021-04-06T15:49:10Z
UgzEJwTU3sJtIs9JzJp4AaABAg,@ranjitsingh8802,,1,7Q-2DEwHW7w,1,1,2021-03-13T23:52:31Z,Loved the initial break down of the problem before you started writing the query. Thanks.,2021-03-13T23:52:31Z
UgzEJwTU3sJtIs9JzJp4AaABAg.9KqhI2Pp3189Kr4NZHB3Fe,@stratascratch,UgzEJwTU3sJtIs9JzJp4AaABAg,2,7Q-2DEwHW7w,0,0,2021-03-14T03:22:59Z,That&#39;s definitely very important! I never start coding until I really understand everything.,2021-03-14T03:22:59Z
Ugzkcbie8aaT18vSsyZ4AaABAg,@madsanty8745,,1,7Q-2DEwHW7w,1,7,2020-10-31T02:36:22Z,"Hey, i love your work i hope you continue making videos even the audience isnt a lot, the people who follow you really loves and understand the value of this info. I love all your series and hope you dont lose your interest in making videos.   Thank you!!!?",2020-10-31T02:36:22Z
Ugzkcbie8aaT18vSsyZ4AaABAg.9FSxWnSxhSc9FV_pstbuiT,@stratascratch,Ugzkcbie8aaT18vSsyZ4AaABAg,2,7Q-2DEwHW7w,0,0,2020-11-01T03:07:07Z,Thanks for the kind words! I&#39;m glad you&#39;re finding these videos valuable. I don&#39;t plan to stop any time soon and hope to churn out at least 1 coding question video a week (going to take a break this week tho =)). I also plan to jump into some python programming and build some tools I use when doing data science work at my job. Let me know if you have any other video recommendations. Happy to accommodate.,2020-11-01T03:07:07Z
UgyWSALLWDnldtbDZsZ4AaABAg,@mohammadsaquibalam9063,,1,7Q-2DEwHW7w,2,2,2020-10-30T05:08:00Z,Hey great resource brother. Thanks,2020-10-30T05:08:00Z
UgyWSALLWDnldtbDZsZ4AaABAg.9FQe4YmaSLq9FRl7bgnNn8,@stratascratch,UgyWSALLWDnldtbDZsZ4AaABAg,2,7Q-2DEwHW7w,0,1,2020-10-30T15:28:49Z,Let me know if you want to see other type of content! I do a lot of SQL but can do some python as well =),2020-10-30T15:28:49Z
UgyWSALLWDnldtbDZsZ4AaABAg.9FQe4YmaSLq9FRoUuyGXfw,@mohammadsaquibalam9063,UgyWSALLWDnldtbDZsZ4AaABAg,2,7Q-2DEwHW7w,0,1,2020-10-30T15:58:13Z,@@stratascratch it will be pretty good to share similar content on python as well. I find your approach very practical. Keep up the good work you are doing for many learners like me. Thanks again.,2020-10-30T15:58:13Z
UgwrksgPAKxEksSxegV4AaABAg,@VloggingMemories,,1,qQHz2KKJMus,0,0,2023-05-11T07:44:32Z,"My Solution:<br>select country, <br>    sum(att) att,<br>    sum(sprint) sprint,<br>    sum(rogers) rogers<br>from<br>(select country,<br>case when carrier=&#39;at&amp;t&#39; then 1 else 0 end as att,<br>case when carrier=&#39;sprint&#39; then 1 else 0 end as sprint,<br>case when carrier=&#39;rogers&#39; then 1 else 0 end as rogers<br>from fb_sms_sends<br>where ds::Date=&#39;2020-08-07&#39; and type=&#39;confirmation&#39;)x<br>group by 1<br>order by 1;",2023-05-11T07:44:32Z
UgyVRQqmvDI0Vum3-uN4AaABAg,@sumonamanna2790,,1,qQHz2KKJMus,0,2,2021-11-13T15:33:39Z,Why do we have to use CASE WHEN in this query??,2021-11-13T15:33:39Z
Ugz6Xec-CsI-m291CnV4AaABAg,@joaopedroreissilva7075,,1,qQHz2KKJMus,1,1,2021-11-02T14:19:58Z,"Hi, Nate.<br><br>I couldn&#39;t find this question.<br>Please, could you share with us the link for it?<br>Thanks in advance",2021-11-02T14:19:58Z
Ugz6Xec-CsI-m291CnV4AaABAg.9UFCmK1c-m89UbhdzAb0Cq,@stratascratch,Ugz6Xec-CsI-m291CnV4AaABAg,2,qQHz2KKJMus,0,1,2021-11-11T17:20:59Z,"Unfortunately, it&#39;s not on the platform anymore but there are many like it on there!",2021-11-11T17:20:59Z
UgwgMdiZrEWs-7KYcTF4AaABAg,@VloggingMemories,,1,kN6WGZAq-AM,0,0,2023-05-11T08:02:58Z,My Solution:<br>select (sum(case when status=&#39;closed&#39; then 1 else 0 end)/(sum(case when status=&#39;open&#39; then 1 else 0 end)+sum(case when status=&#39;closed&#39; then 1 else 0 end))::float) * 100<br>from fb_account_status<br>where date::Date=&#39;2020-01-10&#39;;,2023-05-11T08:02:58Z
Ugx-_n1BJVCcmuLgpSp4AaABAg,@dwaipayansaha4443,,1,kN6WGZAq-AM,0,0,2022-08-21T08:36:51Z,My solution<br>with t1 as(select count(acc_id) no_close from fb_account_status<br>where status=&#39;closed&#39; and date=&#39;2020-01-10&#39;)<br>select (no_close/(select count(*) from fb_account_status where status is not null and date=&#39;2020-01-10&#39;):: float) *100  per_closed from t1,2022-08-21T08:36:51Z
UgxjoFmP-zgN88HDdQh4AaABAg,@brandonsager223,,1,kN6WGZAq-AM,0,0,2022-02-04T20:20:13Z,"strata scratch rules <br><br>with cte1 as(<br>select count(*) as closedcount from fb_account_status<br>where date = &#39;01-10-2020&#39;<br>and status = &#39;closed&#39;),<br>cte2 as(<br>select count(*) as totalcount from fb_account_status<br>where date = &#39;01-10-2020&#39;)<br>select closedcount/totalcount::decimal *100 as <br>percent_closed_01_10_2020<br>from<br>cte1 join cte2 on 1=1",2022-02-04T20:20:13Z
UgwlfO3QkDbGV0B4MI94AaABAg,@niveditakumari701,,1,kN6WGZAq-AM,0,0,2021-08-31T01:37:46Z,"Hi, thanks for sharing this, why do we need a group by open and close, we are not doing any aggregation there, can you please help me explain? Thanks!",2021-08-31T01:37:46Z
UgwRoZqPA_cesIo-HFF4AaABAg,@bassoonatic777,,1,kN6WGZAq-AM,0,3,2021-06-17T04:10:29Z,What about:<br><br>SELECT<br>     SUM(CASE WHEN status=&#39;closed&#39; THEN 1 ELSE 0 END) * 1.0 /<br>     COUNT(status)<br>FROM fb_account_status <br>WHERE date=&#39;2020-01-10&#39; <br>GROUP BY date;,2021-06-17T04:10:29Z
UgzScQT0dABXexTIhd54AaABAg,@RaviSingh-xx2wq,,1,kN6WGZAq-AM,0,0,2021-06-15T17:05:46Z,I know it&#39;s  not efficient but i find it easy to work with- <br><br>SELECT <br>  (<br>    SELECT <br>      count(*) <br>    FROM <br>      fb_account_status <br>    WHERE <br>      date = &#39;2020-01-10&#39; <br>      AND status = &#39;closed&#39;<br>  )/ (<br>    SELECT <br>      count(*) <br>    from <br>      fb_account_status <br>    WHERE <br>      date = &#39;2020-01-10&#39;<br>  ):: FLOAT * 100,2021-06-15T17:05:46Z
UgxC-cqDbJRlshGTF7N4AaABAg,@RaviSingh-xx2wq,,1,WtI7QYCG3uo,0,0,2021-06-15T16:48:06Z,SELECT <br>    (SELECT count(*) FROM fb_search_results r<br>    JOIN fb_search_events e<br>    ON r.result_id = e.search_id<br>    WHERE has_clicked = &#39;yes&#39; AND position &lt;= 3)<br>/<br>    (SELECT count(result_id) FROM fb_search_results)<br>    :: float * 100 AS &quot;Top 3&quot;<br><br><br>is this incorrect solution?,2021-06-15T16:48:06Z
UgzyS-VZZNq-tw4h3TV4AaABAg,@dwaipayansaha4443,,1,pFwYjaeKFvQ,1,1,2022-08-21T08:05:45Z,"My solution<br>with cte1 as (select round(sum(case when clicked=1 and search_results_position&lt;=3 then 1<br>else 0 end),2)*100 no_clicked,<br>round(sum(case when clicked=0 and search_results_position&lt;=3 then 1<br>else 0 end),2)*100 no_not_clicked<br>from fb_search_events)<br>select no_clicked/(select count(*) from fb_search_events) top_3_clicked,no_not_clicked/(select count(*) from fb_search_events) top_3_notclicked from cte1",2022-08-21T08:05:45Z
UgzyS-VZZNq-tw4h3TV4AaABAg.9ezQ3wvEz9N9f-_vwfZpVN,@stratascratch,UgzyS-VZZNq-tw4h3TV4AaABAg,2,pFwYjaeKFvQ,0,0,2022-08-21T18:59:53Z,Wonderful.  Thank you for sharing your solution.,2022-08-21T18:59:53Z
UgzDU6stLrIjLHGHpxp4AaABAg,@AnilKumar-bd2yt,,1,pFwYjaeKFvQ,0,4,2021-12-24T07:40:05Z,Select sum(case when position =3 then 1 else 0)/count(*) from table,2021-12-24T07:40:05Z
UgybRV3aOWZA4cSpDOx4AaABAg,@saumyajain628,,1,pFwYjaeKFvQ,1,0,2021-11-26T21:31:29Z,"Simplified solution-<br><br>select <br>count(case when clicked = 1 and search_results_position &lt; 4 then search_id else null end)/count(search_id)::float*100  as top_3_clicked,<br>count(case when clicked = 0 and search_results_position &lt; 4  then search_id else null end)/count(search_id)::float*100 as<br>    top_3_notclicked<br>from fb_search_events",2021-11-26T21:31:29Z
UgybRV3aOWZA4cSpDOx4AaABAg.9VDmEnra3Qu9VDvWTAIqlQ,@stratascratch,UgybRV3aOWZA4cSpDOx4AaABAg,2,pFwYjaeKFvQ,0,0,2021-11-26T22:52:32Z,Great job!,2021-11-26T22:52:32Z
UgzsEFhnY3pGrBGL0td4AaABAg,@himanisethi6958,,1,pFwYjaeKFvQ,1,3,2021-06-15T12:13:38Z,"Even more simplified version of solution -&gt; <br><br>select (temp.num::decimal/temp.den::decimal)*100 as result<br>from    <br>    (select count(*) as den, count(case when position&lt;=3 then position else null end) as num<br>    from fb_search_results<br>    ) temp<br><br><br>By the way, love your videos and methods :)",2021-06-15T12:13:38Z
UgzsEFhnY3pGrBGL0td4AaABAg.9ObV1QBR96X9OboShJ0eJO,@stratascratch,UgzsEFhnY3pGrBGL0td4AaABAg,2,pFwYjaeKFvQ,0,2,2021-06-15T15:12:07Z,Thanks for the solution! And thanks for watching my videos!,2021-06-15T15:12:07Z
Ugy9-bGi-bOIJ7RY6tV4AaABAg,@shobhamourya8396,,1,pFwYjaeKFvQ,0,0,2021-06-14T05:28:46Z,"Joins are difficult to comprehend :( so here&#39;s one without join:<br>with cet1<br>as<br>(<br>  -- rank the positions <br>    select result_id, position,<br>    DENSE_RANK() over(order by position) rank<br>    from fb_search_results<br>    <br>),<br>cet2 <br>as<br>(<br> -- count top 3 search results<br>    select count(*) as top_3_searches<br>    from (<br>            select rank <br>            from cet1<br>            where rank &lt; 4<br>    ) a<br>),<br>cet3<br>as<br>(<br>  -- count total number of search results<br>  select count(result_id) as total_searches<br>  from fb_search_results<br>)<br><br>-- calculate the percentage of top 3 search results<br>select (c2.top_3_searches*1.00/c3.total_searches*1.00)*100 top_3_percentage<br>from cet2 c2, cet3 c3",2021-06-14T05:28:46Z
UgyNZ7qL69q75HvqMrd4AaABAg,@msvrk123,,1,pFwYjaeKFvQ,1,3,2021-01-26T18:27:11Z,"Hi Nate! I found another way to solve the problem using WITH:<br>WITH fb_top3(query,result_id,position,top3) as <br>(select query,result_id,position,<br>CASE<br>    WHEN position&lt;=3 THEN 1<br>    ELSE 0<br>END AS top3<br>from fb_search_results)<br>SELECT SUM(fb_top3.top3) / COUNT(fb_top3.top3)::FLOAT * 100 &quot;top_3_percentage&quot; FROM fb_top3;",2021-01-26T18:27:11Z
UgyNZ7qL69q75HvqMrd4AaABAg.9IzfUvxYN-C9IzwgfSrBPn,@stratascratch,UgyNZ7qL69q75HvqMrd4AaABAg,2,pFwYjaeKFvQ,0,0,2021-01-26T20:57:28Z,"I like it. I prefer using CTEs (ie., WITH) as well. Sometimes I&#39;ll use a subquery because I feel like I can explain it better as I&#39;m working through the problem (say with an interviewer). But if I&#39;m writing code by myself, I prefer using CTEs like you did.",2021-01-26T20:57:28Z
UgwzfNlavOuI-fMUG5N4AaABAg,@chinmaypanchal3654,,1,pFwYjaeKFvQ,3,4,2020-12-28T23:28:30Z,"Hi Nate, thanks again for your excellent videos and just a quick query as can we avoid using JOINS to solve this? E.g. <br><br>    SELECT<br><br>          COUNT(CASE WHEN position &lt;=3 THEN 1 ELSE NULL END)/<br><br>          COUNT(result_id) * 100.0 as perc_search_results<br><br>    FROM<br><br>          fb_search_results",2020-12-28T23:28:30Z
UgwzfNlavOuI-fMUG5N4AaABAg.9HpXvUJGauq9HpxxCv0D96,@stratascratch,UgwzfNlavOuI-fMUG5N4AaABAg,2,pFwYjaeKFvQ,0,3,2020-12-29T03:24:40Z,"Hi. Implementing a CASE WHEN will work here I believe. You&#39;ll want a sum() in the numerator rather than a count() since your CASE statement inputs a 1 ELSE NULL. If you inputted a result_id, then you can use count(). Then cast either the numerator or the denominator into a float or decimal to be able to get a ratio before * 100 to get a percentage. Reading your code and taking a look at my solution...I definitely over-thought the solution to this question...lol",2020-12-29T03:24:40Z
UgwzfNlavOuI-fMUG5N4AaABAg.9HpXvUJGauq9HrIy9ApmJB,@chinmaypanchal3654,UgwzfNlavOuI-fMUG5N4AaABAg,2,pFwYjaeKFvQ,0,0,2020-12-29T15:56:16Z,"@@stratascratch Understood, thanks a lot  :)",2020-12-29T15:56:16Z
UgwzfNlavOuI-fMUG5N4AaABAg.9HpXvUJGauq9V_zX07yKgA,@Halloumas555,UgwzfNlavOuI-fMUG5N4AaABAg,2,pFwYjaeKFvQ,0,0,2021-12-05T21:50:06Z,count is a counter of non null values so count also works. you just need to convert your counts to decimals,2021-12-05T21:51:26Z
Ugx75u17TENdnw8gxVp4AaABAg,@besartc.835,,1,pFwYjaeKFvQ,3,1,2020-11-06T23:51:18Z,"These videos are super great, Nate, thanks!",2020-11-06T23:51:26Z
Ugx75u17TENdnw8gxVp4AaABAg.9FjgC0SV98N9FlRJB2h-9q,@stratascratch,Ugx75u17TENdnw8gxVp4AaABAg,2,pFwYjaeKFvQ,0,1,2020-11-07T16:10:57Z,Hey Besart! Thanks so much. Feel free to comment on any other topics you&#39;d like me to discuss in new videos!,2020-11-07T16:10:57Z
Ugx75u17TENdnw8gxVp4AaABAg.9FjgC0SV98N9Fo8ztz2EmZ,@besartc.835,Ugx75u17TENdnw8gxVp4AaABAg,2,pFwYjaeKFvQ,0,1,2020-11-08T17:28:37Z,@@stratascratch For sure! Would love it if you can go over some of the hard questions where you have to use Window Functions. :),2020-11-08T17:28:37Z
Ugx75u17TENdnw8gxVp4AaABAg.9FjgC0SV98N9FqXEUeC_zX,@stratascratch,Ugx75u17TENdnw8gxVp4AaABAg,2,pFwYjaeKFvQ,0,1,2020-11-09T15:38:57Z,@@besartc.835 Will do! I&#39;ll cover some window functions the next few weeks. Probably next week or the week after. Thanks for the suggestion. Looks like it&#39;s time for me to do more difficult questions =),2020-11-09T15:38:57Z
UgwTAAruU8haRghpm8F4AaABAg,@kritiverma1342,,1,iR8FDQyCtnw,0,3,2022-03-20T21:52:48Z,"Hi Nate, is it possible to avoid the limit function and use rank instead?<br><br>With cte  AS (<br>SELECT<br>    p.title,<br>    MAX(p.budget) / COUNT(e.employee_id) AS budget_per_employee, <br>   DENSE_RANK() OVER ( ORDER BY MAX(p.budget) / COUNT(DISTINCT e.employee_id) DESC) AS rnk<br>FROM projects AS p<br>INNER JOIN employees_projects AS e<br>ON <a href=""http://p.id/"">p.id</a> = e.project_id<br>GROUP BY p.title )<br> <br>select * from cte  <br>where rnk &lt;6",2022-03-20T21:56:38Z
UgzklALmNZ-S2FSw5Vh4AaABAg,@bobbiewang6663,,1,iR8FDQyCtnw,0,1,2021-10-03T02:40:12Z,"Hi, is it okay to assume project title is unique? would it be better to use group by project_id?",2021-10-03T02:40:12Z
UgxCPaAApYvfMjlyuip4AaABAg,@jb4928,,1,iR8FDQyCtnw,1,0,2021-07-31T02:44:49Z,Hi Nate I believe joining two tables does not exclude projects with 0 employees. I think you still need to use &quot;where emp_id &lt;&gt;0&quot; condition. Correct me If I am wrong.,2021-07-31T02:44:49Z
UgxCPaAApYvfMjlyuip4AaABAg.9QRvVZxW-D-9QTK6vnllG9,@stratascratch,UgxCPaAApYvfMjlyuip4AaABAg,2,iR8FDQyCtnw,0,1,2021-07-31T15:47:53Z,That&#39;s true if you&#39;re assuming there would be a project without employees. If you assume that then you can filter our projects with 0 employees in the where cluse for sure.,2021-07-31T15:47:53Z
UgxKcVaXq5eievyyepR4AaABAg,@harryfeng4199,,1,iR8FDQyCtnw,0,0,2021-07-06T21:58:47Z,thank u sir!,2021-07-06T21:58:47Z
UgzDISTvxx0aTD4czAt4AaABAg,@VloggingMemories,,1,UndSRKwsxKM,0,0,2023-05-16T07:40:55Z,"My Solution:<br>with max_cool_votes(cnt) as<br>(select max(cool) from yelp_reviews)<br>select business_name, review_text<br>from yelp_reviews, max_cool_votes<br>where cool=cnt;",2023-05-16T07:40:55Z
UgwXpxNvnRrHOZXePWh4AaABAg,@vastavtailwal2235,,1,UndSRKwsxKM,0,0,2023-02-26T17:50:12Z,"SELECT<br>      business_name,<br>      review_text<br>FROM yelp_reviews<br>WHERE cool = (SELECT MAX(cool)  FROM yelp_reviews)",2023-02-26T17:50:12Z
UgxmYoUF62Oxh2NIQ9V4AaABAg,@dwaipayansaha4443,,1,UndSRKwsxKM,1,1,2022-08-21T08:04:37Z,"My solution<br>with t1 as(select business_name,review_text,sum(cool) no_cool,rank() over(order by sum(cool) desc) rnk from yelp_reviews<br>group by business_name,review_text<br>order by no_cool desc)<br>select business_name,review_text,no_cool from t1<br>where rnk=1",2022-08-21T08:04:37Z
UgxmYoUF62Oxh2NIQ9V4AaABAg.9ezPwZXdfeg9f-ZTf9kvi3,@stratascratch,UgxmYoUF62Oxh2NIQ9V4AaABAg,2,UndSRKwsxKM,0,0,2022-08-21T18:47:09Z,Way to go!!,2022-08-21T18:47:09Z
UgxqB9LDy0XpG8bZpPh4AaABAg,@nilanjanadutta5074,,1,UndSRKwsxKM,0,0,2022-05-22T09:20:55Z,Can you please share the solution for the Yelp question with ID# 9612 on YouTube?,2022-05-22T09:20:55Z
UgwvY1UpIRgJT_LaRJ94AaABAg,@nilanjanadutta105,,1,UndSRKwsxKM,0,0,2022-05-18T10:26:19Z,"@StrataScratch Can you please post the video for the question with ID<a href=""http://www.youtube.com/results?search_query=%239633"">#9633</a> (the Airbnb question)?",2022-05-18T10:26:19Z
UgyqFRJLKbrcVDGpYzN4AaABAg,@eskoo8396,,1,UndSRKwsxKM,1,0,2022-01-08T08:32:32Z,"I prefer using a sub-query with a MAX window function. Also, you output more than the business_name and review_text that were asked.<br><br>thanks!",2022-01-08T08:32:32Z
UgyqFRJLKbrcVDGpYzN4AaABAg.9Ww6H9Hl28v9Wx78oyJBOv,@stratascratch,UgyqFRJLKbrcVDGpYzN4AaABAg,2,UndSRKwsxKM,0,1,2022-01-08T17:59:22Z,Yup! both methods will work. Ranking is probably a more optimized way though. I covered it in some other videos on the channel. Thanks for watching!,2022-01-08T17:59:22Z
Ugz7k0Cq2-CmQd3o5Y14AaABAg,@jb4928,,1,UndSRKwsxKM,1,0,2021-07-31T02:22:03Z,"Hi Nate can we use something like select top (1) with ties business_name, review_date from yelp_reviews order by cool desc;  (This will work with MSSQL I believe)",2021-07-31T02:22:51Z
Ugz7k0Cq2-CmQd3o5Y14AaABAg.9QRstpIoZL99QTJu5FAQJK,@stratascratch,Ugz7k0Cq2-CmQd3o5Y14AaABAg,2,UndSRKwsxKM,0,1,2021-07-31T15:46:00Z,"The only problem is if there are ties, right? You wouldn&#39;t want to just select the top row.",2021-07-31T15:46:00Z
Ugx_0qmTWmIm4lLOB_d4AaABAg,@vigneshiyer2424,,1,UndSRKwsxKM,1,9,2021-02-16T14:08:50Z,"select business_name, review_text, cool<br>from yelp_reviews<br>where cool = (select max(cool) as max_cool from yelp_reviews)<br><br>Is this alternate solution correct or am I missing anything here?",2021-02-16T14:08:50Z
Ugx_0qmTWmIm4lLOB_d4AaABAg.9JpHc2Gqrcx9JpUWc46F2Y,@stratascratch,Ugx_0qmTWmIm4lLOB_d4AaABAg,2,UndSRKwsxKM,0,5,2021-02-16T16:01:33Z,Nope you got it right. That&#39;s another way to approach the solution. A cleaner way actually!,2021-02-16T16:01:33Z
UgyZaPV16J18G2N7WoZ4AaABAg,@toekneema,,1,UndSRKwsxKM,3,1,2021-02-10T02:43:58Z,I love your tutorials! But how are the difficulties decided? This seems pretty novice?,2021-02-10T02:43:58Z
UgyZaPV16J18G2N7WoZ4AaABAg.9JZbTkapmMo9JZrSgQKPs8,@stratascratch,UgyZaPV16J18G2N7WoZ4AaABAg,2,UndSRKwsxKM,0,1,2021-02-10T05:03:38Z,"Thanks! The difficulties are determined based on both how advanced the syntax is and the technical concept being tested. Here the syntax is easy/medium. Maybe a subquery is considered a medium. But the technical concept is medium/hard because most people would find the top cool votes by doing an ORDER BY Desc and then taking a LIMIT 1, which is the most common mistake I see in interviews. I think this difficulty can be averaged down to medium actually. Thanks for the feedback!",2021-02-10T05:03:38Z
UgyZaPV16J18G2N7WoZ4AaABAg.9JZbTkapmMo9MNzBiFvmib,@AnkitPatel-or9bc,UgyZaPV16J18G2N7WoZ4AaABAg,2,UndSRKwsxKM,0,0,2021-04-21T03:54:31Z,"@@stratascratch Using &quot;LIMIT&quot; would be a major red flag, right in an interview because &quot;top cool votes&quot; could be for a single or multiple restaurants?",2021-04-21T03:54:57Z
UgyZaPV16J18G2N7WoZ4AaABAg.9JZbTkapmMo9MP_QgIKTZS,@stratascratch,UgyZaPV16J18G2N7WoZ4AaABAg,2,UndSRKwsxKM,0,0,2021-04-21T18:47:51Z,@@AnkitPatel-or9bc Yea that&#39;s right. It&#39;s a major red flag in that the interviewee may not have the proper experience working with data.,2021-04-21T18:47:51Z
Ugw1phrhwRc5LxLHKbV4AaABAg,@stratascratch,,1,UndSRKwsxKM,0,1,2020-10-12T03:17:44Z,"Interact with question: <a href=""https://bit.ly/34kEIUV"">https://bit.ly/34kEIUV</a><br><br>Timestamps:<br>Interview Question: (<a href=""https://www.youtube.com/watch?v=UndSRKwsxKM&amp;t=0m00s"">0:00</a>)<br>Exploring the Data: (<a href=""https://www.youtube.com/watch?v=UndSRKwsxKM&amp;t=0m15s"">0:15</a>)<br>Assumptions and Tricky Edge Case: (<a href=""https://www.youtube.com/watch?v=UndSRKwsxKM&amp;t=1m00s"">1:00</a>)<br>Developing the Solution Framework: (<a href=""https://www.youtube.com/watch?v=UndSRKwsxKM&amp;t=1m28s"">1:28</a>)<br>SUBQUERY - SQL Query to Identify Max Cool Votes: (<a href=""https://www.youtube.com/watch?v=UndSRKwsxKM&amp;t=2m37s"">2:37</a>)<br>INNER JOIN to Filter Max Cool Votes: (<a href=""https://www.youtube.com/watch?v=UndSRKwsxKM&amp;t=3m01s"">3:01</a>)",2020-10-12T03:18:33Z
UgxbTfF-hQ49AGlZHxd4AaABAg,@siddeshsakhalkar6117,,1,UndSRKwsxKM,2,2,2020-10-10T12:25:45Z,Good tutorial sir üëçüëåüëåüëåüôè,2020-10-10T12:25:45Z
UgxbTfF-hQ49AGlZHxd4AaABAg.9EcwHFR2HCF9EdNdH_rHZy,@stratascratch,UgxbTfF-hQ49AGlZHxd4AaABAg,2,UndSRKwsxKM,0,0,2020-10-10T16:33:34Z,More to come!,2020-10-10T16:33:34Z
UgxbTfF-hQ49AGlZHxd4AaABAg.9EcwHFR2HCF9EdbbsdkHeS,@siddeshsakhalkar6117,UgxbTfF-hQ49AGlZHxd4AaABAg,2,UndSRKwsxKM,0,0,2020-10-10T18:44:27Z,@@stratascratch I&#39;ll definitely watch your SQL series,2020-10-10T18:44:27Z
Ugx1xt0304prQP8LlQt4AaABAg,@stratascratch,,1,S7gKkRxlk1U,0,0,2020-10-12T03:22:18Z,"Link to question: <a href=""https://platform.stratascratch.com/ed.."">https://platform.stratascratch.com/ed..</a>.",2020-10-12T03:22:18Z
UgxWbiCd_r-zsBxqo514AaABAg,@redwannabil8031,,1,S7gKkRxlk1U,0,0,2022-10-28T10:46:39Z,On date 04 there is no acceptance...then how come the query is resulting as such???,2022-10-28T10:46:39Z
UgyG3s2EaFRDvV-K6kh4AaABAg,@jaden2582,,1,S7gKkRxlk1U,1,2,2022-09-04T08:23:06Z,"In my opinion, one thing is wrong here: the sender is always the one take an action. Then for &#39;send&#39; action, it&#39;s A -&gt; B, and for &#39;accepted&#39;, it&#39;s B -&gt; A. So when we join two tables, shouldn&#39;t it be: b.user_id_receiver = a.user_id_sender and b.user_id_sender = a.user_id_receiver ?",2022-09-04T08:23:50Z
UgyG3s2EaFRDvV-K6kh4AaABAg.9fYVBGgm6CJ9fdRuVKw_fk,@stratascratch,UgyG3s2EaFRDvV-K6kh4AaABAg,2,S7gKkRxlk1U,0,0,2022-09-06T15:49:54Z,Thank you for sharing and for your feedback.,2022-09-06T15:49:54Z
UgxHNpO08_b7FyW4MlJ4AaABAg,@abhinavanand789,,1,S7gKkRxlk1U,0,1,2022-04-23T14:04:41Z,Why do we use group by? Why cannot we avoid it in this case and just order by date desc ?,2022-04-23T14:04:41Z
UgwUyKlOdjiyEQC-tpJ4AaABAg,@joaopedroreissilva7075,,1,S7gKkRxlk1U,5,0,2021-11-04T18:33:31Z,"Really good, Nate!<br>Thank you.<br><br>I coded by this way like below:<br><br>WITH e_sents AS (SELECT<br>                  *<br>                FROM fb_friend_requests<br>                WHERE action = &#39;sent&#39;),<br><br>   e_accepts AS (SELECT<br>                  *<br>                FROM fb_friend_requests<br>                WHERE action = &#39;accepted&#39;),<br><br>    combined AS (SELECT<br>                  t1.user_id_sender,<br>                  t1.user_id_receiver,<br>                  <a href=""http://t1.date/"">t1.date</a>,<br>                  t1.action AS ac_sents,<br>                  t2.action AS ac_accepts<br>                FROM e_sents AS t1<br>                LEFT JOIN e_accepts AS t2<br>                ON t1.user_id_sender = t2.user_id_sender AND t1.user_id_receiver = t2.user_id_receiver<br>                ORDER BY date ASC)<br><br>SELECT<br>  date,<br>  COUNT(ac_accepts)::FLOAT / COUNT(ac_sents) AS acceptance_ratio<br>FROM combined<br>GROUP BY date<br><br><br>I think that I could do the same with less CTEs, but I think in this case it&#39;s worth getting longer to get cleaner.",2021-11-04T18:33:31Z
UgwUyKlOdjiyEQC-tpJ4AaABAg.9UKoO2hlkuo9ULDfIFTxT4,@stratascratch,UgwUyKlOdjiyEQC-tpJ4AaABAg,2,S7gKkRxlk1U,0,1,2021-11-04T22:23:12Z,I like it since it&#39;s much cleaner. The performance might take a hit but that&#39;s usually okay even in practice.,2021-11-04T22:23:12Z
UgwUyKlOdjiyEQC-tpJ4AaABAg.9UKoO2hlkuo9ULXy3KwERD,@joaopedroreissilva7075,UgwUyKlOdjiyEQC-tpJ4AaABAg,2,S7gKkRxlk1U,0,0,2021-11-05T01:20:31Z,"@@stratascratch Thanks, Nate.<br>It&#39;s always good to have your feedback.",2021-11-05T01:20:31Z
UgwUyKlOdjiyEQC-tpJ4AaABAg.9UKoO2hlkuo9aA3iaSWjPO,@abhinavanand789,UgwUyKlOdjiyEQC-tpJ4AaABAg,2,S7gKkRxlk1U,0,1,2022-04-23T14:04:23Z,Why do we use group by? Why cannot we avoid it in this case and just order by date desc ?,2022-04-23T14:04:23Z
UgwUyKlOdjiyEQC-tpJ4AaABAg.9UKoO2hlkuo9aAjsiTtyOB,@joaopedroreissilva7075,UgwUyKlOdjiyEQC-tpJ4AaABAg,2,S7gKkRxlk1U,0,1,2022-04-23T20:21:30Z,"@@abhinavanand789 Because we need to consolidate this data into groups (in this case, the group is date), so we use COUNT.<br>Always when some aggregate function is used (eg: count, sum, avg...), we need to use group by together.",2022-04-23T20:22:54Z
UgwUyKlOdjiyEQC-tpJ4AaABAg.9UKoO2hlkuo9aFEclRcxBS,@abhinavanand789,UgwUyKlOdjiyEQC-tpJ4AaABAg,2,S7gKkRxlk1U,0,1,2022-04-25T14:15:54Z,"@@joaopedroreissilva7075 Sure, thanks a ton pal !!",2022-04-25T14:15:54Z
Ugxgf8fa0s_ghQKkw4d4AaABAg,@priyankalad7789,,1,S7gKkRxlk1U,1,1,2021-03-31T22:49:01Z,"select <a href=""http://a.date/"">a.date</a>,<br>a.total_accepted/a.total_sent as acceptance_rate<br>(select <br>count(case when action = &#39;accepted&#39; then 1 end) as total_accepted,<br>count(case when action = &#39;sent&#39; then 1 end) as total_sent,<br>from fb_friend_requests<br>group by date ) a<br>order by <a href=""http://a.date/"">a.date</a> desc<br><br>Actually, my solution was this. I am just wondering if this will be taken as the correct answer?",2021-03-31T22:49:01Z
Ugxgf8fa0s_ghQKkw4d4AaABAg.9LZwKgs8HQF9L_-VbkSI2R,@stratascratch,Ugxgf8fa0s_ghQKkw4d4AaABAg,2,S7gKkRxlk1U,0,3,2021-03-31T23:25:28Z,"In theory yes, but there is an assumption in your code that there will always be sent actions for every date, which isn&#39;t true. The solution in the video solves for that edge case.",2021-03-31T23:25:28Z
Ugz5AmXeobPFnOVbCRB4AaABAg,@priyankasarkar6600,,1,S7gKkRxlk1U,1,0,2021-03-12T06:51:50Z,Great work Sir! Applaud you :),2021-03-12T06:51:50Z
Ugz5AmXeobPFnOVbCRB4AaABAg.9KmIgOfqnak9KnSV74MCKW,@stratascratch,Ugz5AmXeobPFnOVbCRB4AaABAg,2,S7gKkRxlk1U,0,1,2021-03-12T17:36:46Z,Thanks for watching!,2021-03-12T17:36:46Z
UgzYlgcAw5NVn366IG14AaABAg,@NotFound-iu8wx,,1,S7gKkRxlk1U,4,11,2020-11-21T14:23:26Z,"Hello Nate,<br>Why can&#39;t we don&#39;t the following and avoid joins altogether??<br><br>Select date, count (case when action = &#39;accepted&#39; then action else null end) *1.0/count (case when action = &#39;sent&#39; then action else null end)<br><br>From table",2020-11-21T17:06:26Z
UgzYlgcAw5NVn366IG14AaABAg.9GKI7r65fqt9GKZ24ErlQy,@stratascratch,UgzYlgcAw5NVn366IG14AaABAg,2,S7gKkRxlk1U,0,11,2020-11-21T16:51:11Z,"If you were trying to just count the sents and accepted then you don&#39;t need to do a join. The purpose of the join is to match users who sent a friend request with those that accepted a friend request so that you&#39;re tightly controlling the logic. You can do it your way without the join and it would produce a result that&#39;s directional. The JOIN also allows you to add a time window where you&#39;d only consider friend acceptances that are within 2 weeks. This might be important if you&#39;re trying to see if a feature that&#39;s recently built helped to improve the rate of acceptances. So there&#39;s real world things to look at when designing a JOIN. I should have included this explanation on why we&#39;d use a JOIN. But in this case, you&#39;re also right about not totally having to use a JOIN. Thanks for the message!",2020-11-21T16:51:11Z
UgzYlgcAw5NVn366IG14AaABAg.9GKI7r65fqt9GK_gh-7Hnl,@NotFound-iu8wx,UgzYlgcAw5NVn366IG14AaABAg,2,S7gKkRxlk1U,0,1,2020-11-21T17:05:36Z,"@@stratascratch Thanks Nate for the explanation. I work at a FANG, these are very much similar to what I use on a daily basis.<br><br>I am binge watching your videos. Very informative and easy to understand.",2020-11-21T17:05:36Z
UgzYlgcAw5NVn366IG14AaABAg.9GKI7r65fqt9GKiKxCWUeB,@stratascratch,UgzYlgcAw5NVn366IG14AaABAg,2,S7gKkRxlk1U,0,0,2020-11-21T18:21:08Z,"@@NotFound-iu8wx That&#39;s great to hear! I&#39;ll keep doing these types of videos but if you have any requests please let me know! I got some python automation coming out, more window functions, case whens, data pivoting. Happy to add your requests to the list!",2020-11-21T18:21:08Z
UgzYlgcAw5NVn366IG14AaABAg.9GKI7r65fqt9_ItMhDnAUP,@pramodyadav4422,UgzYlgcAw5NVn366IG14AaABAg,2,S7gKkRxlk1U,0,2,2022-04-02T03:46:54Z,Exactly this was in my mind.<br>Instead of JOIN we can use CASE.<br>But as per explanation given by Nate now it make sense when to use join.<br><br>Thanks üôè,2022-04-02T03:46:54Z
UgxHyC2B1-Ztu0CHMIN4AaABAg,@nidhigupta5977,,1,U9P4M4EPTgI,1,0,2023-03-19T09:04:23Z,where can i find your channel Nick?,2023-03-19T09:04:23Z
UgxHyC2B1-Ztu0CHMIN4AaABAg.9nRFi2I15yW9nSZlteW-fK,@stratascratch,UgxHyC2B1-Ztu0CHMIN4AaABAg,2,U9P4M4EPTgI,0,0,2023-03-19T21:18:54Z,This is my channel. I rebranded it to StrataScratch =),2023-03-19T21:18:54Z
UgzhbWUO0nA9t8WKKPZ4AaABAg,@anuragsingh4766,,1,U9P4M4EPTgI,1,2,2020-09-30T07:25:06Z,Great. Takes questions from python and SQL both and give us a question to do on our own after the video .üëçüëç,2020-09-30T07:25:06Z
UgzhbWUO0nA9t8WKKPZ4AaABAg.9EDdvPLUUVI9EETbyzkLz3,@stratascratch,UgzhbWUO0nA9t8WKKPZ4AaABAg,2,U9P4M4EPTgI,0,0,2020-09-30T15:05:34Z,That&#39;s right! Follow along with me and write the code with me so you can see how I&#39;m implementing my logic. I think it&#39;s helpful to be able to execute code with a real dataset in a live IDE!,2020-09-30T15:05:34Z
Ugxv5jerS2xLIyGLw4p4AaABAg,@borisgiba4510,,1,U9P4M4EPTgI,2,1,2020-09-25T15:41:04Z,Awesome; I look forward to the questions! :),2020-09-25T15:41:04Z
Ugxv5jerS2xLIyGLw4p4AaABAg.9E1ehf38CML9E2r0quX9JV,@stratascratch,Ugxv5jerS2xLIyGLw4p4AaABAg,2,U9P4M4EPTgI,0,0,2020-09-26T02:47:55Z,Great! There&#39;s 2 in the series already. I hope to release 1-2 a week!,2020-09-26T02:47:55Z
Ugxv5jerS2xLIyGLw4p4AaABAg.9E1ehf38CML9E6vwV2MMyH,@borisgiba4510,Ugxv5jerS2xLIyGLw4p4AaABAg,2,U9P4M4EPTgI,0,0,2020-09-27T16:47:51Z,Nice! üëç<br>(I probably won&#39;t look into DS questions at the current moment but I&#39;m sure to review them when I prepare for interviews!),2020-09-27T16:49:36Z
UgyT7wMO4oCemfxRkbF4AaABAg,@user-li7pg7kz6s,,1,U9P4M4EPTgI,0,0,2020-09-23T07:13:43Z,"<a href=""https://www.youtube.com/watch?v=U9P4M4EPTgI&amp;t=0m17s"">0:17</a> 1000 subs for 15usd whatsapp +79671570581",2020-09-23T07:13:43Z
UgyFqg7i3jpdR_sZruN4AaABAg,@meatloaf7765,,1,n6gM265zG68,0,0,2023-04-20T05:16:04Z,That introduction got me üòÇüòÇ,2023-04-20T05:16:04Z
UgzJXo8B1loyV4Oe_nF4AaABAg,@ArvindSingh-ks3ir,,1,n6gM265zG68,2,0,2022-08-10T05:51:21Z,All indians in the comment section,2022-08-10T05:51:21Z
UgzJXo8B1loyV4Oe_nF4AaABAg.9eXqwyx1LE99eZ7LYVLtr-,@stratascratch,UgzJXo8B1loyV4Oe_nF4AaABAg,2,n6gM265zG68,0,1,2022-08-10T17:42:38Z,Is that right?  Awesome thank you!,2022-08-10T17:42:38Z
UgzJXo8B1loyV4Oe_nF4AaABAg.9eXqwyx1LE99eZGE6hUwRp,@ArvindSingh-ks3ir,UgzJXo8B1loyV4Oe_nF4AaABAg,2,n6gM265zG68,0,0,2022-08-10T19:00:16Z,"@@stratascratch Yes, Loads of love from India. keep up the good work :)",2022-08-10T19:00:16Z
UgyHqQGanvEXsRVa4uV4AaABAg,@prashanthigupta,,1,n6gM265zG68,0,0,2022-04-07T01:03:51Z,Thank you so much for your videos. It helped me a lot in my sql interviews. Appreciate your good worküëèüèºüëèüèºüëèüèº,2022-04-07T01:03:51Z
UgzDlm82952q2yqUSL54AaABAg,@ramanadeepsingh,,1,n6gM265zG68,1,1,2021-08-08T13:50:31Z,"For Q3, what if we wanted to show only one result at the end. Since, listing them both &quot;Amitah, Vivek&quot; and then &quot;Vivek, Amitah&quot; doesnt really help.",2021-08-08T13:50:31Z
UgzDlm82952q2yqUSL54AaABAg.9Qmi2I5xMfU9VaucvQTgrT,@wyattsullivan2714,UgzDlm82952q2yqUSL54AaABAg,2,n6gM265zG68,0,1,2021-12-06T06:26:36Z,"A simple solution would be to only select w1.first_name and w1.salary. That way you&#39;ll get:<br>Amitah, 500000<br>Vivek, 500000",2021-12-06T06:26:36Z
UgxypEWypzu_5PsHMUh4AaABAg,@dineshkumar-of1tq,,1,n6gM265zG68,1,2,2020-02-22T03:17:24Z,How ill get a job in google,2020-02-22T03:17:24Z
UgxypEWypzu_5PsHMUh4AaABAg.95K8to1VwkE98khgxb_mSK,@govindugovindu617,UgxypEWypzu_5PsHMUh4AaABAg,2,n6gM265zG68,0,0,2020-05-17T09:43:53Z,It depends on you only,2020-05-17T09:43:53Z
UgwmFKT8LVAC1GM0Q8B4AaABAg,@majubodas6206,,1,9av7jQ9MA44,1,0,2023-01-05T15:52:34Z,"Thanks for the video, I would really appreciate if you could make more videos about A/B testing, and in case there are more types of tests used in the industry could you talk about them as well? Tks in advance!",2023-01-05T15:52:34Z
UgwmFKT8LVAC1GM0Q8B4AaABAg.9kW0OtXP_Dk9kWCfeW57s-,@stratascratch,UgwmFKT8LVAC1GM0Q8B4AaABAg,2,9av7jQ9MA44,0,0,2023-01-05T17:39:51Z,Yes we will! We&#39;ll also try to add more data projects that involve AB testing. More to come! Thanks for your feedback.,2023-01-05T17:39:51Z
Ugz7f-LdZneBwJGCubJ4AaABAg,@PiyushSharma-jv4fy,,1,9av7jQ9MA44,0,1,2022-11-14T17:19:12Z,Suggestion : Speak slowly so that we can comprehend especially when there is a new topic or maybe it&#39;s just me who does not have enough clarity on the topic so i want it slow like 0.85x or 0.9x maybe,2022-11-14T17:19:12Z
UgwAa1zRU0SrDlEeq3p4AaABAg,@jerrywang1550,,1,9av7jQ9MA44,0,1,2022-10-08T23:34:15Z,Don&#39;t rush,2022-10-08T23:34:15Z
Ugxh_ao97V1dueV_GPN4AaABAg,@JOMELLO,,1,9av7jQ9MA44,1,1,2022-10-06T23:36:18Z,Please implement a Dark mode option for the website. I think a lot of the current userbase (and the future customers) will really appreciate not being blinded by the white background lol,2022-10-06T23:36:18Z
Ugxh_ao97V1dueV_GPN4AaABAg.9grX7lWwjAb9gruBPpZXd2,@stratascratch,Ugxh_ao97V1dueV_GPN4AaABAg,2,9av7jQ9MA44,0,1,2022-10-07T03:06:31Z,"haha yea it&#39;s on our roadmap and hope to get it out in a few months or so. In the meantime, I&#39;ve seen users use the dark mode in Chrome. Hope that&#39;s a good interim option for you.",2022-10-07T03:06:31Z
UgxuYdz2TGYA5UDjtHJ4AaABAg,@softwaretestinglearninghub,,1,9av7jQ9MA44,0,0,2022-10-06T19:18:22Z,Thank you for this video!,2022-10-06T19:18:22Z
UgwUpC9zOhHHDe8IZEx4AaABAg,@VloggingMemories,,1,8zeLdtkY2CQ,0,0,2023-08-12T08:33:23Z,"My Solution:<br>with user_lastest as<br>(select account_id,<br>    user_id,<br>    to_char(max(date), &#39;YYYY-MM&#39;) as latest_month_date<br>from sf_events<br>group by 1, 2),<br>dec_2020 as<br>(select account_id,<br>    sum(retention_status)/count(user_id)::float dec_rt<br>from<br>(select t1.account_id,<br>    t1.user_id,<br>    t1.year_month,<br>    t2.latest_month_date,<br>    case <br>        when latest_month_date&gt;year_month then 1 else 0<br>        end as retention_status<br>from<br>(select distinct account_id,<br>    user_id,<br>    to_char(date, &#39;YYYY-MM&#39;) as year_month<br>from sf_events<br>where to_char(date, &#39;YYYY-MM&#39;)=&#39;2020-12&#39;)t1<br>join user_lastest t2 on t1.account_id=t2.account_id and t1.user_id=t2.user_id)t3<br>group by 1),<br>jan_2021 as<br>(select account_id,<br>    sum(retention_status)/count(user_id)::float jan_rt<br>from<br>(select t1.account_id,<br>    t1.user_id,<br>    t1.year_month,<br>    t2.latest_month_date,<br>    case <br>        when latest_month_date&gt;year_month then 1 else 0<br>        end as retention_status<br>from<br>(select distinct account_id,<br>    user_id,<br>    to_char(date, &#39;YYYY-MM&#39;) as year_month<br>from sf_events<br>where to_char(date, &#39;YYYY-MM&#39;)=&#39;2021-01&#39;)t1<br>join user_lastest t2 on t1.account_id=t2.account_id and t1.user_id=t2.user_id)t3<br>group by 1)<br>select t1.account_id,<br>jan_rt/dec_rt::float<br>from jan_2021 t1, dec_2020 t2<br>where t1.account_id=t2.account_id;",2023-08-12T08:33:23Z
Ugx9dIqSfBUBOX-JddJ4AaABAg,@huanchenli4137,,1,8zeLdtkY2CQ,0,0,2023-04-28T00:35:12Z,nice video! but the only thing that guy said is yea....lol,2023-04-28T00:35:12Z
UgxpkTf3xhanww-bb4N4AaABAg,@stanislavdidenko8436,,1,8zeLdtkY2CQ,0,0,2022-10-01T17:11:13Z,"with aum_order as<br>( <br>    select <br>        distinct account_id, user_id, date_trunc(&#39;months&#39;, date) as month<br>    from sf_events<br>    order by account_id, user_id, month desc<br>)<br>, aum_lead as<br>( <br>    select <br>        *,<br>        lead(month, 1) over (partition by account_id, user_id order by month) lead_month<br>    from aum_order<br>)<br>, acc_rr as<br>(<br>    select <br>        account_id, month,<br>        100*sum(case when lead_month is not null then 1 else 0 end )/count(*)*1.0 rr<br>    from aum_lead<br>    where month&lt;&#39;2021-02-01&#39;<br>    group by account_id, month<br>)<br>, rr_next as<br>(<br>select<br>    account_id, month, rr,  lag(rr, 1) over (partition by account_id order by month) rr_next<br>    from acc_rr<br>    order by account_id, month desc<br>)<br>select *,  rr/rr_next<br>from rr_next<br>where rr_next is not null<br>;",2022-10-01T17:11:13Z
UgyV4HHU2nuww82n99V4AaABAg,@challenger42,,1,8zeLdtkY2CQ,1,0,2022-08-09T08:16:22Z,"Hi, thank you for amazing video. During the interview, should we use SQL only, or can we go with pandas as well?",2022-08-09T08:16:22Z
UgyV4HHU2nuww82n99V4AaABAg.9eVXk8kRVvA9eWZG8Wkh3z,@stratascratch,UgyV4HHU2nuww82n99V4AaABAg,2,8zeLdtkY2CQ,0,1,2022-08-09T17:48:51Z,"I would say it will depend on where you are more comfortable and which you think will help you better come up with a solution.  But for this interview video, I would use SQL.",2022-08-09T17:48:51Z
Ugz8bmYiCS9sL9-DZah4AaABAg,@sweetie_py,,1,8zeLdtkY2CQ,1,0,2022-08-06T11:34:50Z,"damn... like i do know about assuming, breaking down the problems and identifying the edge cases but idk how to get good at each of those. Could you make a video on how to think in SQL?",2022-08-06T11:34:50Z
Ugz8bmYiCS9sL9-DZah4AaABAg.9eOA4AGKPQQ9eP03Gh0ncC,@stratascratch,Ugz8bmYiCS9sL9-DZah4AaABAg,2,8zeLdtkY2CQ,0,0,2022-08-06T19:26:34Z,That sounds like a plan.,2022-08-06T19:26:34Z
UgyiYKhrkfL9Ort0kTp4AaABAg,@AnkitPatel-jg1fw,,1,8zeLdtkY2CQ,1,0,2022-07-24T19:20:09Z,Great video. Amazing content. I come from a chemical engineering background. I have an admit for MS in CS in US. Will that help me get into data science? Will not having prior experience cause hindrance in getting full times. I would be grateful if you could answer,2022-07-24T19:20:09Z
UgyiYKhrkfL9Ort0kTp4AaABAg.9dsWzrpKSSR9dwDP-6ALuK,@stratascratch,UgyiYKhrkfL9Ort0kTp4AaABAg,2,8zeLdtkY2CQ,0,0,2022-07-26T05:45:55Z,"Thank you, glad you like it.  There is no set of requirements for Data Science, but an engineering background can help you. You will definitely need to sharpen relevant skills in order to clinch that offer.",2022-07-26T05:45:55Z
UgwKhfO4TE_0vg4C_g54AaABAg,@phuongdinh3769,,1,8zeLdtkY2CQ,1,0,2022-06-29T16:27:35Z,I&#39;ve learned so much from this! Thank you!,2022-06-29T16:27:35Z
UgwKhfO4TE_0vg4C_g54AaABAg.9crqN6G9kgQ9dmmbCMqLEb,@stratascratch,UgwKhfO4TE_0vg4C_g54AaABAg,2,8zeLdtkY2CQ,0,0,2022-07-22T13:49:53Z,Wonderful!  You are welcome.,2022-07-22T13:49:53Z
UgzmKZL6t7JtqSTDlcd4AaABAg,@kaitlynkhanhthidong5452,,1,8zeLdtkY2CQ,1,2,2022-06-29T03:47:12Z,I took your class 2 years ago during the pandemic at USF and I&#39;m so glad your channel is doing well! It really sparked my interest in problem-solving and data analytics for me even though my major is Finance. Found myself here as I need to solve for a SQL problem :) Keep up the good work prof!,2022-06-29T03:47:12Z
UgzmKZL6t7JtqSTDlcd4AaABAg.9cqULrms80G9cvO1kO7Uad,@stratascratch,UgzmKZL6t7JtqSTDlcd4AaABAg,2,8zeLdtkY2CQ,0,1,2022-07-01T01:28:14Z,Oh very cool! Small world that you found me here! And glad you are continuing with SQL and data analytics! Good luck!,2022-07-01T01:28:14Z
UgziCnvONcOpwoR-Qld4AaABAg,@eduarlara3424,,1,8zeLdtkY2CQ,1,0,2022-06-25T03:37:58Z,"i have a college degree in mechanical engineering and naval engineering, will this help me get a job as a data scientist?",2022-06-25T03:37:58Z
UgziCnvONcOpwoR-Qld4AaABAg.9cgA6iG8Y8k9dbNXxsMCBc,@stratascratch,UgziCnvONcOpwoR-Qld4AaABAg,2,8zeLdtkY2CQ,0,0,2022-07-18T03:30:28Z,An engineering background can definitely help you become a Data Scientist.,2022-07-18T03:30:28Z
UgwIjx26t4vK61vbpoh4AaABAg,@rustinshamloo3413,,1,8zeLdtkY2CQ,0,0,2022-06-23T05:56:15Z,Keep up the good work guys!,2022-06-23T05:56:15Z
UgydWC3GfkyfVdCAy9J4AaABAg,@YUVRAJSINGH-iz9gt,,1,8zeLdtkY2CQ,1,0,2022-06-23T04:25:25Z,"Hi team, I am going to buy a lifetime subscription of strata could someone please provide me a discount coupon for the same",2022-06-23T04:25:25Z
UgydWC3GfkyfVdCAy9J4AaABAg.9cb5xXzBATt9dmmmkCqqC-,@stratascratch,UgydWC3GfkyfVdCAy9J4AaABAg,2,8zeLdtkY2CQ,0,0,2022-07-22T13:51:28Z,"Kindly email nate@<a href=""http://stratascratch.com/"">stratascratch.com</a>.  We do have a discount for students.",2022-07-22T13:51:28Z
Ugw8yb3_XIMf9dERtzV4AaABAg,@artivd6429,,1,8zeLdtkY2CQ,0,0,2022-06-22T04:45:28Z,"Please watch for python interview questions with explanations <a href=""https://youtu.be/92_YNkA90Nc?view_as=subscriber?sub_confirmation=1"">https://youtu.be/92_YNkA90Nc?view_as=subscriber?sub_confirmation=1</a>",2022-06-22T04:45:28Z
UgznsTszlkYd0dRtbad4AaABAg,@ethigpen8,,1,8zeLdtkY2CQ,0,1,2022-06-19T17:15:12Z,stratascratch and your video have been huge for me throughout my interview process,2022-06-19T17:15:12Z
UgxVhLHpizEwKyBS9NJ4AaABAg,@joel9616,,1,8zeLdtkY2CQ,0,0,2022-06-19T14:33:00Z,Thank You for this,2022-06-19T14:33:00Z
UgwgCfwOF-d5Jc1n35l4AaABAg,@yuthpatirathi2719,,1,8zeLdtkY2CQ,0,0,2022-06-18T00:36:39Z,Nate this is an amazing video. Thank you,2022-06-18T00:36:39Z
UgyjGTzk8OMeQTZNP2R4AaABAg,@jennymaalouf9928,,1,8zeLdtkY2CQ,0,0,2022-06-17T10:21:39Z,Thank you these videos are so helpful!,2022-06-17T10:21:39Z
UgzNaUNHqakO3LRtLxl4AaABAg,@waskjohnson2033,,1,30hS-MjpU6E,1,0,2023-08-21T02:38:40Z,"Thank you as this helped me think differently around optimization, and that it&#39;s more intuitive than big and scary.<br><br>At work I review a lot of other&#39;s queries (internal &amp; client) and code readability is <b>always</b> an issue. Drives me insane as I always have to rewrite/reformat them before I even bother trying to understand them.",2023-08-21T02:38:40Z
UgzNaUNHqakO3LRtLxl4AaABAg.9tefm_q6SX69thUO5IfuKT,@stratascratch,UgzNaUNHqakO3LRtLxl4AaABAg,2,30hS-MjpU6E,0,0,2023-08-22T04:48:02Z,Great to hear!,2023-08-22T04:48:02Z
Ugw7f15p-c4B6GTGhs54AaABAg,@luckychitundu1070,,1,30hS-MjpU6E,0,0,2023-05-07T23:11:43Z,Hi @ Nate!<br>Does StrataScratch have a shortcut for formatting your SQL code?,2023-05-07T23:11:43Z
UgzjSm22gv93ApVdjRx4AaABAg,@VloggingMemories,,1,30hS-MjpU6E,0,1,2022-11-13T19:05:34Z,"My Solution:<br><br>with user_table as(<br>select t1.user_id, t2.paying_customer<br>from ms_user_dimension t1<br>inner join ms_acc_dimension t2 on t1.acc_id=t2.acc_id<br>),<br>final_res as(<br>select distinct <a href=""http://t1.date/"">t1.date</a>, <br>    t2.paying_customer,<br>    sum(downloads) over (partition by date, paying_customer order by date) sum_downloads<br>from ms_download_facts t1<br>inner join user_table t2 on t1.user_id=t2.user_id)<br>select <a href=""http://t1.date/"">t1.date</a>,<br>t1.sum_downloads as &quot;non-paying downloads&quot;,<br>t2.sum_downloads as &quot;paying downloads&quot;<br>from (select * from final_res where paying_customer=&#39;no&#39;) t1, (select * from final_res where paying_customer=&#39;yes&#39;) t2 <br>where t1.date=<a href=""http://t2.date/"">t2.date</a> and t1.sum_downloads&gt;t2.sum_downloads <br>order by 1 desc;",2022-11-13T19:05:34Z
UgwsZ1btReBbm8oKRJ54AaABAg,@agnespitka3703,,1,30hS-MjpU6E,0,2,2022-10-21T07:55:36Z,Thanks. I learned a lot. I&#39;m pretty new to the industry. I am working with SQL every day so videos that start with &quot;what is SQL&quot; and &quot;this is a SELECT statement&quot; are really not what I need but I am not in the level to write good/optimal code. The video was exactly what I needed. And not too long and not too short. Everything structured well.,2022-10-21T07:55:36Z
Ugzg_9qvhpLyJ074Bpt4AaABAg,@elleandish,,1,30hS-MjpU6E,1,0,2022-07-15T00:48:44Z,"nate!!! i was looking up sql videos and thought i saw a familiar face from high school üòÜ nice channel, very helpful!  i‚Äôm not doing data science by title but wanted to get better at sql. üíñ janelle",2022-07-15T00:48:44Z
Ugzg_9qvhpLyJ074Bpt4AaABAg.9dUMdgypB0E9dUjOXc7d0a,@stratascratch,Ugzg_9qvhpLyJ074Bpt4AaABAg,2,30hS-MjpU6E,0,0,2022-07-15T04:16:14Z,"Wow what a blast from the past. You got a great channel yourself. Way better than mine! If you want to learn SQL, check out my platform =) We&#39;ll start resuming the SQL videos next month so I hope you check back. Also, LeetCode is another great platform with a SQL component.",2022-07-15T04:16:14Z
UgxVlPJKJeCjpLaqWEF4AaABAg,@ManishaParmar,,1,30hS-MjpU6E,0,0,2022-05-26T03:28:00Z,"Really great optimizing techniques! These are some of the key things that I keep in mind while writing my solutions as well, and I like how you explain every step.",2022-05-26T03:28:00Z
UgxhBk2zzp4EdIxS89t4AaABAg,@CruiserPup,,1,30hS-MjpU6E,0,0,2022-05-08T06:28:59Z,I&#39;ve learned so much from your videos and my stratascratch premium subscription in the past year.  Thank you thank you thank you! :),2022-05-08T06:28:59Z
UgywCuaAD8lYsqGFC_N4AaABAg,@insider-training1439,,1,30hS-MjpU6E,0,0,2022-04-13T20:57:10Z,"Very helpful video Nate, thanks for doing this.",2022-04-13T20:57:10Z
Ugww41Nqu9-WWKuNMKV4AaABAg,@austinocampo2410,,1,30hS-MjpU6E,1,0,2022-03-16T15:45:24Z,Cleaner if you use an IF statement instead of CASE!,2022-03-16T15:45:24Z
Ugww41Nqu9-WWKuNMKV4AaABAg.9ZdP4EEBaDq9ZdZbwqPKL2,@stratascratch,Ugww41Nqu9-WWKuNMKV4AaABAg,2,30hS-MjpU6E,0,0,2022-03-16T17:17:31Z,IF statement in SQL?,2022-03-16T17:17:31Z
UgyJPBUrmxDjdlLxnv54AaABAg,@bocanegradev,,1,30hS-MjpU6E,0,1,2022-02-15T16:41:34Z,amazing! always it&#39;s better to understand best practices with examples like this... It would be awesome to see more like this,2022-02-15T16:41:34Z
UgzoItLqI60E05oIGOd4AaABAg,@dhanushph8170,,1,30hS-MjpU6E,1,0,2022-02-07T21:38:25Z,"Can anyone help me , which is the best site to learn the syntax for window functions",2022-02-07T21:38:25Z
UgzoItLqI60E05oIGOd4AaABAg.9Y9l3QUXVz59Y9nQ0qoMAN,@stratascratch,UgzoItLqI60E05oIGOd4AaABAg,2,30hS-MjpU6E,0,1,2022-02-07T21:58:59Z,Try mode analytics SQL tutorial for a free tutorial. That&#39;s how I learned how to write window functions. Then try StrataScratch to practice and get better. Good luck!,2022-02-07T21:58:59Z
UgydSL4c8uWgGwbdlst4AaABAg,@nandiniguntur4509,,1,30hS-MjpU6E,1,0,2022-01-31T20:23:38Z,"Hey Nate! Kudos to the great series. You are a good tutor. I have a request, can you please make a playlist for machine learning &amp; statistics with python? It would be extremely helpful.",2022-01-31T21:19:42Z
UgydSL4c8uWgGwbdlst4AaABAg.9Xsawa0qX699XscpCFGHdC,@stratascratch,UgydSL4c8uWgGwbdlst4AaABAg,2,30hS-MjpU6E,0,0,2022-01-31T20:40:06Z,We are definitely doing this soon!,2022-01-31T20:40:06Z
Ugwx-4R9uSi5gZXZOw94AaABAg,@diaconescutiberiu7535,,1,30hS-MjpU6E,3,0,2022-01-30T11:04:25Z,"Quick question (I&#39;ve just started with sql). From what i learned so far for case when...then ... statement, after the &quot;then&quot;  comes labeling (strings; all examples were using case when for binning). In your code i saw something else (dowlnds.downloads) ...what did you do here? (Is it also a labeling but with a downlds reference, or...?)",2022-01-30T11:04:25Z
Ugwx-4R9uSi5gZXZOw94AaABAg.9Xp18rfcwU19Xq0MnjAwxr,@stratascratch,Ugwx-4R9uSi5gZXZOw94AaABAg,2,30hS-MjpU6E,0,1,2022-01-30T20:16:49Z,"I&#39;m just referencing the ms_download_facts table. I&#39;m calling it downlds as you can see from the JOIN statement. So when I am referencing columns, SQL knows what table the column is in.",2022-01-30T20:16:49Z
Ugwx-4R9uSi5gZXZOw94AaABAg.9Xp18rfcwU19XrlsQz0n6Q,@diaconescutiberiu7535,Ugwx-4R9uSi5gZXZOw94AaABAg,2,30hS-MjpU6E,0,0,2022-01-31T12:39:57Z,"@@stratascratch Got it. I was able to replicate this on some other exercise. You are referencing the downloads (from the downlds) column so as SUM knows what to ... sum.<br>Can you please tell me, if we were to have 2 columns (something to sum and something to count) how would the case when look like? Would you just write 4 case when(s)? (2 with sums for paying &amp; non_paying and 2 with counts for paying &amp; non_paying)... or is it possible to optimize further into 2 case when(s)",2022-01-31T12:39:57Z
Ugwx-4R9uSi5gZXZOw94AaABAg.9Xp18rfcwU19XsEL0dIFo2,@stratascratch,Ugwx-4R9uSi5gZXZOw94AaABAg,2,30hS-MjpU6E,0,1,2022-01-31T16:57:23Z,@@diaconescutiberiu7535 It would be 4 cases just as you mentioned. You can&#39;t combine sum and counts when writing the cases. And you still need to split paying and non-paying so it&#39;s 4 cases in the end.,2022-01-31T16:57:23Z
Ugz-UKH-93k82JVZB6N4AaABAg,@jennajia9522,,1,30hS-MjpU6E,1,2,2022-01-29T03:54:58Z,"Hi Nate,  I cannot explain how much appreciation I have to you. Your video is extremely helpful and I have almost watch every single SQL one. All of the questions you explain in a clear way and the platform you built out is also the best. You are my best SQL teacher I want to say. Thank you so much. Keep up making these, we love it.",2022-01-29T03:54:58Z
Ugz-UKH-93k82JVZB6N4AaABAg.9XlgCacDx8S9XmtrPiCi2m,@stratascratch,Ugz-UKH-93k82JVZB6N4AaABAg,2,30hS-MjpU6E,0,1,2022-01-29T15:13:30Z,Thanks so much for the kind words! I&#39;ll keep making more content! You&#39;ll also see much more from my team. We&#39;re hoping to release a lot more videos this year!,2022-01-29T15:13:30Z
Ugy9q9aJyfJ_CvqJN-B4AaABAg,@alfatmiuzma,,1,30hS-MjpU6E,0,1,2022-01-11T18:12:21Z,"Loved this video‚ù§Ô∏è. Everything is on point! I really liked the optimization techniques using WITH, CASE statements! Looking forward to more videos on SQL. Thanks a tonüòä",2022-01-11T18:12:21Z
Ugyzcs8XP9kewlQQ8B54AaABAg,@dmitriyp3702,,1,30hS-MjpU6E,1,0,2022-01-09T20:36:23Z,"Hello. I prefer to use USING, instead of ON in INNER JOIN string. Do think it&#39;s a good idea?",2022-01-09T20:36:23Z
Ugyzcs8XP9kewlQQ8B54AaABAg.9WzyulBmZcE9X-AyQqHiag,@stratascratch,Ugyzcs8XP9kewlQQ8B54AaABAg,2,30hS-MjpU6E,0,1,2022-01-09T22:30:29Z,"Not many people I know use USING instead of ON so I can&#39;t really comment on if it&#39;s a good idea or not. Out of everyone I&#39;ve ever interviewed for a job, I don&#39;t think anyone has ever used USING.",2022-01-09T22:30:29Z
UgwChT8bglNiG5alyKl4AaABAg,@rakibraihan1572,,1,30hS-MjpU6E,0,0,2022-01-09T16:07:01Z,great,2022-01-09T16:07:01Z
UgzInLkPchk7BlYQ_-x4AaABAg,@StanleySI,,1,30hS-MjpU6E,2,2,2022-01-08T14:35:41Z,"Hi Nate, are you sure we can use where clause after group by clause ?  [HAVING vs WHERE: (<a href=""https://www.youtube.com/watch?v=30hS-MjpU6E&amp;t=12m48s"">12:48</a>)]",2022-01-08T14:35:41Z
UgzInLkPchk7BlYQ_-x4AaABAg.9Wwkq-ptgDw9Wx7FpUYoaI,@stratascratch,UgzInLkPchk7BlYQ_-x4AaABAg,2,30hS-MjpU6E,0,0,2022-01-08T18:00:19Z,Nope! I made a mistake! I should have ran the code to double check to see if it worked. All you have to do is place the WHERE clause before the GROUP BY and it&#39;s fixed.,2022-01-08T18:00:19Z
UgzInLkPchk7BlYQ_-x4AaABAg.9Wwkq-ptgDw9WxcObd5pMu,@StanleySI,UgzInLkPchk7BlYQ_-x4AaABAg,2,30hS-MjpU6E,0,0,2022-01-08T22:41:08Z,"@@stratascratch The question asked &#39;Include only records where non-paying customers have more downloads than paying customers&quot; - this may imply we need to use WHERE clause first to filter individual records, then aggregate results with GROUP BY date.",2022-01-08T22:41:08Z
Ugz9_5_yu_VAMTUIkMB4AaABAg,@tran3490,,1,30hS-MjpU6E,0,0,2022-01-08T04:46:54Z,Super helpful! Thank you!,2022-01-08T04:46:54Z
Ugy-OcGjTrA3d0ceZkt4AaABAg,@zukeplastic,,1,30hS-MjpU6E,0,0,2022-01-04T19:21:18Z,great content!,2022-01-04T19:21:18Z
UgxRu6_xsIBWzwKMLuB4AaABAg,@totleariss,,1,30hS-MjpU6E,1,0,2022-01-01T06:39:01Z,"Love this video! I‚Äôm at the point where I can get through a lot of the problems, but the code is a mess and way longer than the optimized solution. It‚Äôs been a challenge to ‚Äúthink in SQL‚Äù to write a better code to begin with.",2022-01-01T06:39:01Z
UgxRu6_xsIBWzwKMLuB4AaABAg.9Wdsigs7BsD9WfL4MgHDLA,@stratascratch,UgxRu6_xsIBWzwKMLuB4AaABAg,2,30hS-MjpU6E,0,1,2022-01-01T20:14:46Z,That&#39;ll come with time and experience. The initial code is always a mess. Don&#39;t worry too much about it. Just optimize at a later time!,2022-01-01T20:14:46Z
UgxYBfcFF__JN3YQ0T14AaABAg,@bien.papachin,,1,30hS-MjpU6E,1,3,2022-01-01T02:08:46Z,"great video but as a a rule of thumb run the SQL to see if it works, for example the &#39;where&#39; clause at the last select was left after the &#39;group by&#39; , nonetheless the explanation is great! thanks for the video... cheers",2022-01-01T02:08:46Z
UgxYBfcFF__JN3YQ0T14AaABAg.9WdOnLum3D39WfKzXr44G3,@stratascratch,UgxYBfcFF__JN3YQ0T14AaABAg,2,30hS-MjpU6E,0,2,2022-01-01T20:13:58Z,Yea you&#39;re right. I totally forgot to run the SQL query at the end to see if it worked. =),2022-01-01T20:13:58Z
UgxoX3XUuFTE1YigHrJ4AaABAg,@sagarmgandhi,,1,30hS-MjpU6E,0,1,2021-12-29T06:02:51Z,I think I could apply all this when I write my queries. Good session,2021-12-29T06:02:51Z
Ugz3flZjGJ1qpaC9Sdh4AaABAg,@ahyoungkim8256,,1,30hS-MjpU6E,0,0,2021-12-27T07:20:10Z,"i love this contents, this is so useful to look back our codes! thank you so much Nate !",2021-12-27T07:20:10Z
Ugx1A3Z_RGbMnFQ2X4t4AaABAg,@israelgonzalez677,,1,30hS-MjpU6E,0,0,2021-12-25T07:09:00Z,In love with your videos. I am enjoying them a lot. Greetings from MX! üòÄ,2021-12-25T07:09:00Z
UgzdQOL2lRZidkIivjF4AaABAg,@user-dl3qr5hm3t,,1,30hS-MjpU6E,2,4,2021-12-25T04:56:49Z,"Do we really need ‚Äúgroup by date, n_nonpaying, n_paying‚Äù in the last query? cte returns data already grouped by date: one row - one date. <br>PS. Surprised that it‚Äôs on Hard level. Seems pretty basic.",2021-12-25T04:56:49Z
UgzdQOL2lRZidkIivjF4AaABAg.9WMfT3IgZSE9WNugDDLwuX,@stratascratch,UgzdQOL2lRZidkIivjF4AaABAg,2,30hS-MjpU6E,0,3,2021-12-25T16:29:04Z,"The groupby only facilitates any de-duping needed. I wouldn&#39;t expect any duplicates so it&#39;s not needed. The groupby might just be an artifact of some other code that was refactored. Agree with you that it&#39;s a basic question that would be asked on an interview. Probably a medium level question tbh. But with all the CTEs with joins and case statement, we graded it hard.",2021-12-25T16:29:04Z
UgzdQOL2lRZidkIivjF4AaABAg.9WMfT3IgZSE9WO-SqsyXUm,@user-dl3qr5hm3t,UgzdQOL2lRZidkIivjF4AaABAg,2,30hS-MjpU6E,0,0,2021-12-25T17:19:32Z,"@@stratascratch Thanks for prompt response, Nate. Highly appreciate what you do on your channel.",2021-12-25T17:19:32Z
Ugw3HCkN19Mfx5r4l1B4AaABAg,@bandhammanikanta6302,,1,30hS-MjpU6E,0,1,2021-12-24T16:12:03Z,"I really appreciate your efforts, Nate.<br><br>but please do improve the title of the video according to the question,.",2021-12-24T16:12:03Z
UgwPikH5WH1EgdzW72R4AaABAg,@henrold6228,,1,30hS-MjpU6E,0,0,2021-12-23T17:13:02Z,nate you are my best friend,2021-12-23T17:13:02Z
UgymlxxMhGyZpZUzX3R4AaABAg,@dianadennis7225,,1,30hS-MjpU6E,0,0,2021-12-23T05:49:43Z,Such a great video!,2021-12-23T05:49:43Z
UgwJJzC-kZ7s1V1nF1J4AaABAg,@parantikaghosh4396,,1,30hS-MjpU6E,0,0,2021-12-23T00:02:27Z,"This is an amazing video, it is sooooooooooo helpful!! :)",2021-12-23T00:02:27Z
UgzYZIyEUD2-2A7HcZl4AaABAg,@abaji434,,1,30hS-MjpU6E,0,0,2021-12-22T13:16:41Z,great!,2021-12-22T13:16:41Z
Ugzu0C6pyoUOFIPMrX14AaABAg,@Sanatos98,,1,30hS-MjpU6E,0,1,2021-12-22T11:00:13Z,I love your videos! Hope you get the recognition you deserve brother,2021-12-22T11:00:13Z
UgyvkImgyMARiDU6MdB4AaABAg,@weiyang8650,,1,30hS-MjpU6E,2,7,2021-12-22T07:57:07Z,"thank you for your video, but WHERE should be put before the GROUP BY clause, right?",2021-12-22T07:57:07Z
UgyvkImgyMARiDU6MdB4AaABAg.9WFGhxhDAVr9WFbb38l_bC,@Sanatos98,UgyvkImgyMARiDU6MdB4AaABAg,2,30hS-MjpU6E,0,3,2021-12-22T11:08:25Z,Yes!,2021-12-22T11:08:25Z
UgyvkImgyMARiDU6MdB4AaABAg.9WFGhxhDAVr9WGCZjZYMxg,@stratascratch,UgyvkImgyMARiDU6MdB4AaABAg,2,30hS-MjpU6E,0,4,2021-12-22T16:40:09Z,Whoops. Yes! haha sorry about that.,2021-12-22T16:40:09Z
UgzqhTHlXUZ2pfDYfFl4AaABAg,@BBBBBBAAAl,,1,30hS-MjpU6E,0,0,2021-12-22T07:21:08Z,perfect,2021-12-22T07:21:08Z
UgzBic3dV-2aAjA6BMZ4AaABAg,@rishikeshbharti5064,,1,30hS-MjpU6E,0,0,2021-12-22T06:49:20Z,GREAT VIDEOüî•üî•,2021-12-22T06:49:20Z
UgyvjYuYzTneZ934OHV4AaABAg,@raxmatillomaribjonov3787,,1,30hS-MjpU6E,0,0,2021-12-22T03:35:00Z,thanksüëç,2021-12-22T03:35:00Z
Ugxeo1bezH5m4hyhLM54AaABAg,@kritiverma1342,,1,Bgpp99iz0I0,1,0,2022-03-20T22:50:15Z,"Hi Nate, I have recently come across your videos and you have knack for making a problem look simple. Why should you use &quot;Salary IN&quot; when there is going to be just one maximum salary? I guess IN is less efficient. <br><br>select worker_title<br>From title JOIN worker <br>	ON worker.worker_id= title. worker_refer_id<br>where salary = (select Max(salary) from worker)",2022-03-20T22:50:15Z
Ugxeo1bezH5m4hyhLM54AaABAg.9ZoSsU-vxEk9Zovc_zg0Cx,@stratascratch,Ugxeo1bezH5m4hyhLM54AaABAg,2,Bgpp99iz0I0,0,1,2022-03-21T03:10:14Z,Great approach. I&#39;m not sure which solution is more optimized since you have a JOIN and I have nests. But I like yours better =),2022-03-21T03:10:14Z
UgzQJdhKEuGXE2HEgrh4AaABAg,@melissamengxuanlu2216,,1,Bgpp99iz0I0,1,0,2022-01-07T23:57:37Z,"I could watch your video whole day long, I learned so much!! Thank you!!",2022-01-07T23:57:37Z
UgzQJdhKEuGXE2HEgrh4AaABAg.9WvBLoJ8pvG9kV-NG0rGdK,@stratascratch,UgzQJdhKEuGXE2HEgrh4AaABAg,2,Bgpp99iz0I0,0,0,2023-01-05T06:24:22Z,You are so welcome!,2023-01-05T06:24:22Z
UgyBhizd8T6BqLkx8q54AaABAg,@denisbaranoff,,1,Bgpp99iz0I0,0,0,2022-01-01T12:25:39Z,Excellent pitfall in Q ID 10031 &quot;Find the number of Bodegas outside of Spain by the country and region that produces wines with the blackberry taste&quot;,2022-01-01T12:25:39Z
UgwsBeRAyrEgGU-AkmN4AaABAg,@Dimalena1,,1,Bgpp99iz0I0,1,0,2021-12-03T21:06:44Z,Hello how i can connect with you i have few questions,2021-12-03T21:06:44Z
UgwsBeRAyrEgGU-AkmN4AaABAg.9VVkyeb1hR69VVtPLIj__M,@stratascratch,UgwsBeRAyrEgGU-AkmN4AaABAg,2,Bgpp99iz0I0,0,0,2021-12-03T22:20:25Z,"If it&#39;s about a technical question that&#39;s related to a question you found on the platform, then you should leave a comment in our discussion forum on our platform that&#39;s linked to the question. Otherwise, you can email me at nate@<a href=""http://stratascratch.com/"">stratascratch.com</a>. Thanks!",2021-12-03T22:20:25Z
UgySEI2-ttrD-Ze5SNN4AaABAg,@mic0917,,1,Bgpp99iz0I0,1,0,2021-12-02T18:46:27Z,"Hi Nate, just noticed free solutions are no longer avaible...",2021-12-02T18:46:27Z
UgySEI2-ttrD-Ze5SNN4AaABAg.9VSw7GM4f-O9VTHyVG9gzD,@stratascratch,UgySEI2-ttrD-Ze5SNN4AaABAg,2,Bgpp99iz0I0,0,0,2021-12-02T22:06:05Z,"If you go click on the Freemium button on the platform, you&#39;ll see the list of freemium questions. They&#39;re still available. This one might not be because sometimes we rotate the freemium questions so that people can try out more free questions.",2021-12-02T22:06:05Z
UgzY4-afVx6GRdiLjW54AaABAg,@rrdevyt,,1,Bgpp99iz0I0,2,1,2021-11-11T18:02:49Z,"Hi Nate, this video is fantastic. I&#39;ve been binging your videos over the past 6 months while prepping for Data Science/Eng roles and I&#39;m happy to say that as of Tuesday I accepted an offer from Facebook :). I refer people to this channel all the time and I believe its the best resource on YouTube for SQL interview prep. Keep the content coming!!",2021-11-11T18:02:49Z
UgzY4-afVx6GRdiLjW54AaABAg.9UbmRIUZ1VU9UbyDvpx6FC,@stratascratch,UgzY4-afVx6GRdiLjW54AaABAg,2,Bgpp99iz0I0,0,0,2021-11-11T19:45:51Z,Congrats! I&#39;m glad you found these videos useful. I&#39;m going to be expanding the videos to other types of data science questions in the near future. I know this won&#39;t help you since you already found a job =) but I hope you come back to this channel in future! Congrats on your new job again!,2021-11-11T19:45:51Z
UgzY4-afVx6GRdiLjW54AaABAg.9UbmRIUZ1VU9YzdAnoO93C,@agrim8863,UgzY4-afVx6GRdiLjW54AaABAg,2,Bgpp99iz0I0,0,0,2022-02-28T10:29:16Z,Robert Rutter can we connect on linkedin need some guidance.,2022-02-28T10:29:16Z
UgzTErbgqh87Yd-BUjB4AaABAg,@zynkers401,,1,Bgpp99iz0I0,0,0,2021-11-06T14:38:17Z,Nice hints here. Still training my skills so I can be able to jump into a Data Science role. Looking forward for more content. Thanks!,2021-11-06T14:38:17Z
UgxJglFS7ltknOXKTDh4AaABAg,@shobhamourya8396,,1,Bgpp99iz0I0,2,3,2021-11-06T05:28:06Z,"Using ranking window function: <br>select title, salary<br>from<br>(<br>    select worker_title title, salary,<br>      dense_rank() over(order by salary desc) sal_rank<br>      from worker w, title t<br>      where w.worker_id = t.worker_ref_id<br>      order by sal_rank<br>) a<br>where a.sal_rank = 1<br>order by title",2021-11-06T05:28:06Z
UgxJglFS7ltknOXKTDh4AaABAg.9UOZ5Fm5XL-9UQ1gLF910k,@stratascratch,UgxJglFS7ltknOXKTDh4AaABAg,2,Bgpp99iz0I0,0,1,2021-11-06T19:14:41Z,I like this ranking solution!,2021-11-06T19:14:41Z
UgxJglFS7ltknOXKTDh4AaABAg.9UOZ5Fm5XL-9V5kSkQ0TZL,@txreal2,UgxJglFS7ltknOXKTDh4AaABAg,2,Bgpp99iz0I0,0,0,2021-11-23T18:41:59Z,"SQL newbie here :) I was puzzling about the logic. And played around with it to make it clear to me.<br>Thanks.<br><br>SELECT title, salary<br>from<br>(<br>    SELECT worker_title AS title, salary,<br>    DENSE_RANK() OVER(ORDER BY salary desc) sal_rank<br>    FROM worker AS w, title AS t<br>    WHERE w.worker_id = t.worker_ref_id<br>    ORDER BY sal_rank<br>) <br>AS a<br>WHERE a.sal_rank = 1",2021-11-23T18:41:59Z
UgyyXE1I_2ve56w7TaZ4AaABAg,@SeattleDataGuy,,1,Bgpp99iz0I0,2,4,2021-11-05T22:44:05Z,Loving this concept! Thanks for going through and explaining some of these common SQL mistakes.,2021-11-05T22:44:05Z
UgyyXE1I_2ve56w7TaZ4AaABAg.9UNprCMK5jh9UQ1r9nRXeW,@stratascratch,UgyyXE1I_2ve56w7TaZ4AaABAg,2,Bgpp99iz0I0,0,2,2021-11-06T19:16:10Z,Thanks for watching! More to come...it&#39;s been hard filming consistently..=/,2021-11-06T19:16:10Z
UgyyXE1I_2ve56w7TaZ4AaABAg.9UNprCMK5jh9UQfoskY4PR,@SeattleDataGuy,UgyyXE1I_2ve56w7TaZ4AaABAg,2,Bgpp99iz0I0,0,1,2021-11-07T01:14:07Z,@@stratascratch I feel you! I have been putting out 30-40% less videos myself.,2021-11-07T01:14:07Z
UgyaWx92l6nhimFWDed4AaABAg,@istvandarvas3372,,1,Bgpp99iz0I0,2,0,2021-11-04T13:30:15Z,"First of all, I would like to say, I love your chanel and the site (StrataScrach) also. but I like to ask somthing about the block (<a href=""https://www.youtube.com/watch?v=Bgpp99iz0I0&amp;t=7m15s"">7:15</a>). in your final solution the subquery in the where clase is a nested loop on the dataset without the chance to use a proper index in the query planner I think, but please correct me if I wrong. So if you would use / apply the general rule - filter, aggregate, join in this case finding the max and filter on that condition and join only on those attributes - which is very similar to your first solution, would be better, would not be? :D thanks",2021-11-04T13:34:07Z
UgyaWx92l6nhimFWDed4AaABAg.9UKGfw9MZ_E9UKjjf79sol,@stratascratch,UgyaWx92l6nhimFWDed4AaABAg,2,Bgpp99iz0I0,0,1,2021-11-04T17:52:55Z,I actually think the most optimal solution would be to use a ranking so that we wouldn&#39;t need to use a nested loop. I wrote a blog post on this question with several different approaches/solutions and prefer the ranking solution over all. Let me know if that makes sense.,2021-11-04T17:52:55Z
UgyaWx92l6nhimFWDed4AaABAg.9UKGfw9MZ_E9UKr1_UlatK,@istvandarvas3372,UgyaWx92l6nhimFWDed4AaABAg,2,Bgpp99iz0I0,0,0,2021-11-04T18:56:39Z,"@@stratascratch I think you are right, thanks!",2021-11-04T18:56:39Z
UgzQGiCi9Q6VrCLcwFx4AaABAg,@user-ix7vs6vr7m,,1,Bgpp99iz0I0,3,0,2021-11-04T06:46:32Z,"I have a SQL interview tomorrow for an entry level role. Do you think they will ask about functions like UPDATE, DELETE, etc? I&#39;m not sure how to practice those on sites like StrataScratch and others.<br><br>Do you ask those questions when you interview people?",2021-11-04T06:46:32Z
UgzQGiCi9Q6VrCLcwFx4AaABAg.9UJYTxvfGSg9UKjuvpB1yu,@stratascratch,UgzQGiCi9Q6VrCLcwFx4AaABAg,2,Bgpp99iz0I0,0,1,2021-11-04T17:54:27Z,It depends on the role. I see it a lot in final round interviews when white boarding code. Update/deletes don&#39;t always happen in 1st round interviews in my experience. And I don&#39;t know any sites that can test for these concepts. It&#39;s something I&#39;d like to build on StrataScratch soon =) Good luck on your interview! Don&#39;t worry if it&#39;s the 1st round entry level role...it likely won&#39;t be asked!,2021-11-04T17:54:27Z
UgzQGiCi9Q6VrCLcwFx4AaABAg.9UJYTxvfGSg9ULhWrmARh_,@firstname4337,UgzQGiCi9Q6VrCLcwFx4AaABAg,2,Bgpp99iz0I0,0,1,2021-11-05T02:52:47Z,just do <br>delete * from table <br>and you&#39;ll be fine,2021-11-05T02:52:47Z
UgzQGiCi9Q6VrCLcwFx4AaABAg.9UJYTxvfGSg9V5kbIbNHef,@txreal2,UgzQGiCi9Q6VrCLcwFx4AaABAg,2,Bgpp99iz0I0,0,0,2021-11-23T18:43:18Z,How was the interview? Hope you nailed it.,2021-11-23T18:43:18Z
UgzslZdZnreAN57PtXl4AaABAg,@anandvyavahare2031,,1,Bgpp99iz0I0,3,5,2021-11-04T04:36:32Z,"To the point man! When you are in interview even a simple question can be tricky. I was asked a pretty straight forward question in an interview and I gave the ans. Interviewer then asked me how I can optimize the solution. And to me the ans I gave was easiest and fastest. The interviewer was amazing though. She gave quite a few hints. Then I was in a dilemma whether to use a subquery over join for performance. I knew join is better in performance, but just because I could not think of joining the tables optimally at the point I said I think subquery is faster. Needless to say I did not get to hear back from them :( <br>Btw the question is similar to the energy consumption question you asked to Tina in her course. And like you said, the interviewer would not explicitly mention the multiple highest scenario, which you also did not mention in that mock interview and Tina talked through the scenario and prior to that when I had attempted the question on my own I too did not consider the scenario. So yeah this video is most relatable to me... :) Keep posting such great content!",2021-11-04T04:36:32Z
UgzslZdZnreAN57PtXl4AaABAg.9UJJap0DTwO9UKjXYq6uF0,@stratascratch,UgzslZdZnreAN57PtXl4AaABAg,2,Bgpp99iz0I0,0,0,2021-11-04T17:51:07Z,Thanks for watching and sorry for your interview experience! Better luck next time. And thanks for watching/purchasing Tina&#39;s course. You&#39;re the 1st person that tell me that you enjoyed her course!,2021-11-04T17:51:07Z
UgzslZdZnreAN57PtXl4AaABAg.9UJJap0DTwO9UKsbFLepy5,@anandvyavahare2031,UgzslZdZnreAN57PtXl4AaABAg,2,Bgpp99iz0I0,0,0,2021-11-04T19:10:24Z,@@stratascratch Yes one interview taught me a lot!! So surely growing with such amazing videos. Yeah I just started with the course. It is pretty amazing. Very practical. <br>You and Tina have best SQL related videos out there. So keep rocking! Thank you again!,2021-11-04T19:10:24Z
UgzslZdZnreAN57PtXl4AaABAg.9UJJap0DTwO9ULDSoZ5P1z,@stratascratch,UgzslZdZnreAN57PtXl4AaABAg,2,Bgpp99iz0I0,0,1,2021-11-04T22:21:21Z,@@anandvyavahare2031 Thank you for your support and good luck!,2021-11-04T22:21:21Z
UgwCriGQbDrA4iRDn394AaABAg,@dianadennis7225,,1,Bgpp99iz0I0,0,0,2021-11-04T03:09:20Z,would you cover regular expressions?,2021-11-04T03:09:20Z
UgxKzeif3a3NA7H-7dF4AaABAg,@oneminutedaily6394,,1,uY2wfR8Dkqo,2,0,2022-02-10T09:23:52Z,May I know how do I prepare well for Product Questions (product management)? Are there any more links/guides on this ?<br><br>Appreciate it ! :),2022-02-10T09:25:12Z
UgxKzeif3a3NA7H-7dF4AaABAg.9YGAO77SYBI9YGt3IjQPGC,@stratascratch,UgxKzeif3a3NA7H-7dF4AaABAg,2,uY2wfR8Dkqo,0,1,2022-02-10T16:02:59Z,Check out StrataScratch. There are a lot of guides on the blog!,2022-02-10T16:02:59Z
UgxKzeif3a3NA7H-7dF4AaABAg.9YGAO77SYBI9YIZm_7Oqua,@oneminutedaily6394,UgxKzeif3a3NA7H-7dF4AaABAg,2,uY2wfR8Dkqo,0,0,2022-02-11T07:44:17Z,@@stratascratch Cool ! Thanks for sharing :),2022-02-11T07:44:17Z
Ugx3fXOkOKcx7RoA93V4AaABAg,@AH-us8nw,,1,uY2wfR8Dkqo,1,0,2021-12-15T22:03:19Z,Where is the Long-term preparation plan (mentioned by Ben) located ?,2021-12-15T22:03:19Z
Ugx3fXOkOKcx7RoA93V4AaABAg.9Vzkzbem2O19W-Q0hroRtG,@stratascratch,Ugx3fXOkOKcx7RoA93V4AaABAg,2,uY2wfR8Dkqo,0,0,2021-12-16T04:10:37Z,hmm..not sure? Ask him on his channel!,2021-12-16T04:10:37Z
Ugw2NSu2t8hGKlHCXYJ4AaABAg,@hruthikkcchand9993,,1,uY2wfR8Dkqo,1,0,2021-12-06T09:21:06Z,"Hi sir, is there a chance for people from non-tech field fetch a job as a data scientist? It will be very helpful if you can give few tips about this issue...",2021-12-06T09:21:06Z
Ugw2NSu2t8hGKlHCXYJ4AaABAg.9VbDazq3POI9Vd-mUMjTGV,@stratascratch,Ugw2NSu2t8hGKlHCXYJ4AaABAg,2,uY2wfR8Dkqo,0,1,2021-12-07T01:58:48Z,Yea absolutely. There are many paths. Some that I know are people that get a job at a company with a data science team and gradually learns how to be a ds while working on that company and then transferirng to that team. Others go through bootcamps or school again. You can go and be self taught so long as you build projects and have a portfolio. Practicing and doing projects is the best way to learn,2021-12-07T01:58:48Z
UgxFXKYJUxU6e8bNsGt4AaABAg,@taki1jeden1,,1,uY2wfR8Dkqo,1,1,2021-11-25T22:09:09Z,"A two week ago I interviewed for a data scientist intern position (and got a paid internship ;&gt; ). I was immersed in DS and ML for the previous 3-4 months. While learning, I focused most on regression and classification problems. I didn&#39;t have any end-to-end project, but I could still talk about the algorithms I used and the problems I faced and that was the most important thing that helped me pass the recruitment (as confirmed by the interviewer).",2021-11-25T22:09:09Z
UgxFXKYJUxU6e8bNsGt4AaABAg.9VBGkm5zLCw9VDOPMOz_Bd,@stratascratch,UgxFXKYJUxU6e8bNsGt4AaABAg,2,uY2wfR8Dkqo,0,0,2021-11-26T17:54:28Z,Congrats on the internship =),2021-11-26T17:54:28Z
UgxvhSPfT3d45WVldKN4AaABAg,@ruima721,,1,uY2wfR8Dkqo,0,0,2021-11-20T13:55:12Z,Thanks Nate and all collaborators. These tips are clear and helpful.,2021-11-20T13:55:12Z
UgzyGHFbkACF3QkzMtB4AaABAg,@IngloriousDotUs,,1,uY2wfR8Dkqo,1,0,2021-09-13T15:13:27Z,Great video,2021-09-13T15:13:27Z
UgzyGHFbkACF3QkzMtB4AaABAg.9SEZ9HjgHX09SEqOhhHIvw,@stratascratch,UgzyGHFbkACF3QkzMtB4AaABAg,2,uY2wfR8Dkqo,0,0,2021-09-13T17:52:51Z,Thank you for watching!,2021-09-13T17:52:51Z
Ugy5tXR4DrglyKKNyad4AaABAg,@arunvaibhav5059,,1,uY2wfR8Dkqo,0,0,2021-08-10T05:17:21Z,"Videos are helpful, but please increase the volume :) Thanks!",2021-08-10T05:17:21Z
Ugw6OQwvuKUV-R3Uab14AaABAg,@nadyaam.1139,,1,uY2wfR8Dkqo,1,0,2021-08-03T12:59:34Z,üêà so excited!! Mine starts in 6hrs.,2021-08-03T12:59:34Z
Ugw6OQwvuKUV-R3Uab14AaABAg.9Q_kEmZ4KYo9Qa0fXQUx8A,@stratascratch,Ugw6OQwvuKUV-R3Uab14AaABAg,2,uY2wfR8Dkqo,0,0,2021-08-03T15:31:54Z,Good luck!,2021-08-03T15:31:54Z
Ugz1hGMGrKGAlj1-nOd4AaABAg,@candidlyvivian,,1,uY2wfR8Dkqo,1,0,2021-08-01T13:54:43Z,"Hi Nate (and everyone else involved!) Loved this video, so informative and can‚Äôt wait to see more! a fellow data scientist and small YouTuber here üòä",2021-08-01T13:54:43Z
Ugz1hGMGrKGAlj1-nOd4AaABAg.9QVgxx4S7hX9QWKemnaVFP,@stratascratch,Ugz1hGMGrKGAlj1-nOd4AaABAg,2,uY2wfR8Dkqo,0,0,2021-08-01T19:50:22Z,"Hi! Very cool channel. You got a sub! I see that you have some DS interview and career content. If you ever need any content or help, let me know. Happy to help in any way.",2021-08-01T19:50:22Z
Ugzr9XB6G7Z5vbjHu-Z4AaABAg,@TinaHuang1,,1,uY2wfR8Dkqo,3,11,2021-07-28T23:41:08Z,Yay!!! Awesome video - loved the collab ‚ù§Ô∏è,2021-07-28T23:41:08Z
Ugzr9XB6G7Z5vbjHu-Z4AaABAg.9QMRtTIEiaH9QMoLi5bsD4,@stratascratch,Ugzr9XB6G7Z5vbjHu-Z4AaABAg,2,uY2wfR8Dkqo,0,0,2021-07-29T03:06:06Z,Great collab!,2021-07-29T03:06:06Z
Ugzr9XB6G7Z5vbjHu-Z4AaABAg.9QMRtTIEiaH9QNcnnDGXlb,@AndreTheBarbadian,Ugzr9XB6G7Z5vbjHu-Z4AaABAg,2,uY2wfR8Dkqo,0,1,2021-07-29T10:44:27Z,So well prepared with a list. You are hired!,2021-07-29T10:44:27Z
Ugzr9XB6G7Z5vbjHu-Z4AaABAg.9QMRtTIEiaH9QOqROywzwC,@ibi_1,Ugzr9XB6G7Z5vbjHu-Z4AaABAg,2,uY2wfR8Dkqo,0,1,2021-07-29T22:02:50Z,Beep Beep is the star of this video :3 ‚ù§Ô∏è Sorry Tina you&#39;ve been overshadowed,2021-07-29T22:02:50Z
UgyzpyRZXG-jVpbRVTd4AaABAg,@learnenglishwithdania8977,,1,uY2wfR8Dkqo,0,0,2021-07-27T23:51:24Z,Perfect Thanks üòä <br>I actually I&#39;m studying data engineer and I have final project and I need someone help me I will be thankful üôè,2021-07-27T23:51:24Z
Ugz7HqAuqiRPMjQnVYR4AaABAg,@tejasphirke3436,,1,uY2wfR8Dkqo,0,0,2021-07-27T11:53:46Z,This is great!!,2021-07-27T11:53:46Z
UgwK06UzIv-aUd2bbVB4AaABAg,@rrdevyt,,1,uY2wfR8Dkqo,3,0,2021-07-27T02:27:15Z,"How would you compare internship level interviews and full-time position interviews? Should interns focus more on specific skillsets? I&#39;ve got internship interviews at Facebook coming next month and I&#39;m trying to figure the best way to divide my time. I&#39;m confident in SQL and the technical side, but my product sense and stats are not as strong.",2021-07-27T02:27:15Z
UgwK06UzIv-aUd2bbVB4AaABAg.9QHaJTPF_QT9QHiY14HTuQ,@stratascratch,UgwK06UzIv-aUd2bbVB4AaABAg,2,uY2wfR8Dkqo,0,1,2021-07-27T03:39:09Z,"That&#39;s a great question. There isn&#39;t much difference between internships and FTE roles. You need a strong coding foundation as well as product sense and stats (FB loveesss to give Bayes Theorem questions =). But be prepared to also talk about your projects -- the technical underpinnings and the impact. Internships and entry-level full-time positions are really about measuring potential. Of course, the technical foundation needs to be there, but the hiring managers need to feel convinced that you&#39;re someone that can make an impact, be a great team player, and be okay learning on the job. You can convince them through your stories and enthusiasm.",2021-07-27T03:39:09Z
UgwK06UzIv-aUd2bbVB4AaABAg.9QHaJTPF_QT9QHiZEme-DD,@stratascratch,UgwK06UzIv-aUd2bbVB4AaABAg,2,uY2wfR8Dkqo,0,1,2021-07-27T03:39:19Z,Oh and good luck!,2021-07-27T03:39:19Z
UgwK06UzIv-aUd2bbVB4AaABAg.9QHaJTPF_QT9QHn1nyYHA0,@rrdevyt,UgwK06UzIv-aUd2bbVB4AaABAg,2,uY2wfR8Dkqo,0,1,2021-07-27T04:18:26Z,"@@stratascratch Thanks so much for this reply. Your videos and StrataScratch have been invaluable in my prep for the coding, technical, and communication portions. I&#39;ve found the content on SS is far more useful and relevant than a lot of the content on Leetcode/Hackerrank. I&#39;ll definitely be doing more prep on product sense, stats (Bayes Theorem included), and doing some recall on my past projects. Thanks for the good luck wishes! If I&#39;m successful I&#39;ll certainly drop a comment and let you know :)",2021-07-27T04:18:26Z
UgyYKElYzPJ__JhDX7t4AaABAg,@vivianamarquez5932,,1,uY2wfR8Dkqo,1,0,2021-07-26T20:42:52Z,"I really love this video, is a complete guide. I&#39;m a new subscriber!! And the subtitles are the cherry on top. üòÑ",2021-07-26T20:42:52Z
UgyYKElYzPJ__JhDX7t4AaABAg.9QGyu1jk6N_9QHEMVbH3mO,@stratascratch,UgyYKElYzPJ__JhDX7t4AaABAg,2,uY2wfR8Dkqo,0,0,2021-07-26T23:06:42Z,Thank you for subscribing! Hope you find the other videos useful too!,2021-07-26T23:06:42Z
Ugw2V5-KubP8w2ZPMox4AaABAg,@LukeBarousse,,1,uY2wfR8Dkqo,1,2,2021-07-26T18:32:00Z,Thanks Nate!  This was so much fun to collab on!!!,2021-07-26T18:32:54Z
Ugw2V5-KubP8w2ZPMox4AaABAg.9QGjvbFBvtu9QGy3R1SBA8,@stratascratch,Ugw2V5-KubP8w2ZPMox4AaABAg,2,uY2wfR8Dkqo,0,1,2021-07-26T20:35:32Z,Great collab! Can&#39;t wait until the next video!,2021-07-26T20:35:32Z
UgxAgrTUBMfpc7qLBgB4AaABAg,@SeattleDataGuy,,1,uY2wfR8Dkqo,1,5,2021-07-26T15:08:02Z,Thanks for setting this up Nate! It was great to be part of this interview prep video!,2021-07-26T15:08:02Z
UgxAgrTUBMfpc7qLBgB4AaABAg.9QGN_cfdCDk9QGPzFtYHeO,@stratascratch,UgxAgrTUBMfpc7qLBgB4AaABAg,2,uY2wfR8Dkqo,0,0,2021-07-26T15:29:00Z,Thanks for being part of it!,2021-07-26T15:29:00Z
UgxmuMYYN1TsF0P3Re54AaABAg,@strategy_gal,,1,uY2wfR8Dkqo,1,6,2021-07-26T14:56:20Z,I love how you&#39;ve broken down each of these tips in this video. It&#39;s absolutely brilliant! I&#39;m sure it&#39;s super helpful to those who are preparing for their DS interviews!üôå,2021-07-26T14:56:20Z
UgxmuMYYN1TsF0P3Re54AaABAg.9QGMEzaP16q9QGPx_U-LnL,@stratascratch,UgxmuMYYN1TsF0P3Re54AaABAg,2,uY2wfR8Dkqo,0,0,2021-07-26T15:28:46Z,Thanks for watching! Glad you enjoyed the video!,2021-07-26T15:28:46Z
UgwPm0YEMSnL_zVYiQF4AaABAg,@96merluzzo,,1,uY2wfR8Dkqo,1,3,2021-07-26T12:39:37Z,"First I click like, then I watch",2021-07-26T12:39:37Z
UgwPm0YEMSnL_zVYiQF4AaABAg.9QG6aeS5cNh9QGKA4Ie3GM,@stratascratch,UgwPm0YEMSnL_zVYiQF4AaABAg,2,uY2wfR8Dkqo,0,1,2021-07-26T14:38:11Z,Thank you as always!,2021-07-26T14:38:11Z
Ugz-VHWnc-EQoFESTkJ4AaABAg,@theghostwhowalk,,1,uY2wfR8Dkqo,1,3,2021-07-26T06:39:19Z,Your series is amazing! Keep great videos coming :),2021-07-26T06:39:19Z
Ugz-VHWnc-EQoFESTkJ4AaABAg.9QFTMlZz2dk9QGK9CZ_2KP,@stratascratch,Ugz-VHWnc-EQoFESTkJ4AaABAg,2,uY2wfR8Dkqo,0,0,2021-07-26T14:38:04Z,Thanks for watching!,2021-07-26T14:38:04Z
Ugwb7zuxktlxPjDFNkd4AaABAg,@kamaloveromance1448,,1,uY2wfR8Dkqo,0,1,2021-07-26T03:53:37Z,Good Job Nate and Others.,2021-07-26T03:53:37Z
UgyFEumlaeL1B2bYp6p4AaABAg,@ass2412,,1,RgKMbpEylWA,1,0,2023-08-07T14:31:39Z,"So there is no data structures and algorithms? If yes, what topics should be covered",2023-08-07T14:31:39Z
UgyFEumlaeL1B2bYp6p4AaABAg.9t6uFSrwvdy9t7PC6cy7I_,@stratascratch,UgyFEumlaeL1B2bYp6p4AaABAg,2,RgKMbpEylWA,0,1,2023-08-07T19:10:49Z,Not all the time. But sometimes there are DSA questions. You will need to ask the hiring manager.,2023-08-07T19:10:49Z
UgwNs0ny1fjKK7hAN3h4AaABAg,@ashaysingh680,,1,RgKMbpEylWA,0,0,2022-05-11T23:23:09Z,Please try for Microsoft too,2022-05-11T23:23:09Z
Ugwio540OO-facGZRVF4AaABAg,@starship1701,,1,RgKMbpEylWA,0,0,2022-02-10T06:45:16Z,"Don&#39;t you mean MAANG?<br>Just kidding, thanks for the quality content Nate.",2022-02-10T06:45:16Z
Ugx0BE8d_D-Of-OlOAF4AaABAg,@tanmaysinghi1868,,1,RgKMbpEylWA,0,1,2021-12-22T09:34:58Z,Doesn&#39;t this fall under unfair graph representation ? The scales of both the graphs should be same when being compared.,2021-12-22T09:34:58Z
UgyElIoyVUdjXnv9GDN4AaABAg,@lettucechunks,,1,RgKMbpEylWA,1,1,2021-06-11T02:32:03Z,"Dear Nate, thank you for the wonderful video! I have one quick question. I generally tend to struggle with SQL interviews where interviewers ask to write queries on coderpad, without running them. Writing a complex query, end to end, without viewing the derived tables is a little challenging for me. Could you possibly suggest a way to overcome this? Your feedback would be of great help!",2021-06-11T02:32:03Z
UgyElIoyVUdjXnv9GDN4AaABAg.9OR9IJ4iq0q9OSm6GMzbgH,@stratascratch,UgyElIoyVUdjXnv9GDN4AaABAg,2,RgKMbpEylWA,0,1,2021-06-11T17:39:10Z,Most of my newest videos where I walk through a coding question will talk about how to answer the question without looking at the values in the dataset so definitely watch some of those videos and let me know what you think. I can always iterate and improve based on your feedback. But I totally understand your problem. Most interviews aren&#39;t using executable databases.,2021-06-11T17:39:10Z
Ugw2wZuKgQkW7EElbOh4AaABAg,@modhua4497,,1,RgKMbpEylWA,1,0,2021-06-10T00:19:15Z,You are absolutely right. More SQL questions from FANG companies,2021-06-10T00:19:15Z
Ugw2wZuKgQkW7EElbOh4AaABAg.9OOLInftDFv9OPwXtoN9xY,@modhua4497,Ugw2wZuKgQkW7EElbOh4AaABAg,2,RgKMbpEylWA,0,0,2021-06-10T15:12:36Z,I really learn a lots from your SQL video demo,2021-06-10T15:12:36Z
UgzajN-g5W3Ad1pcejJ4AaABAg,@ranjithsuram7919,,1,RgKMbpEylWA,1,2,2021-06-04T14:21:14Z,"Nate you are truly awesome, i was able to crack my SQL interview by watching your videos. Respect üôèüèΩ",2021-06-04T14:21:14Z
UgzajN-g5W3Ad1pcejJ4AaABAg.9OAOtM9OqwV9OAlkF0QHUu,@stratascratch,UgzajN-g5W3Ad1pcejJ4AaABAg,2,RgKMbpEylWA,0,0,2021-06-04T17:49:42Z,That&#39;s great! I&#39;m glad you found the videos useful.,2021-06-04T17:49:42Z
UgzK95u4bK6trqKRQdZ4AaABAg,@SoumyajitGanguly_ALKZ,,1,RgKMbpEylWA,1,0,2021-05-30T13:00:29Z,Uber DS interview is quite close to Facebook - more of product.,2021-05-30T13:00:29Z
UgzK95u4bK6trqKRQdZ4AaABAg.9NyNf_crhK79Nz5Jnp3Fb5,@stratascratch,UgzK95u4bK6trqKRQdZ4AaABAg,2,RgKMbpEylWA,0,0,2021-05-30T19:39:19Z,Yup that&#39;s true. I&#39;ve also heard that Uber and Lyft interviews are a lot of optimization questions since it&#39;s always about trying to optimize rider/driver experience and get wait times down. So expect a lot of those types of questions.,2021-05-30T19:39:19Z
Ugzt5UKXWBKCn1f3G2d4AaABAg,@oscartran9009,,1,RgKMbpEylWA,1,1,2021-05-24T18:36:48Z,Thank you for sharing your insights Nate. Very useful knowledge to utilize.,2021-05-24T18:36:48Z
Ugzt5UKXWBKCn1f3G2d4AaABAg.9NjXOd_mlJ09NjrA1c69eA,@stratascratch,Ugzt5UKXWBKCn1f3G2d4AaABAg,2,RgKMbpEylWA,0,1,2021-05-24T21:38:18Z,Thanks so much. I like adding some information about my research on interviews and careers that doesn&#39;t involve coding. So glad you are liking these videos.,2021-05-24T21:38:18Z
UgzR_D-Nx1Kj6ccUNRZ4AaABAg,@karanbhosale5390,,1,RgKMbpEylWA,0,1,2021-05-24T05:52:22Z,You worked for FAANG nate as data scientist?,2021-05-24T05:52:22Z
UgzHSCPZrTUXMNMdHXh4AaABAg,@YUVRAJSINGH-iz9gt,,1,giGIPINLqJs,1,0,2022-06-25T16:47:26Z,Please share the coupon for buying a lifetime subscription course,2022-06-25T16:47:26Z
UgzHSCPZrTUXMNMdHXh4AaABAg.9ch_SzU6vL_9cl5TtsPm--,@stratascratch,UgzHSCPZrTUXMNMdHXh4AaABAg,2,giGIPINLqJs,0,0,2022-06-27T01:33:38Z,ss15 gives you 15% the lifetime plan!,2022-06-27T01:33:38Z
UgwhrFh61XuG9wDuWxh4AaABAg,@ganeshhegde4049,,1,giGIPINLqJs,1,0,2022-01-07T04:34:28Z,Love your content and StrataScratch platform. It&#39;s helping in my interview preparation. :),2022-01-07T04:34:28Z
UgwhrFh61XuG9wDuWxh4AaABAg.9Wt6Eh5Yw879kV-Nrpq6Nc,@stratascratch,UgwhrFh61XuG9wDuWxh4AaABAg,2,giGIPINLqJs,0,0,2023-01-05T06:24:27Z,Happy to help!,2023-01-05T06:24:27Z
Ugxi7sMiExzm8hWKmfB4AaABAg,@ajaydeepak8812,,1,giGIPINLqJs,0,3,2021-09-24T19:11:56Z,This is youtube gold . . . Hats off Nate .,2021-09-24T19:11:56Z
UgzYvaiPLG1lvIaTlBl4AaABAg,@aymandark8246,,1,giGIPINLqJs,0,2,2021-09-16T02:57:46Z,"Hi Nate, Thank you for this great content.<br>I am trying to find an internship for summer 2022, Can you please tell me if there is any difference between a Data Science internship vs a Machine learning internship interviews?",2021-09-16T02:57:58Z
UgzLf6LSjeRlcMvkWiF4AaABAg,@kartiikss8515,,1,giGIPINLqJs,2,1,2021-05-24T23:53:25Z,Love your content Nate! Would it be possible for you to include SQL and Product sense questions from Snap Inc?,2021-05-24T23:53:25Z
UgzLf6LSjeRlcMvkWiF4AaABAg.9Nk5cZFyM3O9Nlnrwnb8yr,@stratascratch,UgzLf6LSjeRlcMvkWiF4AaABAg,2,giGIPINLqJs,0,1,2021-05-25T15:47:58Z,Hey absolutely! I&#39;ll try to find some and post it on the platform in 1-2 months. I&#39;ll also try to make some coding videos from those questions if I can find them.,2021-05-25T15:47:58Z
UgzLf6LSjeRlcMvkWiF4AaABAg.9Nk5cZFyM3O9NmwjRx-MOB,@kartiikss8515,UgzLf6LSjeRlcMvkWiF4AaABAg,2,giGIPINLqJs,0,0,2021-05-26T02:24:41Z,@@stratascratch Thank you so much!,2021-05-26T02:24:41Z
Ugydc-MNGrl-79JhVxl4AaABAg,@vrajpatel8256,,1,giGIPINLqJs,1,5,2021-05-13T08:19:10Z,Great work. StrataScratch is great for SQL and non-coding questions.,2021-05-13T08:19:10Z
Ugydc-MNGrl-79JhVxl4AaABAg.9NH5yHOTCGU9NHzxkYE3lL,@stratascratch,Ugydc-MNGrl-79JhVxl4AaABAg,2,giGIPINLqJs,0,2,2021-05-13T16:37:10Z,Thanks for watching! It&#39;s one of my few non-coding videos...let&#39;s see if people like it!,2021-05-13T16:37:10Z
UgyW07VbSyESyZF18El4AaABAg,@anuragsingh4766,,1,giGIPINLqJs,1,1,2021-05-13T07:08:10Z,üëçüëçüëçüëçüëç,2021-05-13T07:08:10Z
UgyW07VbSyESyZF18El4AaABAg.9NGyqKFV5Dy9NHzvD6yFvl,@stratascratch,UgyW07VbSyESyZF18El4AaABAg,2,giGIPINLqJs,0,0,2021-05-13T16:36:49Z,Thanks for watching!,2021-05-13T16:36:49Z
UgxG_-0Vvw2KIi1f3eR4AaABAg,@ozan4702,,1,tNXliLTlrV8,2,0,2023-08-30T17:03:14Z,I cant even check solution without premium. Checking solution requires premium. Are you for real guys? If I cant check solution why am i wasting my time.,2023-08-30T17:03:14Z
UgxG_-0Vvw2KIi1f3eR4AaABAg.9u2OsmO06A49u2PJu9yvDB,@stratascratch,UgxG_-0Vvw2KIi1f3eR4AaABAg,2,tNXliLTlrV8,0,0,2023-08-30T17:07:04Z,Why not just run the code and manually check the output?,2023-08-30T17:07:04Z
UgxG_-0Vvw2KIi1f3eR4AaABAg.9u2OsmO06A49uOsiMWZdD0,@ozan4702,UgxG_-0Vvw2KIi1f3eR4AaABAg,2,tNXliLTlrV8,0,0,2023-09-08T10:35:59Z,@@stratascratch That wont check for edge cases.,2023-09-08T10:35:59Z
Ugy_S-efwNS1qqUUO-94AaABAg,@djjiang3718,,1,tNXliLTlrV8,1,0,2023-02-04T01:42:58Z,Thank you Nate! And love your content and love your web site!,2023-02-04T01:42:58Z
Ugy_S-efwNS1qqUUO-94AaABAg.9lgk0P1dyLL9lhbg2yW-78,@stratascratch,Ugy_S-efwNS1qqUUO-94AaABAg,2,tNXliLTlrV8,0,1,2023-02-04T09:49:23Z,Glad you enjoy it!,2023-02-04T09:49:23Z
UgxNAhA284dWFEQwzaB4AaABAg,@christiankuntz2150,,1,tNXliLTlrV8,1,1,2022-12-21T04:01:32Z,"Nate, would it also work to select user_id, session_type, MIN(session_start) to identify each users first session type instead of using the rank window function?",2022-12-21T04:01:32Z
UgxNAhA284dWFEQwzaB4AaABAg.9jt75zKDHYM9kEazl9iLMS,@stratascratch,UgxNAhA284dWFEQwzaB4AaABAg,2,tNXliLTlrV8,0,0,2022-12-29T21:34:43Z,Yes it would work but rank() is more appropriate and more explicit.,2022-12-29T21:34:43Z
Ugyh9OWsv0-cblC_VAh4AaABAg,@gotchihaeyo1825,,1,tNXliLTlrV8,0,0,2022-09-20T23:45:25Z,"Can I use <br>(Select  User_ID, MIN( session_start)<br>From twitch<br>Where session_type = &#39; viewer &#39;)<br>As a sub query instead of rank( )????",2022-09-20T23:46:30Z
Ugxe-Rhkvw_lKNxd84l4AaABAg,@chaos8514,,1,tNXliLTlrV8,0,1,2022-08-17T16:24:25Z,When i first saw it 6-7 months ago got scared and demotivated to shift job. But recently i started practicing sql basics and now Alhamdulillah this video does not look that hard. It gave me great confidence thanks for great video. And can you share your insta id please,2022-08-17T16:24:25Z
UgxN1uz0C4TSLsi5YD94AaABAg,@teddy911,,1,tNXliLTlrV8,1,0,2022-07-08T06:50:43Z,"this coding format is so hard to read, don&#39;t just some how indent the line as you feel like it, looks terrible guys",2022-07-08T06:50:43Z
UgxN1uz0C4TSLsi5YD94AaABAg.9dCzVrW38p39dDyZWLhVDn,@stratascratch,UgxN1uz0C4TSLsi5YD94AaABAg,2,tNXliLTlrV8,0,1,2022-07-08T16:01:43Z,"We typically put 4 spaces for indentations. This code was actually copied from a site that helps format code. It looks like code I&#39;ve seen in industry and follows best practices. Although, I see 1-2 lines that have too much spacing, all the other lines look okay to me. Maybe there&#39;s just too many nested parts that could be refactored into CTEs?",2022-07-08T16:01:43Z
UgxdKmUS0QNo9PYlgfB4AaABAg,@curio_cureator,,1,tNXliLTlrV8,1,1,2022-03-02T17:22:38Z,"Great video, and your content has been a huge help preparing for my BIE interview. <br><br>Quick question, let&#39;s say there was more than one session created at the same time due to dirty data. My intuition is to use ROW_NUMBER() then to get a single first session no matter what, ordering by another column to break weird ties based on assumptions about which session id tends to be the &quot;right&quot; one when duplicated. Am I in the right ballpark here, or am I thinking about this wrong?",2022-03-02T17:22:38Z
UgxdKmUS0QNo9PYlgfB4AaABAg.9Z4X47KaWfh9ZAPaTwn-lB,@stratascratch,UgxdKmUS0QNo9PYlgfB4AaABAg,2,tNXliLTlrV8,0,2,2022-03-05T00:12:43Z,You&#39;re on the right track. There&#39;s no right answer on how to process dirty data. It&#39;s more of an agreement with the interviewer. Once you both agree then start solving the question.,2022-03-05T00:12:43Z
UgwYUqBL96Ej55dRYcR4AaABAg,@yuthpatirathi2719,,1,tNXliLTlrV8,0,0,2022-02-27T20:40:42Z,Thank you Nate,2022-02-27T20:40:42Z
UgzN7iETnTq4wJ2xlPt4AaABAg,@diaconescutiberiu7535,,1,tNXliLTlrV8,1,0,2022-01-30T10:40:13Z,"Is there a video with this question solved using CTEs and temp tables? I&#39;ve just started SQL and for now it&#39;s not clear to me when to use subqueries, cte or temp tables. Would love to see the same problem solved in 3 ways. It will help with improving the logic as well",2022-01-30T10:40:13Z
UgzN7iETnTq4wJ2xlPt4AaABAg.9XozNeMOtTV9Xq09kzAfNT,@stratascratch,UgzN7iETnTq4wJ2xlPt4AaABAg,2,tNXliLTlrV8,0,0,2022-01-30T20:15:02Z,"There isn&#39;t a video specifically that uses CTEs and temp tables to answer this question. But you can use them for sure to do so. I would go on the platform and try to solve this question using CTEs. To address your last comment, you can use subqueries, ctes, and temp tables interchangeably. It really just depends on the use case and how much data is in our tables that will dictate what you choose.",2022-01-30T20:15:02Z
UgzD0iRr37Hq7fKqCYF4AaABAg,@prashanthigupta,,1,tNXliLTlrV8,0,0,2022-01-30T00:47:21Z,Thank you so much for your videos. I learned a lot üëçüèªüëçüèªüëçüèª,2022-01-30T00:47:21Z
UgwtzZOqHqNlzfuPMXx4AaABAg,@MirasAdilov,,1,tNXliLTlrV8,0,0,2022-01-19T15:17:14Z,Thank you!,2022-01-19T15:17:14Z
UgxY4uEpOEAZfOtbKO14AaABAg,@Innovade,,1,tNXliLTlrV8,0,1,2021-08-06T23:46:07Z,"Nate, thank you for these videos. Extremely helpful.",2021-08-06T23:46:07Z
UgzJ65QSJPMHU0IVdO14AaABAg,@TheBartboy007,,1,tNXliLTlrV8,1,1,2021-06-21T01:35:10Z,"Gave it a crack before watching your solution and now I hate myself for solving it like this, wish you could just throw in a HAVING session_type = &#39;viewer&#39; in the first cte and mysql would know to select the session_type associated with the row of the MIN(session_start), but unfortunately that&#39;s not how it works so that&#39;s where the second cte comes from. I realized I should probably use a window function after I started to pursue my solution, but I wanted to see if I could do without one: <br><br>initial logic: <br>#1*Find min session_start grouped by user_id having a session_type of viewer saved as cte<br><br>#2*Use cte from 1* to join original table on user_id (this keeps only the users who had first session as a viewer) <br><br><br>WITH sq AS(<br>SELECT user_id , MIN(session_start) as first_session<br>FROM twitch_sessions<br>GROUP BY user_id<br>),<br><br>sq2 AS(<br>SELECT sq.user_id, sq.first_session, twitch_sessions.session_type<br>FROM sq<br>JOIN twitch_sessions ON twitch_sessions.user_id = sq.user_id AND twitch_sessions.session_start = sq.first_session<br>WHERE session_type = &#39;viewer&#39;<br>)<br><br><br>SELECT sq.user_id , COUNT(*) as num_of_sessions<br>FROM sq<br>JOIN twitch_sessions ON sq.user_id = twitch_sessions.user_id<br>WHERE twitch_sessions.session_type = &#39;streamer&#39;<br>GROUP BY sq.user_id<br>ORDER BY COUNT(*) DESC , user_id ASC;<br><br>If anyone knows a clever way to do get the session_type associated with the min(session_start) without a second cte or a window function I&#39;d love to hear it. Great content Nate.",2021-06-21T01:35:34Z
UgzJ65QSJPMHU0IVdO14AaABAg.9OpnjTfldI49Oq-4zocT57,@stratascratch,UgzJ65QSJPMHU0IVdO14AaABAg,2,tNXliLTlrV8,0,1,2021-06-21T03:23:06Z,"I like this way just fine. But if you do want some help, just post this message on the user discussion board on the link to the question. Someone from my team will help!",2021-06-21T03:23:06Z
Ugxq03YGD7dWlTFWmgN4AaABAg,@monagulapa3022,,1,tNXliLTlrV8,1,0,2021-05-14T11:18:07Z,I shared your youtube channel on my Facebook.  Your tutorials are great! Will share it also on my Linkedin.,2021-05-14T11:18:07Z
Ugxq03YGD7dWlTFWmgN4AaABAg.9NK-EoYfaU99NKPHMxcvUC,@stratascratch,Ugxq03YGD7dWlTFWmgN4AaABAg,2,tNXliLTlrV8,0,0,2021-05-14T15:05:39Z,Thanks so much! Hope your network enjoys the videos on this channel.,2021-05-14T15:05:39Z
UgzKFsRW4JRebtIle-p4AaABAg,@SeattleDataGuy,,1,tNXliLTlrV8,2,2,2021-04-01T19:33:55Z,"Hey, I am digging your videos! I love the focus on particular companies SQL questions.",2021-04-01T19:33:55Z
UgzKFsRW4JRebtIle-p4AaABAg.9Lb9nXiV8nO9LbCJO6t84C,@stratascratch,UgzKFsRW4JRebtIle-p4AaABAg,2,tNXliLTlrV8,0,1,2021-04-01T19:55:52Z,"Thanks so much man! I also got your message on Mediu and emailed you. But if you don&#39;t get my email feel free to reach out at nate@<a href=""http://stratascratch.com/"">stratascratch.com</a>",2021-04-01T19:55:52Z
UgzKFsRW4JRebtIle-p4AaABAg.9Lb9nXiV8nO9LbFPZ5xJ4u,@SeattleDataGuy,UgzKFsRW4JRebtIle-p4AaABAg,2,tNXliLTlrV8,0,0,2021-04-01T20:22:56Z,@@stratascratch Awesome!,2021-04-01T20:22:56Z
UgyAJwgd0Yi31tLYHxx4AaABAg,@rakibraihan1572,,1,tNXliLTlrV8,0,0,2021-03-30T19:21:36Z,thanks,2021-03-30T19:21:36Z
UgxDNS_k2LIRo6fX9YN4AaABAg,@InYourHorror,,1,tNXliLTlrV8,1,0,2021-03-28T07:00:42Z,Thanks for putting out all of this great content.,2021-03-28T07:00:42Z
UgxDNS_k2LIRo6fX9YN4AaABAg.9LQWQNgRt309LScKKxn50r,@stratascratch,UgxDNS_k2LIRo6fX9YN4AaABAg,2,tNXliLTlrV8,0,0,2021-03-29T02:39:31Z,Thanks for watching the videos! More to come,2021-03-29T02:39:31Z
UgyPIGO74KmW6GRZhK54AaABAg,@SuperLOLABC,,1,tNXliLTlrV8,1,0,2021-03-27T15:57:09Z,Will there be a collection of SQL and Python questions asked in PayPal?,2021-03-27T15:57:09Z
UgyPIGO74KmW6GRZhK54AaABAg.9LOu0XhHtnq9LScNn9bW34,@stratascratch,UgyPIGO74KmW6GRZhK54AaABAg,2,tNXliLTlrV8,0,0,2021-03-29T02:40:00Z,I haven&#39;t found any so far but I do continue to look out for Paypal interview questions. They&#39;ll definitely be on the monthly releases if I find any. Thanks for following!,2021-03-29T02:40:00Z
Ugx0C0-QMGLSQNtqRsB4AaABAg,@iitian2012,,1,tNXliLTlrV8,0,0,2021-03-27T06:46:17Z,You are awsome sir....Keep posting such a great sql videos....,2021-03-27T06:46:17Z
Ugwv5K4oz_YdNGo8JtB4AaABAg,@ahyoungkim8256,,1,tNXliLTlrV8,1,3,2021-03-25T11:08:09Z,hope that i can get any technical interview test soon so that i can apply what i learn from you :),2021-03-25T11:08:09Z
Ugwv5K4oz_YdNGo8JtB4AaABAg.9LJEM8AMh279LJa3GnlK1R,@stratascratch,Ugwv5K4oz_YdNGo8JtB4AaABAg,2,tNXliLTlrV8,0,0,2021-03-25T14:26:33Z,I think this video will certainly help! Good luck,2021-03-25T14:26:33Z
Ugz4EUFcsLRSa_0BW-d4AaABAg,@PATRICKCHUAD,,1,tNXliLTlrV8,1,0,2021-03-25T06:56:42Z,Thanks Nate! As always very clear explanation.,2021-03-25T06:56:42Z
Ugz4EUFcsLRSa_0BW-d4AaABAg.9LIm_UshG239LJa1jAAD8l,@stratascratch,Ugz4EUFcsLRSa_0BW-d4AaABAg,2,tNXliLTlrV8,0,0,2021-03-25T14:26:21Z,Thanks! Always appreciate your comments on my videos!,2021-03-25T14:26:21Z
UgwW3wKX7oIh6pKHGjB4AaABAg,@anuragsingh4766,,1,tNXliLTlrV8,0,1,2021-03-25T04:44:31Z,üëåüëåüëç,2021-03-25T04:44:31Z
UgySAyr318jVWLS6ZI54AaABAg,@lizard_sinno,,1,tNXliLTlrV8,1,0,2021-03-24T19:50:51Z,Great!,2021-03-24T19:50:51Z
UgySAyr318jVWLS6ZI54AaABAg.9LHaNYB17vV9LHic_M4BxU,@stratascratch,UgySAyr318jVWLS6ZI54AaABAg,2,tNXliLTlrV8,0,0,2021-03-24T21:02:56Z,Thanks man =),2021-03-24T21:02:56Z
UgwlBYc-sWDGsSGp7hh4AaABAg,@its_me7363,,1,tNXliLTlrV8,1,1,2021-03-24T19:27:43Z,"Hey Nate, Your videos really helped me learning SQL...Can you make videos on how to use API for data and then saving it in some database as you told in one of your video that it is important part in data science process and it is widely used in professional data scientist projects?",2021-03-24T19:27:43Z
UgwlBYc-sWDGsSGp7hh4AaABAg.9LHYj7vrEf-9LHibMzLuG4,@stratascratch,UgwlBYc-sWDGsSGp7hh4AaABAg,2,tNXliLTlrV8,0,1,2021-03-24T21:02:46Z,Yup! That&#39;s coming out soon. It&#39;s taking a while to create it ... sorry!,2021-03-24T21:02:46Z
UgyQvtpb4-Jl-Y_llC14AaABAg,@kapuriaritik,,1,tNXliLTlrV8,1,0,2021-03-24T19:06:09Z,Great video!,2021-03-24T19:06:09Z
UgyQvtpb4-Jl-Y_llC14AaABAg.9LHWGB59YO79LHi_NF5jOI,@stratascratch,UgyQvtpb4-Jl-Y_llC14AaABAg,2,tNXliLTlrV8,0,0,2021-03-24T21:02:30Z,Thanks for watching!,2021-03-24T21:02:30Z
UgxWP7tVU9dMBw2bo7V4AaABAg,@mosherchtman,,1,pV_mnZgzXEo,0,1,2022-12-21T13:01:00Z,"I&#39;m a little confused - <a href=""https://www.youtube.com/watch?v=pV_mnZgzXEo&amp;t=5m40s"">5:40</a>  most recent, shouldn&#39;t use the MAX function instead of MIN?",2022-12-21T13:01:00Z
UgxK1X6fWH4EUH-hUG14AaABAg,@subinivi,,1,pV_mnZgzXEo,0,0,2022-09-27T13:58:01Z,Thanks a lot. Much appreciated,2022-09-27T13:58:01Z
Ugwpa2sSyrcvbhASJQN4AaABAg,@callravik,,1,pV_mnZgzXEo,1,0,2022-04-16T07:03:11Z,"I had an interview with FB a couple of years ago.  My interview ended with one question.  How do you order by data without using order by clause in SQL?  I could not give a satisfactory answer which ended my interview for a Data Architect.  I tried to get more information from the interviewer but in vain. She was arrogant and condescending.  I ran it by Tom Kyte, he said I was better off not working for that manager.  I was curious since then, if any of you got these kinds of gotcha questions.",2022-04-16T07:06:08Z
Ugwpa2sSyrcvbhASJQN4AaABAg.9_sHxXOvagP9_teKfuECNm,@stratascratch,Ugwpa2sSyrcvbhASJQN4AaABAg,2,pV_mnZgzXEo,0,2,2022-04-16T19:46:42Z,I never had someone ask me that question before in an interview. But I guess you could create an index or ranking column that would give an order to the rows? I&#39;m not a data architech so my db knowledge isn&#39;t as good as traditional architects. Looks like you dodged a bullet!,2022-04-16T19:46:42Z
UgzX_BZnDGG2Hyd6EKl4AaABAg,@kritiverma1342,,1,pV_mnZgzXEo,1,0,2022-03-23T16:05:10Z,"Hi Nate, great content. I was wondering if you have any video on nested subqueries or on query optimization techniques.",2022-03-23T16:05:10Z
UgzX_BZnDGG2Hyd6EKl4AaABAg.9ZvSuCRrXdw9Zw5fMu693-,@stratascratch,UgzX_BZnDGG2Hyd6EKl4AaABAg,2,pV_mnZgzXEo,0,1,2022-03-23T22:01:25Z,"Hi! Thanks for watching. No specific video on nested queries but many of my video shave nested queries. And towards the end, I talk about optimization on most of my SQL <a href=""http://videos.so/"">videos.So</a> I think that might help.",2022-03-23T22:01:25Z
UgyQ2Q-dGwuV7xu9e_F4AaABAg,@TheCazz10,,1,pV_mnZgzXEo,0,0,2022-01-04T03:50:03Z,"Very helpful, thank you",2022-01-04T03:50:03Z
UgycuT5GI6aAotO8hux4AaABAg,@t2udu,,1,pV_mnZgzXEo,2,0,2021-12-17T11:20:39Z,I learned sql through the sqlite &#39;flavor&#39; so I do understand what&#39;s going on. Is there a particular flavor that&#39;s popular in interviews?,2021-12-17T11:20:39Z
UgycuT5GI6aAotO8hux4AaABAg.9W2l1HC7J8a9W3LA5LpExT,@stratascratch,UgycuT5GI6aAotO8hux4AaABAg,2,pV_mnZgzXEo,0,1,2021-12-17T16:45:10Z,"For interviews, you can use any flavor (in my experience) so sqlite is totally fine. At the job, most companies have industry grade db engines so you&#39;ll see HIVE and Snowflake which most people don&#39;t have access to so that&#39;s why they allow any sql engine on interviews.",2021-12-17T16:45:10Z
UgycuT5GI6aAotO8hux4AaABAg.9W2l1HC7J8a9W3fLQvCBOv,@t2udu,UgycuT5GI6aAotO8hux4AaABAg,2,pV_mnZgzXEo,0,0,2021-12-17T19:50:13Z,@@stratascratch Thanks for clearing that up!. Keep up the awesome videos.,2021-12-17T19:50:13Z
Ugz2VtvDBBd1zoMFBll4AaABAg,@Tridentor,,1,pV_mnZgzXEo,1,0,2021-09-04T18:49:53Z,"With all of your videos, I click &#39;Like&#39; first and listen afterwards. And I am never disappointed :)",2021-09-04T18:49:53Z
Ugz2VtvDBBd1zoMFBll4AaABAg.9RsllaSpBC99Rvf4XKNgCh,@stratascratch,Ugz2VtvDBBd1zoMFBll4AaABAg,2,pV_mnZgzXEo,0,0,2021-09-05T21:49:10Z,Thanks so much =),2021-09-05T21:49:10Z
Ugw5JJvUWEU1HZh6TEp4AaABAg,@majorcemp3612,,1,pV_mnZgzXEo,2,1,2021-07-27T07:30:51Z,Is count(cas when x=&#39;y&#39; then 1 else null end) Equivalent as sum(cas when x=&#39;y&#39; then 1 else null end) ?,2021-07-27T07:30:51Z
Ugw5JJvUWEU1HZh6TEp4AaABAg.9QI8329gRJo9QIyk-Yf-Ax,@stratascratch,Ugw5JJvUWEU1HZh6TEp4AaABAg,2,pV_mnZgzXEo,0,1,2021-07-27T15:19:58Z,"Yes, I think so since you&#39;re using NULLs in the sum(). However, not sure if that would work since standard is to use 0 in the sum(). In which case, it would not be equivalent. Here&#39;s a resource on it <a href=""https://stackoverflow.com/questions/46894050/difference-between-count-and-sum-within-an-aggregate-case-statement/46894311"">https://stackoverflow.com/questions/46894050/difference-between-count-and-sum-within-an-aggregate-case-statement/46894311</a>",2021-07-27T15:19:58Z
Ugw5JJvUWEU1HZh6TEp4AaABAg.9QI8329gRJo9QIzGLO5itj,@majorcemp3612,Ugw5JJvUWEU1HZh6TEp4AaABAg,2,pV_mnZgzXEo,0,0,2021-07-27T15:24:31Z,"@@stratascratch okay, nice to know thanks :D",2021-07-27T15:24:31Z
Ugz7Tt0C8OFOGZL2Xi54AaABAg,@jimwoodward7293,,1,pV_mnZgzXEo,0,0,2021-07-25T16:38:11Z,Thanks Nate - very informative content.,2021-07-25T16:38:11Z
Ugw18yWStB6LTACnYFt4AaABAg,@pastramiking,,1,pV_mnZgzXEo,1,3,2021-06-06T00:47:09Z,Is it just me or does data science have less and less to do with modeling each year?,2021-06-06T00:47:09Z
Ugw18yWStB6LTACnYFt4AaABAg.9OE5Jf3mCfD9Vck4s-YfMl,@johnwig285,Ugw18yWStB6LTACnYFt4AaABAg,2,pV_mnZgzXEo,0,1,2021-12-06T23:32:54Z,"That&#39;s because modelling is just a tool, a means to an end. Data Science is about deriving insights to create a business impact. Not everything requires models.",2021-12-06T23:32:54Z
UgyVJlITWEo4qRkaq4J4AaABAg,@thebestofbests7746,,1,pV_mnZgzXEo,1,0,2021-05-14T12:50:55Z,"You are really doing a great job, cleared most of my concepts.",2021-05-14T12:50:55Z
UgyVJlITWEo4qRkaq4J4AaABAg.9NK9rX4iHed9NKPCMduMYz,@stratascratch,UgyVJlITWEo4qRkaq4J4AaABAg,2,pV_mnZgzXEo,0,0,2021-05-14T15:04:58Z,That&#39;s great. Hope the other videos were also helpful,2021-05-14T15:04:58Z
UgwpQH-0A2AXyyHSJTN4AaABAg,@rahidulislam5129,,1,pV_mnZgzXEo,1,0,2021-03-10T08:54:02Z,Thank You,2021-03-10T08:54:02Z
UgwpQH-0A2AXyyHSJTN4AaABAg.9KhN4ef76jR9KibHmuynjJ,@stratascratch,UgwpQH-0A2AXyyHSJTN4AaABAg,2,pV_mnZgzXEo,0,0,2021-03-10T20:26:08Z,Thanks for watching!,2021-03-10T20:26:08Z
UgwhY_sdAR5JwPQ7xyF4AaABAg,@kenehrman342,,1,pV_mnZgzXEo,1,0,2021-03-08T02:27:59Z,"So, I know all that SQL stuff, that doesn&#39;t make me a data science candidate does it? Surely there is more to it than that.",2021-03-08T02:28:23Z
UgwhY_sdAR5JwPQ7xyF4AaABAg.9KbXJS3gURE9Kbco_eIaDU,@stratascratch,UgwhY_sdAR5JwPQ7xyF4AaABAg,2,pV_mnZgzXEo,0,1,2021-03-08T03:24:48Z,No not at all. SQL is just a component of data science. But the questions that I go through are data science type. I should probably start covering other data science topics on this channel =),2021-03-08T03:24:48Z
UgwdwCYQFn90dl4tOYZ4AaABAg,@vrajpatel8256,,1,pV_mnZgzXEo,2,0,2021-03-07T18:20:18Z,"Great work sir, I finally found what I was looking for on your channel.",2021-03-07T18:20:18Z
UgwdwCYQFn90dl4tOYZ4AaABAg.9KaeVT04dXP9KbHphHpd9p,@stratascratch,UgwdwCYQFn90dl4tOYZ4AaABAg,2,pV_mnZgzXEo,0,2,2021-03-08T00:12:43Z,Thanks for watching! So this is the type of content you&#39;d like to see more of?,2021-03-08T00:12:43Z
UgwdwCYQFn90dl4tOYZ4AaABAg.9KaeVT04dXP9KbY0o4o1vb,@vrajpatel8256,UgwdwCYQFn90dl4tOYZ4AaABAg,2,pV_mnZgzXEo,0,0,2021-03-08T02:34:11Z,@@stratascratch Yes sir!!,2021-03-08T02:34:11Z
UgwvlN4WJZTAS1f5LjJ4AaABAg,@mikemihay,,1,pV_mnZgzXEo,1,0,2021-03-06T18:57:12Z,Thank you!,2021-03-06T18:57:12Z
UgwvlN4WJZTAS1f5LjJ4AaABAg.9KZ8vvopNjb9KZd3TC9VUb,@stratascratch,UgwvlN4WJZTAS1f5LjJ4AaABAg,2,pV_mnZgzXEo,0,0,2021-03-06T23:29:15Z,Thank you for watching!,2021-03-06T23:29:15Z
UgxMjN_6ALEuyFtHT_F4AaABAg,@ItzLarry,,1,pV_mnZgzXEo,1,1,2021-03-04T03:57:49Z,Super useful content Nate! Which of your previous videos would you recommend I watch/study for a SQL interview for a company like Square? I have my interview pretty soon and I haven‚Äôt found many tips/examples online about Square,2021-03-04T03:57:49Z
UgxMjN_6ALEuyFtHT_F4AaABAg.9KSOPtXByGi9KUCwRtbvUn,@stratascratch,UgxMjN_6ALEuyFtHT_F4AaABAg,2,pV_mnZgzXEo,0,0,2021-03-04T20:56:01Z,"Ah, unfortunately, I don&#39;t have any Square interview questions on youtube. I don&#39;t even think I have too many on my platform (https://platform.stratascratch.com). Check out both the coding and non-coding sections. There might only be a handful of them. I would recommend doing questions from companies in the same industry which might be helpful. And companies from tech like Twitter since the talent pool they source from is similar.",2021-03-04T20:56:01Z
Ugyopa-iKdey1vKVbmF4AaABAg,@BBBBBBAAAl,,1,pV_mnZgzXEo,1,1,2021-03-04T03:39:12Z,"Thank you Nate! Great content as usual, I really hope you keep going and I thank you alot.",2021-03-04T03:39:12Z
Ugyopa-iKdey1vKVbmF4AaABAg.9KSMHZOaV889KUClGh2yJj,@stratascratch,Ugyopa-iKdey1vKVbmF4AaABAg,2,pV_mnZgzXEo,0,1,2021-03-04T20:54:30Z,"Yup, will keep this series of interview questions going. I come out with a set of questions every month so I want to make sure those are communicated on Youtube. Over time we&#39;ll go over the trends and concepts tested on interviews as soon as I am able to analyze the trends.",2021-03-04T20:54:30Z
UgyBJ-f8gTsQlaR-oqx4AaABAg,@Joan.Zacarias,,1,c4Af2FcgamA,0,0,2023-09-01T18:50:58Z,"Hello!<br>I have some doubts, and maybe you can help me:<br><br>I don&#39;t have a clear view of the market to choose a sector to specialize in. Should I still start a good project in any sector?<br><br>I&#39;ve never used APIs, and I wonder: How should I contact someone to get permission?<br><br>Although my knowledge in data science is still limited, would you recommend starting to create high-quality YT videos, beginning with simple topics to build a more attractive profile for potential employers?<br><br>Is web scraping typically not allowed?<br><br>Thanks for the content!",2023-09-01T18:50:58Z
Ugy-9RoszxFKrD8I1tV4AaABAg,@user-gb1sg9kb5k,,1,c4Af2FcgamA,1,0,2023-07-21T23:35:42Z,"I am from Taiwn, and thank you so much.",2023-07-21T23:35:42Z
Ugy-9RoszxFKrD8I1tV4AaABAg.9sS6-KAj9JT9s_rQYsXDCs,@stratascratch,Ugy-9RoszxFKrD8I1tV4AaABAg,2,c4Af2FcgamA,0,0,2023-07-25T09:12:46Z,Welcome!,2023-07-25T09:12:46Z
UgxzATV3fIKmcZc_mEJ4AaABAg,@MuaaaaahahahaH,,1,c4Af2FcgamA,0,0,2023-07-01T00:21:13Z,Thknx for this vid!,2023-07-01T00:21:13Z
Ugyc6wWvw_rYktG8lHV4AaABAg,@doop9134,,1,c4Af2FcgamA,0,1,2023-06-24T20:05:51Z,"I‚Äôm going to start my final project in the next 2 weeks. To be honest , I still don‚Äôt know which project I should build. So this helps so much in terms of planning and give me more clarity what I should focus on!  Thank you so much!! üôè‚ú®‚ù§Ô∏è",2023-06-24T20:05:51Z
UgxiSDci4QfeNiXaPZx4AaABAg,@FinneganClancy,,1,c4Af2FcgamA,1,0,2023-06-19T23:03:32Z,1. Stay away from the Titanic and Iris datasets.<br><br>2. Avoid Kaggle unless you can rank top 10.<br><br>3. Interviews are looking for data scientists with real world skills:<br>- coding and analytics<br>-  using modern tech and tools<br><br>4. The properties of a good project:<br>(a). Uses real data - real time data <br>(b). Uses modern tech - APIs + Databases<br>(c). Building models - Why this model? How did you clean your data? etc. Interviewers don&#39;t care about results as much as they care about your reasoning behind your choices.<br>(d). Making an impact/validation - Share your code + Output your insights on graphs and make a blog or deploy an app.<br><br>Make one amazing project. Then you can <b>almost</b> copy and paste your original project to have many great projects.,2023-06-19T23:03:32Z
UgxiSDci4QfeNiXaPZx4AaABAg.9r9dsi-y4hB9rA5EHxbXoz,@stratascratch,UgxiSDci4QfeNiXaPZx4AaABAg,2,c4Af2FcgamA,0,0,2023-06-20T03:11:17Z,Great summary!,2023-06-20T03:11:17Z
Ugz197EkysEmB3XzNwV4AaABAg,@senyotsedze3388,,1,c4Af2FcgamA,1,0,2023-06-02T01:16:29Z,This is too much for an entry level DS.,2023-06-02T01:16:29Z
Ugz197EkysEmB3XzNwV4AaABAg.9qRXmiriyfT9qSqI2uzd8X,@stratascratch,Ugz197EkysEmB3XzNwV4AaABAg,2,c4Af2FcgamA,0,0,2023-06-02T13:26:10Z,It&#39;s definitely a lot to be a DS. But I would say this is the bar at this time.,2023-06-02T13:26:10Z
UgwmgFJUQ3v5h7EdvE94AaABAg,@senyotsedze3388,,1,c4Af2FcgamA,1,0,2023-06-02T01:12:59Z,So how do we get these kinds of practice.,2023-06-02T01:12:59Z
UgwmgFJUQ3v5h7EdvE94AaABAg.9qRXO1vd5o69qSqCW-nfnK,@stratascratch,UgwmgFJUQ3v5h7EdvE94AaABAg,2,c4Af2FcgamA,0,0,2023-06-02T13:25:24Z,Personal projects is the best way. You can use StrataScratch to find projects that are tailored to what you&#39;re trying to learn.,2023-06-02T13:25:24Z
UgzlvFS1XhrL0vwIAIh4AaABAg,@BreadForBrain100,,1,c4Af2FcgamA,1,0,2023-05-31T13:28:42Z,"he doesnt recommend kaggle and everybody doesnt join any competitions in kaggle. Plottwist is, he compete in kaggle competition and win without another challenger",2023-05-31T13:28:42Z
UgzlvFS1XhrL0vwIAIh4AaABAg.9qNgznzqnJl9qPDCJV8JTQ,@stratascratch,UgzlvFS1XhrL0vwIAIh4AaABAg,2,c4Af2FcgamA,0,0,2023-06-01T03:38:08Z,"Trust me, I would never win a Kaggle competition =). The reason why I would stay away from Kaggle is that they&#39;re more about building ML models and that&#39;s it. DS is much more than just building models. There are many other things to learn.",2023-06-01T03:38:08Z
UgzvccDNGSnFbS8zOpN4AaABAg,@eristonmansambu8225,,1,c4Af2FcgamA,0,0,2023-05-26T07:18:44Z,"At first, it&#39;s daunting then after I realized that it&#39;s better to know the truth than to live believing a lie. Now I know exactly things I&#39;m supposed to learn. Thank you so muchüôèüèΩ",2023-05-26T07:18:44Z
UgwaycIqVV3nvuA7ZqJ4AaABAg,@krishgaikwad6108,,1,c4Af2FcgamA,0,0,2023-05-18T10:56:04Z,"Thank you so much, the most genuine video about DS project, every concept and mistakes were explained in a very well manner.",2023-05-18T10:56:04Z
UgzRGr3dl0sCpR4pcq94AaABAg,@virtual240,,1,c4Af2FcgamA,0,0,2023-04-16T10:15:54Z,I think requiring all this experience for a junior analyst is ridiculous and unrealistic.,2023-04-16T10:15:54Z
UgxWQM75uaBSpZkBIAx4AaABAg,@yosigam,,1,c4Af2FcgamA,0,0,2023-04-11T18:14:58Z,Thank you very much,2023-04-11T18:14:58Z
Ugw4eh4RmEwYGsxFY2J4AaABAg,@vectoralphaAI,,1,c4Af2FcgamA,2,1,2023-03-21T01:49:18Z,This is for data science which is more advanced than data analysis. What would you recommend for an entry level data analysis project instead of the more advanced data science?,2023-03-21T01:49:18Z
Ugw4eh4RmEwYGsxFY2J4AaABAg.9nVcW9VRNyb9nXLXapP6ZD,@stratascratch,Ugw4eh4RmEwYGsxFY2J4AaABAg,2,c4Af2FcgamA,0,1,2023-03-21T17:50:41Z,"Really any project that requires you to use SQL or python to clean and transform data. That&#39;s all that is expected our of most DAs. Maybe add some visualization dashboards as an end product (e.g., build a Tableau dashboard) would be a great bonus. Building models is a DS role so you can skip that part of any project. Hope that helps.",2023-03-21T17:50:41Z
Ugw4eh4RmEwYGsxFY2J4AaABAg.9nVcW9VRNyb9nXU5XO_6W8,@vectoralphaAI,Ugw4eh4RmEwYGsxFY2J4AaABAg,2,c4Af2FcgamA,0,1,2023-03-21T19:05:30Z,@@stratascratch yes that helps. Thank you.,2023-03-21T19:05:30Z
Ugz8VILqrENiQVClVMh4AaABAg,@quadrialli3715,,1,c4Af2FcgamA,0,0,2023-03-11T17:07:21Z,Beautiful Video. Thank you,2023-03-11T17:07:21Z
UgzH01PFJDkWeuULitl4AaABAg,@YuriyBiks,,1,c4Af2FcgamA,0,0,2023-02-28T19:07:54Z,"Thank you for such encouraging and concise video, especially for those who makes first steps  in the data science path!!!",2023-02-28T19:07:54Z
UgyVOO6eGbHmcD5hSKJ4AaABAg,@sasukeslime,,1,c4Af2FcgamA,0,0,2023-01-26T09:40:09Z,Haha Kaggle is not too easy,2023-01-26T09:40:09Z
UgyHxtjANg4JDwxgHmF4AaABAg,@noriejeannepereira2837,,1,c4Af2FcgamA,0,0,2023-01-20T09:16:24Z,Your advice is real and sharing this knowledge will help those who want to be a data scientist.,2023-01-20T09:16:24Z
UgxBw5E-2Qhy7-4dGQx4AaABAg,@karunakaranr2473,,1,c4Af2FcgamA,0,0,2023-01-13T03:26:36Z,Really good input to start with. Thank you for your time.,2023-01-13T03:26:36Z
Ugxwj27n_fYqH2glPmR4AaABAg,@gatorpika,,1,c4Af2FcgamA,1,1,2022-12-26T04:37:40Z,"Really good video.  I guess I would reiterate the piece he said about being able to explain your work in detail as if to a child and communicate the value of the outcome.  Typically I am explaining my output to children, which we also refer to as upper management, and it&#39;s often difficult to get on the same page as them.  This is also mostly with charts, data and simple math analyses let alone trying to explain why machine learning is telling them something different than their gut says is right.  Business value is also something to incorporate into your analyses for those working in the commercial sector.  Don&#39;t just show correlations, but include the revenue impact or whatever because ultimately they are going to want to pick 3 things out of the dozen you listed to do and want the highest impact stuff.  Having a great outcome story to tell is very powerful.",2022-12-26T04:37:40Z
Ugxwj27n_fYqH2glPmR4AaABAg.9k53CySeu8C9k7FX-rQY5S,@stratascratch,Ugxwj27n_fYqH2glPmR4AaABAg,2,c4Af2FcgamA,0,0,2022-12-27T01:03:44Z,That&#39;s exactly right! Great write up. Agree with everything you mentioned.,2022-12-27T01:03:44Z
Ugwl2hEUr3uDpWqdFe54AaABAg,@jamescutler428,,1,c4Af2FcgamA,0,0,2022-12-23T13:39:20Z,"I feel like this is the most helpful data science video I‚Äôve ever seen. Now I just need to find the resources to actually learn these things lol! (Configuring APIs, making API calls, working with the twitter or Amazon API, using AWS, etc.)",2022-12-23T13:39:20Z
UgxyM3gK1OR2SqrBF5x4AaABAg,@avinashajmera2775,,1,c4Af2FcgamA,0,0,2022-12-14T11:33:37Z,"can you provide some link of apies , which you have said to see in description , for data .",2022-12-14T11:33:37Z
UgwvAtGZD8LBpyv53x14AaABAg,@balford2112,,1,c4Af2FcgamA,0,11,2022-12-11T22:29:16Z,So helpful. I took an entire course on DA and still had no clear idea of what the process was supposed to be in practice! I also watched videos that suggested tons of projects but not exactly how to go about them. Thank you for doing this video. I feel like I finally have an inkling about what data science work entails.,2022-12-11T22:29:16Z
UgyPD1o5LLZawbVrQUl4AaABAg,@shresthaditya2950,,1,c4Af2FcgamA,0,33,2022-12-09T16:22:36Z,"<a href=""https://www.youtube.com/watch?v=c4Af2FcgamA&amp;t=3m30s"">3:30</a>-Example of A.P.I Twitter, Google Analytics,YouTube,Netflix,Amazon <br><a href=""https://www.youtube.com/watch?v=c4Af2FcgamA&amp;t=4m06s"">4:06</a>-1) How To Setup and Configure <a href=""http://a.p.is/"">A.P.Is</a>,2)Learn how to use libraries to help you make A.P.I calls <br>3)Learn How to work with data structures like JSON and Dictionaries<br><a href=""https://www.youtube.com/watch?v=c4Af2FcgamA&amp;t=4m00s"">4:00</a>-Use A.P.I for data sciences:<br><a href=""https://www.youtube.com/watch?v=c4Af2FcgamA&amp;t=4m48s"">4:48</a>-Use Cloud Database,Make Datapipeline of AWS and Google Cloud.<br><a href=""https://www.youtube.com/watch?v=c4Af2FcgamA&amp;t=6m20s"">6:20</a>-1)Building Models 2)Why did you pick that model can other models do the same 3)Why did you clean the data 4) What type of validation tests did you perform on the data to prepare for the model<br>5) Tell me about the assumptions of your model 6) How did you optimize your model 7)How did you optimize your model 8) Explain the math behind your model<br><a href=""https://www.youtube.com/watch?v=c4Af2FcgamA&amp;t=7m48s"">7:48</a>-Make an impact and get validation by sharing what you have done 1)Code Sharing on Data Science Subreddits or GitHub 2) Share your Insights Create Visualisations and create Blogs/ Articles or TowardsDataScience 3)Learn Django/Flask/Aws to deploy an application that can serve as an interactive dashboard or A.P.I",2023-01-01T06:17:32Z
UgyML7slOBRJgtNisih4AaABAg,@JuanHernandez-xh2js,,1,c4Af2FcgamA,1,1,2022-12-03T00:34:47Z,Very useful. Thank you!,2022-12-03T00:34:47Z
UgyML7slOBRJgtNisih4AaABAg.9j9P7vPnEUf9jBArQWKrrL,@stratascratch,UgyML7slOBRJgtNisih4AaABAg,2,c4Af2FcgamA,0,0,2022-12-03T17:08:32Z,You&#39;re welcome.,2022-12-03T17:08:32Z
UgyUOVwgp5Kjqe0pHNJ4AaABAg,@tommyboy7820,,1,c4Af2FcgamA,1,1,2022-11-20T07:11:47Z,Thanks for sharing,2022-11-20T07:11:47Z
UgyUOVwgp5Kjqe0pHNJ4AaABAg.9iddEJ216p29ih5vfSrFkt,@stratascratch,UgyUOVwgp5Kjqe0pHNJ4AaABAg,2,c4Af2FcgamA,0,0,2022-11-21T15:28:58Z,You&#39;re welcome.  We hoped our video helped you.,2022-11-21T15:28:58Z
Ugy9srDZP9K1gEMtmCJ4AaABAg,@stefano_er,,1,c4Af2FcgamA,1,1,2022-11-16T17:47:09Z,Thanks for advices!,2022-11-16T17:47:23Z
Ugy9srDZP9K1gEMtmCJ4AaABAg.9iVTlMAgMRN9jk27YNiY27,@stratascratch,Ugy9srDZP9K1gEMtmCJ4AaABAg,2,c4Af2FcgamA,0,0,2022-12-17T15:24:53Z,You bet!,2022-12-17T15:24:53Z
UgyE5srtqmZo1OuJq2x4AaABAg,@adhiwahyukurniawan9988,,1,c4Af2FcgamA,0,0,2022-11-08T05:01:53Z,I&#39;m lucky enough that YouTube algorithm show me this really great video. I will use this as guidelines for my DS journey. Thank you so much for this great information. Keep it up !,2022-11-08T05:01:53Z
UgxtEBqegyK0qQQaL8p4AaABAg,@Eswar.,,1,c4Af2FcgamA,0,2,2022-11-01T06:48:38Z,I love ml looking for someone to learn along<br>Is there any ?,2022-11-01T06:48:38Z
UgzsNuBa1hZYpw-R_xd4AaABAg,@elliepenkova3853,,1,c4Af2FcgamA,0,1,2022-10-23T14:47:07Z,"The first video about DS projects that&#39;s actually 100% useful. Great job,  Nate, I&#39;ll follow this path for my project for sure.",2022-10-23T14:47:07Z
UgwfUh-4aA1APUE-4KF4AaABAg,@aureliohess9349,,1,c4Af2FcgamA,0,0,2022-10-21T15:25:30Z,Great work. Congrats!,2022-10-21T15:25:30Z
UgwNjrE1QYVLqYpc9-t4AaABAg,@mihamilojevic5894,,1,c4Af2FcgamA,0,0,2022-10-19T12:38:08Z,"nice , no bullshit tips",2022-10-19T12:38:08Z
Ugyy7WErpjSDaqeKIQN4AaABAg,@anamikas2567,,1,c4Af2FcgamA,0,0,2022-10-17T14:43:37Z,This video needs more views honestly,2022-10-17T14:43:37Z
UgzwaX20XVobryIfLcR4AaABAg,@manoelm.m.6121,,1,c4Af2FcgamA,0,0,2022-10-03T10:11:54Z,Thank tou for the amazing video.,2022-10-03T10:11:54Z
UgwpYV8r0ZmPuUvIBmt4AaABAg,@christopherknudson1842,,1,c4Af2FcgamA,0,1,2022-09-26T02:57:21Z,Which video is the next video?,2022-09-26T02:57:21Z
UgxKhzN2I-FZbE_ITXB4AaABAg,@im4485,,1,c4Af2FcgamA,1,1,2022-09-22T00:07:38Z,"Hi, can you please make a video on exmple projects? I am having trouble deciding on a topic.",2022-09-22T00:07:38Z
UgxKhzN2I-FZbE_ITXB4AaABAg.9gFxn8qAnqc9jk2CI5yR9t,@stratascratch,UgxKhzN2I-FZbE_ITXB4AaABAg,2,c4Af2FcgamA,0,0,2022-12-17T15:25:32Z,Thank for the suggestion.,2022-12-17T15:25:32Z
Ugw3mJ8wPjQeRj3Njox4AaABAg,@MikeD-qx1kr,,1,c4Af2FcgamA,1,1,2022-09-20T11:33:22Z,"I agree, there are so many great real-life projects you can start. Incidentally I learned Pandas on the titanic dataset ahahah.<br>ü§£ü§£",2022-09-20T11:33:22Z
Ugw3mJ8wPjQeRj3Njox4AaABAg.9gC1frEolC59gCPo2Hb8zs,@stratascratch,Ugw3mJ8wPjQeRj3Njox4AaABAg,2,c4Af2FcgamA,0,0,2022-09-20T15:04:12Z,That is a popular project.  Good for you :),2022-09-20T15:04:12Z
Ugyd3k2iwmtLFtD4B9R4AaABAg,@tianag4627,,1,c4Af2FcgamA,0,0,2022-09-17T18:10:21Z,"Great breakdown, thanks! I intuitively felt that using datasets like Titanic are not very valuable, so always used my own data for learning.",2022-09-17T18:10:21Z
Ugxp12cODW4g6j2Yxql4AaABAg,@FG-tu9et,,1,c4Af2FcgamA,1,1,2022-09-17T15:56:20Z,This is a gem!,2022-09-17T15:56:20Z
Ugxp12cODW4g6j2Yxql4AaABAg.9g4mO8_1o929g6wU6K_lof,@stratascratch,Ugxp12cODW4g6j2Yxql4AaABAg,2,c4Af2FcgamA,0,0,2022-09-18T12:03:00Z,Thank you!  My team is happy you find our video helpful.,2022-09-18T12:03:00Z
Ugxa2D8P8GbE4t6vCeh4AaABAg,@ThePenguinJunkie,,1,c4Af2FcgamA,0,1,2022-09-12T17:53:45Z,I have a growing interest in data science and have been posting greater attention to it. <br>This video is a tremendous help! I was expecting to have to build 3-4 separate projects to show my skill set. Building then the way you described would help streamline the whole process. <br>Thank you very much for putting out this video! <br>-Bryan,2022-09-12T17:53:45Z
Ugxtl_7MXZHccN9J7lh4AaABAg,@daisydream757,,1,c4Af2FcgamA,1,1,2022-09-12T12:41:49Z,I&#39;ve done that titanic project haha lol. but i also added up with another project on google analytics. currently doing another one on MySQL for credit risk project. never tried collecting data through api and never know how to do it. will work on that soon.,2022-09-12T12:41:49Z
Ugxtl_7MXZHccN9J7lh4AaABAg.9fsZ97Yd-fW9fvBihM79he,@stratascratch,Ugxtl_7MXZHccN9J7lh4AaABAg,2,c4Af2FcgamA,0,1,2022-09-13T13:14:49Z,That is awesome!  Good luck with your new project.,2022-09-13T13:14:49Z
UgzJJcu0dQVjcr9h64x4AaABAg,@dzikirullahfolorunsho294,,1,c4Af2FcgamA,1,1,2022-09-04T19:37:19Z,I have been through all of these steps. I hope to land my dream job soon.,2022-09-04T19:37:19Z
UgzJJcu0dQVjcr9h64x4AaABAg.9fZhLOJQBsV9fdS-vNb0L6,@stratascratch,UgzJJcu0dQVjcr9h64x4AaABAg,2,c4Af2FcgamA,0,0,2022-09-06T15:50:47Z,That is wonderful!  Hope you land your dream job!,2022-09-06T15:50:47Z
Ugw4zS6f4XyLL8N6NT14AaABAg,@Chuukwudi,,1,c4Af2FcgamA,1,1,2022-08-23T21:49:30Z,Why am I just seeing this channel today?,2022-08-23T21:49:30Z
Ugw4zS6f4XyLL8N6NT14AaABAg.9f51vwXteCN9f7kO46JsiN,@stratascratch,Ugw4zS6f4XyLL8N6NT14AaABAg,2,c4Af2FcgamA,0,0,2022-08-24T23:05:10Z,Thanks! Come and visit us anytime.,2022-08-24T23:05:10Z
UgzJC0HWC0EN7LXKoGJ4AaABAg,@sulemaanfarooq7563,,1,c4Af2FcgamA,3,1,2022-08-20T10:23:01Z,"I have visited around 25-30 countries and I am 19 years old. Do you think doing the &quot;only data science project you need&quot; but tailoring it around the problems of countries that I visited would be a good idea? For example I went to Bosnia and saw the effects of the 1992 war. So perhaps I can do some data science that can show the effects of war and perhaps can get a data set from the late 80&#39;s where I can create a model that would predict the chances of the 1992 war starting. I have a lot of ideas like these for Bosnia and other countries like Turkey, England, Qatar etc. For reference I am a rising sophomore at Rutgers where I study computer science and data science.",2022-08-20T10:23:01Z
UgzJC0HWC0EN7LXKoGJ4AaABAg.9ex4zP8mk9e9eyH016E1b_,@stratascratch,UgzJC0HWC0EN7LXKoGJ4AaABAg,2,c4Af2FcgamA,0,0,2022-08-20T21:27:20Z,That‚Äôs a great project and something many people would be very interested in. Where are you thinking of getting the data? That‚Äôs the toughest part in my opinion.,2022-08-20T21:27:20Z
UgzJC0HWC0EN7LXKoGJ4AaABAg.9ex4zP8mk9e9ezaldIQEYd,@sulemaanfarooq7563,UgzJC0HWC0EN7LXKoGJ4AaABAg,2,c4Af2FcgamA,0,1,2022-08-21T09:47:58Z,@@stratascratch thats my question. perhaps kaggle or government websites. would there be a way to connect with you? I hope to learn a lot from you,2022-08-21T09:47:58Z
UgzJC0HWC0EN7LXKoGJ4AaABAg.9ex4zP8mk9e9f-dI7WJHEO,@stratascratch,UgzJC0HWC0EN7LXKoGJ4AaABAg,2,c4Af2FcgamA,0,0,2022-08-21T19:29:16Z,"@@sulemaanfarooq7563 Getting data is the toughest part of any project, I believe.  I hope you get the data you need.  Visit our website <a href=""https://www.stratascratch.com/"">https://www.stratascratch.com/</a> to get in touch.",2022-08-21T19:29:16Z
UgxhISDx89Nub7VcvoB4AaABAg,@daviddevita,,1,c4Af2FcgamA,0,0,2022-08-18T00:37:35Z,"May I ask you a very silly question? There&#39;s something I simply don&#39;t understand: let&#39;s say your build a project, run the analysis and jump into an oitcome. Fine. Project finished. What&#39;s next? Even if you build a dashboard, it becomes &quot;their&quot; problem because you aim them to self manage their decision making process. Is this right? If so, what is the daily basis activity of a data scientist? I&#39;m kind of stuck with this doubt. Thabks for your great videos. Greetings from Argentina!",2022-08-18T00:37:35Z
UgwRzR3cUijwlZUIC6Z4AaABAg,@kawthooleidevelopers2864,,1,c4Af2FcgamA,1,0,2022-08-14T01:10:00Z,"Excellent!!  You ignite a spark of idea of how I should go about my project.  Thank you, sir.",2022-08-14T01:10:00Z
UgwRzR3cUijwlZUIC6Z4AaABAg.9egdvgf3mQA9eiZCwFLkHa,@stratascratch,UgwRzR3cUijwlZUIC6Z4AaABAg,2,c4Af2FcgamA,0,0,2022-08-14T18:58:32Z,That is just wonderful to hear.  I am excited for you and your project.,2022-08-14T18:58:32Z
UgytfiYLQC8LFLsou4x4AaABAg,@moonemujjal,,1,c4Af2FcgamA,1,0,2022-08-08T21:17:15Z,Thanks for such a comprehensive set of advice. I truly agree with it.,2022-08-08T21:17:15Z
UgytfiYLQC8LFLsou4x4AaABAg.9eUMJiBAFLB9eWY68gYGXf,@stratascratch,UgytfiYLQC8LFLsou4x4AaABAg,2,c4Af2FcgamA,0,0,2022-08-09T17:38:44Z,You&#39;re welcome!  I appreciate your comment.,2022-08-09T17:38:44Z
UgxP2zsVZPMEeXpD9dt4AaABAg,@quadropheniaguy9811,,1,c4Af2FcgamA,1,0,2022-07-26T15:38:55Z,Great podcast. Thanks.,2022-07-26T15:38:55Z
UgxP2zsVZPMEeXpD9dt4AaABAg.9dxHGJlrINd9e1PRXYKA6w,@stratascratch,UgxP2zsVZPMEeXpD9dt4AaABAg,2,c4Af2FcgamA,0,0,2022-07-28T15:26:34Z,Thanks for listening.  Appreciate it.,2022-07-28T15:26:34Z
Ugwmjr4OOknXeqVnlwh4AaABAg,@henriqueguazzelli8265,,1,c4Af2FcgamA,1,0,2022-07-20T13:21:55Z,Great video! I saw it yesterday and today I&#39;m starting my project! I&#39;ll come back  soon to tell you I got my job!,2022-07-20T13:21:55Z
Ugwmjr4OOknXeqVnlwh4AaABAg.9dh_o_2wtnD9dkA5InxRL4,@stratascratch,Ugwmjr4OOknXeqVnlwh4AaABAg,2,c4Af2FcgamA,0,0,2022-07-21T13:26:07Z,That is wonderful!  Good luck on that job.,2022-07-21T13:26:07Z
Ugwe3ZK3qHhaHOvod2V4AaABAg,@praveen_me,,1,c4Af2FcgamA,1,0,2022-07-19T18:23:41Z,The most practical video. Period.,2022-07-19T18:23:41Z
Ugwe3ZK3qHhaHOvod2V4AaABAg.9dfYYu6aqBB9dhHDx8aWnP,@stratascratch,Ugwe3ZK3qHhaHOvod2V4AaABAg,2,c4Af2FcgamA,0,0,2022-07-20T10:30:45Z,Ah that is great to hear.  Thank you.,2022-07-20T10:30:45Z
UgwL2jlIVG47K_RKsgt4AaABAg,@oluwadamilaretijani1777,,1,c4Af2FcgamA,1,0,2022-07-12T07:04:25Z,"Whao! This is highly informative! How a real data scientist talk, please can you guide us through all these processes.",2022-07-12T07:04:25Z
UgwL2jlIVG47K_RKsgt4AaABAg.9dNJFfwEIdq9dOVB7zK9Am,@stratascratch,UgwL2jlIVG47K_RKsgt4AaABAg,2,c4Af2FcgamA,0,1,2022-07-12T18:07:53Z,"Hi Oluwadamilare, glad you find my content informative,  Subscribe to my channel and browser through my playlist.  You can find a whole set of videos that can help you with your next project.",2022-07-12T18:07:53Z
UgwMLM37Ke3HgBXwJH14AaABAg,@vonsabido2281,,1,c4Af2FcgamA,0,0,2022-06-28T12:36:12Z,I wish I have known  this 9 years ago,2022-06-28T12:36:12Z
UgzprzZIQP7A_ANr7dB4AaABAg,@vibhudalal901,,1,c4Af2FcgamA,2,6,2022-06-21T03:17:24Z,"Hey Nate, thanks for the highly informative and to-the-point video. I was just wondering if you could also provide one or two examples of projects which fit the requirements that you mentioned? That would be great. And if you&#39;ve already answered a similar question before, then kindly direct me to it, I went through quite a few comments but couldn&#39;t find a similar question which had been answered.",2022-06-21T03:17:24Z
UgzprzZIQP7A_ANr7dB4AaABAg.9cWp_eaAiOW9cvOh_cJnvp,@stratascratch,UgzprzZIQP7A_ANr7dB4AaABAg,2,c4Af2FcgamA,0,3,2022-07-01T01:34:04Z,"Depends what you&#39;re into but here&#39;s an idea...go to <a href=""https://www.reddit.com/r/dataisbeautiful/"">https://www.reddit.com/r/dataisbeautiful/</a> and try to recreate these visualizations. Try to collect the data from and API (if there is one), manipulate the data, and visualize it with whatever tool you want. Better yet, create an app that would take parameters so that the visualizations change depending on user input and serve that up on AWS or GCP. Share it on the same subreddit and see if people like it. If they don&#39;t, ask how you can improve it, and iterate from there until you build something people would want to see.",2022-07-01T01:34:04Z
UgzprzZIQP7A_ANr7dB4AaABAg.9cWp_eaAiOW9cvehBU1xFr,@vibhudalal901,UgzprzZIQP7A_ANr7dB4AaABAg,2,c4Af2FcgamA,0,0,2022-07-01T04:02:34Z,"@@stratascratch Great, will have a look. Thanks for the reply!",2022-07-01T04:02:34Z
UgzeHecKU7cMBO_B3N94AaABAg,@maryzakiandourrugrats4671,,1,c4Af2FcgamA,1,0,2022-06-15T13:03:06Z,This was probably the best video I‚Äôve seen on a data science project,2022-06-15T13:03:06Z
UgzeHecKU7cMBO_B3N94AaABAg.9cIQq7-BDe49dmmHyNTR75,@stratascratch,UgzeHecKU7cMBO_B3N94AaABAg,2,c4Af2FcgamA,0,0,2022-07-22T13:47:07Z,Glad this is helpful.,2022-07-22T13:47:07Z
UgyJXiHV9Jk0RTc-FwJ4AaABAg,@adamstrejcovsky8257,,1,c4Af2FcgamA,0,0,2022-06-13T22:18:18Z,this is DS with no BS.,2022-06-13T22:18:18Z
UgxLcGP8FI9dhFDOYsR4AaABAg,@benjaminoluyori2924,,1,c4Af2FcgamA,0,0,2022-06-12T19:49:37Z,Great content! Just Subscribed,2022-06-12T19:49:37Z
UgyEqFY-vafUMg-49mN4AaABAg,@rodrigosauda5548,,1,c4Af2FcgamA,0,0,2022-06-11T06:28:05Z,Amazing content. Congrats,2022-06-11T06:28:05Z
UgzjXHuraPjaYE8lTEh4AaABAg,@puranjaykwatra0314,,1,c4Af2FcgamA,1,0,2022-06-07T06:27:55Z,thankyou for sharing such valuable information and it also gives me a roadmap which skills i need to build and what I have currently done . Just watched your first video and loved the way you explain all these things . Keep the good work,2022-06-07T06:27:55Z
UgzjXHuraPjaYE8lTEh4AaABAg.9by7Fjm04649kXCPlXwk_1,@stratascratch,UgzjXHuraPjaYE8lTEh4AaABAg,2,c4Af2FcgamA,0,0,2023-01-06T02:56:47Z,Glad it was helpful!,2023-01-06T02:56:47Z
Ugyyqi4iHsAT8c9C9eJ4AaABAg,@sauravsahay8803,,1,c4Af2FcgamA,1,0,2022-06-05T07:28:04Z,"If this is your expectations from someone trying to enter this field, probably you have too high expectations from people who are just getting a taste of everything new and overwhelming",2022-06-05T07:28:04Z
Ugyyqi4iHsAT8c9C9eJ4AaABAg.9bt4YaRT4089buTusSXILq,@stratascratch,Ugyyqi4iHsAT8c9C9eJ4AaABAg,2,c4Af2FcgamA,0,0,2022-06-05T20:28:57Z,"I don&#39;t think it&#39;s too high tbh. It&#39;s something to strive for and expected of you if you come out of college with a masters degree + 1 summer of an internship. I hire for DEs and DS, and expect this quality in my candidates. I haven&#39;t had much trouble finding them. I can also confirm that many of my other friends that are hiring managers in tech look for the same skills.",2022-06-05T20:28:57Z
UgzQEuqeQvPZQGc0geh4AaABAg,@astrophysics6326,,1,c4Af2FcgamA,0,0,2022-06-01T01:26:11Z,"ohmygod this guy is so cute, and he&#39;s also so smart. joking aside, this is one of the best pieces of advice I&#39;ve got on the internet, thank you so much for this. it&#39;s such a disappointment that I haven&#39;t subscribed earlier.",2022-06-01T01:26:11Z
UgxyMZzBhV1zOAxPfvF4AaABAg,@cedricvillani8502,,1,c4Af2FcgamA,1,0,2022-05-21T20:26:16Z,That was totally worthless thank you lol,2022-05-21T20:26:16Z
UgxyMZzBhV1zOAxPfvF4AaABAg.9bIqg4neYat9bIrdJdJ79c,@stratascratch,UgxyMZzBhV1zOAxPfvF4AaABAg,2,c4Af2FcgamA,0,0,2022-05-21T20:34:37Z,No worries. Good luck in your future. I&#39;m sure you&#39;re going to need it =),2022-05-21T20:34:37Z
UgwaLGRu8q1pGBoiGQV4AaABAg,@haibhanu,,1,c4Af2FcgamA,1,0,2022-05-19T00:30:52Z,Thank you for your advice. This video gives me full bandwidth of duties what a data scientist could do in real time. Thanks a lot,2022-05-19T00:30:52Z
UgwaLGRu8q1pGBoiGQV4AaABAg.9bBZI0qCLl-9kXNAQ9XiIl,@stratascratch,UgwaLGRu8q1pGBoiGQV4AaABAg,2,c4Af2FcgamA,0,0,2023-01-06T04:30:49Z,You are welcome!,2023-01-06T04:30:49Z
UgzfJP9cA4Y_zV4DixZ4AaABAg,@IvanE1990,,1,c4Af2FcgamA,1,0,2022-05-16T18:11:11Z,You are creating worldwide impact with your videos. Thank you,2022-05-16T18:11:11Z
UgzfJP9cA4Y_zV4DixZ4AaABAg.9b5jFK8dZb_9kUzh4YqEz6,@stratascratch,UgzfJP9cA4Y_zV4DixZ4AaABAg,2,c4Af2FcgamA,0,0,2023-01-05T06:18:28Z,Thankyou! Much appreciated,2023-01-05T06:18:28Z
UgyJ1nSVeceNLBi-hGF4AaABAg,@angelmitra7148,,1,c4Af2FcgamA,0,1,2022-05-14T19:58:05Z,i knew this video was going to be meaningful the second you said avoid kaggle lol; so sick of kaggle at this point,2022-05-14T19:58:05Z
Ugx8u-Ndr0d6zRY8tSZ4AaABAg,@hardy2175,,1,c4Af2FcgamA,0,1,2022-05-14T13:39:20Z,Titanic will literally sink you. üòÖüòÖ,2022-05-14T13:39:20Z
UgwXFZymiKKBhtlkdPB4AaABAg,@billybitcoin,,1,c4Af2FcgamA,1,0,2022-05-13T18:12:26Z,"That was extremely helpful, thank you.",2022-05-13T18:12:26Z
UgwXFZymiKKBhtlkdPB4AaABAg.9az0-qBQqxn9kUzjpiBX6K,@stratascratch,UgwXFZymiKKBhtlkdPB4AaABAg,2,c4Af2FcgamA,0,0,2023-01-05T06:18:51Z,Glad it was helpful!,2023-01-05T06:18:51Z
Ugx6Q7Eq3Kmfw_78thZ4AaABAg,@faisalbargi5903,,1,TYHWv1vT0Pk,0,0,2023-04-01T08:09:26Z,what if the date format is dd-mm-yyyy,2023-04-01T08:09:26Z
UgwopheQxnagyo9twSh4AaABAg,@JNET_Reloaded,,1,TYHWv1vT0Pk,0,0,2022-11-09T13:58:36Z,"how do i select all records that have an event datetime witin 1 hour before, for example mysql select all that time_diff less than 1 hour from datetime? its for a reminder email i want to make on a cronjob can you help with this sql query?",2022-11-09T13:58:36Z
UgyBQReNxLc1JDADZmN4AaABAg,@oagengtembo226,,1,TYHWv1vT0Pk,1,1,2022-09-06T08:19:28Z,You speak slow and clear<br>I love you,2022-09-06T08:19:28Z
UgyBQReNxLc1JDADZmN4AaABAg.9fcdMOHHI6N9fdSFDNCvCL,@stratascratch,UgyBQReNxLc1JDADZmN4AaABAg,2,TYHWv1vT0Pk,0,0,2022-09-06T15:52:52Z,"Oh, thank you for noticing.  Yeah, I consciously try to speak slowly and clearly in my videos.",2022-09-06T15:52:52Z
UgxuYbbYH7xER5bnAYl4AaABAg,@mahesh26sai,,1,TYHWv1vT0Pk,1,0,2022-07-19T17:45:07Z,Your channel is AMAZING() over ( PARTITION by videos),2022-07-19T17:46:06Z
UgxuYbbYH7xER5bnAYl4AaABAg.9dfU8PddSkf9dhHEkcK4r3,@stratascratch,UgxuYbbYH7xER5bnAYl4AaABAg,2,TYHWv1vT0Pk,0,0,2022-07-20T10:30:52Z,Glad you like them!,2022-07-20T10:30:52Z
Ugx5m9AYwyFPI08X8e54AaABAg,@ItsWithinYou,,1,TYHWv1vT0Pk,0,0,2022-06-12T21:53:09Z,"Many thanks brother! I appreciate your help! <br>Can we use datepart and count together? Something like <br>SELECT COUNT(DATEPART(YEAR,inspection_date AS DATE)) <br>FROM sf_restaurant_health_violation <br>WHERE EXTRACT (YEAR FROM CAST(inspection_date AS DATE) = 2015",2022-06-12T21:57:20Z
UgykYVDE0VUM2llORzh4AaABAg,@rozario1309,,1,TYHWv1vT0Pk,0,0,2022-04-10T00:54:35Z,Great Video. Clearly explained.,2022-04-10T00:54:35Z
UgzlvpURmgo6H8MyuZt4AaABAg,@brianle8899,,1,TYHWv1vT0Pk,0,0,2022-03-30T22:59:32Z,One of the best SQL channels out there. Please keep the content coming! Thank you!,2022-03-30T22:59:32Z
UgzRzRAfTp1kll-tnLF4AaABAg,@yuthpatirathi2719,,1,TYHWv1vT0Pk,0,1,2022-02-25T02:24:30Z,Thank you,2022-02-25T02:24:30Z
Ugx10WkEJmWGFYZqEHN4AaABAg,@hudatolah,,1,TYHWv1vT0Pk,0,0,2022-02-10T21:52:22Z,"‚Äòextract‚Äô is not a recognized built-in function name. ??? MSSQL<br>I used datepart(year, ‚Äòdatefield‚Äô)  instead.",2022-02-10T22:05:56Z
UgxV046hzcj2uMF_J3p4AaABAg,@ryandavis280,,1,TYHWv1vT0Pk,0,0,2022-01-19T21:40:18Z,thanksÔºÅ,2022-01-19T21:40:18Z
UgyrLuLZLnPIYr-6jNR4AaABAg,@user-wi7un7le5g,,1,TYHWv1vT0Pk,1,0,2022-01-02T11:49:24Z,Do you have video on Date Manipulations in Excel ?,2022-01-02T11:49:24Z
UgyrLuLZLnPIYr-6jNR4AaABAg.9Wh01uTjrgF9WiFeNc0TOX,@stratascratch,UgyrLuLZLnPIYr-6jNR4AaABAg,2,TYHWv1vT0Pk,0,0,2022-01-02T23:25:06Z,"Unfortunately, I do not. I would have thought there are a lot of resources for Excel?",2022-01-02T23:25:06Z
UgzPBwCisy_x_SDiWYF4AaABAg,@dianadennis7225,,1,TYHWv1vT0Pk,1,0,2021-12-26T05:19:47Z,"I appreciate your content a lot! I&#39;ve watched quite a few of your videos, and I&#39;m grateful for your willingness to help others learn. Now, curious as to why you cover mainly SQL and not Python(not complaining though, as a BI, I use SQL, not Python)<br>Also, would like to see more on APIs and data streams on your channel(how data is collected via APIs, and anything you consider relevant to this topic). Still new to this, but I&#39;m using it at work and would like to get a better understanding( I have a background in math, so SQL wasn&#39;t complicated to learn, as I had already covered set theory); the whole API thing seems a bit challenging to understand.",2021-12-26T05:19:47Z
UgzPBwCisy_x_SDiWYF4AaABAg.9WPHt2VhXO59WQy7q4Z9qA,@stratascratch,UgzPBwCisy_x_SDiWYF4AaABAg,2,TYHWv1vT0Pk,0,3,2021-12-26T20:56:55Z,"Thanks for the kind words. I mainly cover SQL because I originally started out my channel with a focus on SQL. Over the past year I explored other languages like python and built data science projects just to see if people found them interesting. They did, which is great, so in 2022, I&#39;m hoping to expand to other data science topics like python, probability, statistics, modeling, and projects. My team will be helping to create these videos so you&#39;ll see a lot more production in addition to expanded topics!",2021-12-26T20:56:55Z
Ugw3ZFZ_5qR500jB5jF4AaABAg,@luckychitundu1070,,1,TYHWv1vT0Pk,0,1,2021-12-14T06:33:39Z,"Nate, I&#39;ve been enjoying your content brotherüëå",2021-12-14T06:33:39Z
Ugx2ggna3MzmXiL-5ER4AaABAg,@bandhammanikanta6302,,1,TYHWv1vT0Pk,0,0,2021-11-29T12:54:54Z,I get confidence when I listed to your explanations. Thank you..,2021-11-29T12:54:54Z
Ugz-suThmu4lprnax7h4AaABAg,@alphar85,,1,TYHWv1vT0Pk,0,0,2021-11-27T10:24:21Z,Am glad i subcribed. You are amazing Nate. I had SQL experience and haven&#39;t used it for long time.  It is great to have a refresher. Real one,2021-11-27T10:24:21Z
Ugx-dcJoZLZlRd9kHuZ4AaABAg,@anandvyavahare2031,,1,TYHWv1vT0Pk,2,0,2021-11-11T16:13:03Z,So before going through your solution I tried the question and I did not have to cast the column to date datatype and I just used extract and it gave me output. And even when I tried to filter for 2015 year with extract it still worked.  But overall great tip. I have always found it difficult to deal with dates in python or even in SQL. No less than a nightmare to be honest..,2021-11-11T16:13:03Z
Ugx-dcJoZLZlRd9kHuZ4AaABAg.9Ub_sRVUGpO9Ubh__SIowg,@stratascratch,Ugx-dcJoZLZlRd9kHuZ4AaABAg,2,TYHWv1vT0Pk,0,1,2021-11-11T17:20:23Z,"Thanks for letting me know. Sometimes it works, sometimes it doesn&#39;t. It&#39;s not always obvious if something is a date dtype but there are ways to check it before you cast. I just cast automatically sometimes.",2021-11-11T17:20:23Z
Ugx-dcJoZLZlRd9kHuZ4AaABAg.9Ub_sRVUGpO9UbifTuyItp,@anandvyavahare2031,Ugx-dcJoZLZlRd9kHuZ4AaABAg,2,TYHWv1vT0Pk,0,0,2021-11-11T17:29:56Z,@@stratascratch Got it!! üòÉüòÉ,2021-11-11T17:29:56Z
UgwH6rty5EOj9SKV8F14AaABAg,@shobhamourya8396,,1,TYHWv1vT0Pk,1,0,2021-10-25T07:29:07Z,"Here&#39;s my solution:<br>select inspection_year, count(*)<br>from <br>(<br>    select EXTRACT(YEAR FROM inspection_date) as inspection_year<br>    from sf_restaurant_health_violations<br>    where business_name ilike &#39;%Roxanne Cafe%&#39; and <br>    violation_id is not null<br>)a<br>group by inspection_year<br>order by inspection_year",2021-10-25T07:29:07Z
UgwH6rty5EOj9SKV8F14AaABAg.9TusP3BMUbD9TwFMc7pG1_,@stratascratch,UgwH6rty5EOj9SKV8F14AaABAg,2,TYHWv1vT0Pk,0,1,2021-10-25T20:17:44Z,That&#39;s great! Feel free to check on the platform to see if it validates.,2021-10-25T20:17:44Z
Ugx0S2JoJTJcPa3X0D14AaABAg,@levongalstyan8009,,1,TYHWv1vT0Pk,0,0,2021-10-21T21:53:44Z,man you rock ! thank you,2021-10-21T21:53:44Z
UgzpVqSNTDXKehQyryh4AaABAg,@kurtji8170,,1,TYHWv1vT0Pk,4,0,2021-10-21T01:23:44Z,"Hey Nate, Why no just use &#39;WHERE year = 2015&#39;? Why can you use &#39;GROUP BY(year)&#39; but not &#39;WHERE year = 2015&#39;?",2021-10-21T01:23:44Z
UgzpVqSNTDXKehQyryh4AaABAg.9TjvPW0aAZ49Tlt4nDYlsZ,@stratascratch,UgzpVqSNTDXKehQyryh4AaABAg,2,TYHWv1vT0Pk,0,0,2021-10-21T19:41:55Z,The GROUP BY is necessary because I have a variable &#39;year&#39; with an aggregate count(violation_id). It&#39;s required for me to have a GROUP BY. Great question tho.,2021-10-21T19:41:55Z
UgzpVqSNTDXKehQyryh4AaABAg.9TjvPW0aAZ49TmQHyFjMTI,@kurtji8170,UgzpVqSNTDXKehQyryh4AaABAg,2,TYHWv1vT0Pk,0,0,2021-10-22T00:40:49Z,"@@stratascratch Hi Nate, I think I did not make my question clear. What I meant is why not use &#39;year&#39; in the WHERE clause since you already defined &#39;year&#39;, as it has been used in GROUP BY(year)",2021-10-22T00:40:49Z
UgzpVqSNTDXKehQyryh4AaABAg.9TjvPW0aAZ49To2h6otcQf,@stratascratch,UgzpVqSNTDXKehQyryh4AaABAg,2,TYHWv1vT0Pk,0,0,2021-10-22T15:53:09Z,"@@kurtji8170 I see what you mean now. I don&#39;t think that&#39;s possible to use `year` in the WHERE clause due to how SQL is processed. GROUP BY and ORDER BY are processed last so it knows that `year` exists. However, WHERE is processed before GROUP BY and ORDER BY so it doesn&#39;t know `year`. I think this is why I still had to extract the year from the date field in the WHERE clause.",2021-10-22T15:53:09Z
UgzpVqSNTDXKehQyryh4AaABAg.9TjvPW0aAZ49Tp_fVwLQtB,@kurtji8170,UgzpVqSNTDXKehQyryh4AaABAg,2,TYHWv1vT0Pk,0,0,2021-10-23T06:09:16Z,"@@stratascratch That explains the confusion, thanks!",2021-10-23T06:09:16Z
UgzjAi2YgE2OxGEukGR4AaABAg,@alanlinaaa,,1,TYHWv1vT0Pk,1,0,2021-10-03T15:15:25Z,Really love this channel!!! Its so important for DA to pull the data out!!,2021-10-03T15:15:25Z
UgzjAi2YgE2OxGEukGR4AaABAg.9T23HAG2jaY9T3Hkua2g5F,@stratascratch,UgzjAi2YgE2OxGEukGR4AaABAg,2,TYHWv1vT0Pk,0,0,2021-10-04T02:41:11Z,Thanks for watching! Really appreciate it! :),2021-10-04T02:41:11Z
Ugx-7bEm9tACQp-kjV54AaABAg,@BJTangerine,,1,TYHWv1vT0Pk,2,2,2021-09-25T01:10:50Z,"Wow, you handled that sample question effortlessly. I&#39;m learning SQL foundations right now and have added the CAST function to my notes in case I have to do it in a SQL dialect that doesn&#39;t have the &#39;::&#39; function",2021-09-25T01:10:50Z
Ugx-7bEm9tACQp-kjV54AaABAg.9SgxGF1Pr3b9Slh-WmPbS4,@stratascratch,Ugx-7bEm9tACQp-kjV54AaABAg,2,TYHWv1vT0Pk,0,0,2021-09-26T21:24:57Z,That&#39;s great! Only postgres has :: for casting but can also use cast(). Most other sql engines use cast() so it&#39;s a great function to know.,2021-09-26T21:24:57Z
Ugx-7bEm9tACQp-kjV54AaABAg.9SgxGF1Pr3b9U-4YDBLasF,@rohith98,Ugx-7bEm9tACQp-kjV54AaABAg,2,TYHWv1vT0Pk,0,0,2021-10-27T08:00:09Z,"@@stratascratch  hi,<br>Can you please make a tutorials on the specific topics individually so that we can build the concept intially and then apply it whenever required.Waiting for your reply.",2021-10-27T08:00:09Z
Ugxpy6e1-ZOyuJ7x9yd4AaABAg,@yousfoss4367,,1,TYHWv1vT0Pk,0,0,2021-08-26T18:32:45Z,thks a lot,2021-08-26T18:32:45Z
Ugy3gESrnQTy1Ts_PzN4AaABAg,@shalupatil,,1,TYHWv1vT0Pk,1,0,2021-07-05T15:03:44Z,"I am confused, how Alias was available in group by???",2021-07-05T15:03:44Z
Ugy3gESrnQTy1Ts_PzN4AaABAg.9PQIOobrTI19PRlNLqo8bD,@stratascratch,Ugy3gESrnQTy1Ts_PzN4AaABAg,2,TYHWv1vT0Pk,0,0,2021-07-06T04:44:55Z,It&#39;s postgres...so maybe it&#39;s a postgres thing? I&#39;ve always been able to use alias for group bys,2021-07-06T04:44:55Z
UgzskOTbPS9hrQX3_Qd4AaABAg,@siranxie844,,1,TYHWv1vT0Pk,1,0,2021-06-24T18:20:39Z,Very useful! Thank you! Your channel helped me a lot!,2021-06-24T18:20:39Z
UgzskOTbPS9hrQX3_Qd4AaABAg.9OzKBSr_MN69OzmJkuyjTF,@stratascratch,UgzskOTbPS9hrQX3_Qd4AaABAg,2,TYHWv1vT0Pk,0,0,2021-06-24T22:35:11Z,I&#39;m glad! Thank you for watching!,2021-06-24T22:35:11Z
UgwLcfOpGIWV1GDugnZ4AaABAg,@TheFiratayrilik,,1,TYHWv1vT0Pk,1,0,2021-06-18T09:17:10Z,Keep doing these contents mate! You are the best,2021-06-18T09:17:10Z
UgwLcfOpGIWV1GDugnZ4AaABAg.9OiuD_THnC69OjtX4fcoR-,@stratascratch,UgwLcfOpGIWV1GDugnZ4AaABAg,2,TYHWv1vT0Pk,0,1,2021-06-18T18:30:20Z,Thank you! More vids to come!,2021-06-18T18:30:20Z
UgwuFHb0_wXkCZdIhpV4AaABAg,@judyhe686,,1,TYHWv1vT0Pk,2,0,2021-05-30T19:26:10Z,Thanks for the explanation! What if I need to compare dates of year month like &quot;YYYY-MM&quot; and how can I do that after extracting that component?,2021-05-30T19:26:10Z
UgwuFHb0_wXkCZdIhpV4AaABAg.9Nz3oQo8EbB9Nz5U61kL3V,@stratascratch,UgwuFHb0_wXkCZdIhpV4AaABAg,2,TYHWv1vT0Pk,0,0,2021-05-30T19:40:44Z,"You wouldn&#39;t be able to compare dates if the dates are in the format &#39;YYYY-MM&#39; since it has a char data type. My advice would be to keep the full date &#39;YYYY-MM-DD&#39; and do the comparison then. Then once you are ready to aggregate and group, change the dates to &#39;YYYY-MM&#39;. Hope that helps!",2021-05-30T19:40:44Z
UgwuFHb0_wXkCZdIhpV4AaABAg.9Nz3oQo8EbB9OG3Nc2lJXK,@judyhe686,UgwuFHb0_wXkCZdIhpV4AaABAg,2,TYHWv1vT0Pk,0,0,2021-06-06T19:08:42Z,"@@stratascratch Thanks Nate, that helps!",2021-06-06T19:08:42Z
Ugxq2ZX9lBiIToPXmw14AaABAg,@huanchenli4137,,1,TYHWv1vT0Pk,1,0,2021-04-28T04:36:56Z,Very helpful!!!,2021-04-28T04:36:56Z
Ugxq2ZX9lBiIToPXmw14AaABAg.9Mf4bTWxF149MgKDLGTKZd,@stratascratch,Ugxq2ZX9lBiIToPXmw14AaABAg,2,TYHWv1vT0Pk,0,0,2021-04-28T16:12:33Z,Hope you enjoy the series! There&#39;s dozens of videos,2021-04-28T16:12:33Z
UgxMcpx5XO7LbDinWfF4AaABAg,@ghinwamoujaes9059,,1,TYHWv1vT0Pk,2,1,2021-03-28T17:20:24Z,Thank you very much. I love your content :) Can extract be used to filter by YY-MMMM? Or do you have any other recommendations?,2021-03-28T17:20:24Z
UgxMcpx5XO7LbDinWfF4AaABAg.9LRcL9uF0yE9LScz7muyD2,@stratascratch,UgxMcpx5XO7LbDinWfF4AaABAg,2,TYHWv1vT0Pk,0,1,2021-03-29T02:45:14Z,"Thank you! Unfortunately, extract can&#39;t do YY-MM because it&#39;s trying to grab just one component of the date field. Try to_char(). But just remember that YY-MM will be a string. It won&#39;t be able to recognize that field as a date.",2021-03-29T02:45:14Z
UgxMcpx5XO7LbDinWfF4AaABAg.9LRcL9uF0yE9LT1t4lq4O-,@ghinwamoujaes9059,UgxMcpx5XO7LbDinWfF4AaABAg,2,TYHWv1vT0Pk,0,0,2021-03-29T06:31:36Z,@@stratascratch Thanks for the tip!,2021-03-29T06:31:36Z
Ugz6un8fetz-QGV5Km54AaABAg,@weiyang2116,,1,TYHWv1vT0Pk,5,0,2021-03-05T04:11:46Z,Could you do some videos on the coding questions in your site? Would love the same explanation method in a different env. Thanks!,2021-03-05T04:12:40Z
Ugz6un8fetz-QGV5Km54AaABAg.9KUznvOuULt9KWNY1_dKtH,@stratascratch,Ugz6un8fetz-QGV5Km54AaABAg,2,TYHWv1vT0Pk,0,0,2021-03-05T17:07:09Z,Hi sorry what do you mean? The questions on this video are from my site. Do you want me to do questions on another site or platform?,2021-03-05T17:07:09Z
Ugz6un8fetz-QGV5Km54AaABAg.9KUznvOuULt9NBduErWPUP,@pvgirish7801,Ugz6un8fetz-QGV5Km54AaABAg,2,TYHWv1vT0Pk,0,0,2021-05-11T05:29:00Z,"@@stratascratch He might be asking that, along with SQL, can you prepare content for cracking coding interviews as well? Actually that&#39;d be great.. your videos are awesome and I upgraded to the paid subscription..",2021-05-11T05:29:00Z
Ugz6un8fetz-QGV5Km54AaABAg.9KUznvOuULt9NClQukyqjA,@stratascratch,Ugz6un8fetz-QGV5Km54AaABAg,2,TYHWv1vT0Pk,0,1,2021-05-11T15:54:00Z,@@pvgirish7801 Thanks! Many of my SQL videos are of the point of view of cracking the coding interview. Unless there&#39;s another type of question or programming language you&#39;d like for me to use?,2021-05-11T15:54:00Z
Ugz6un8fetz-QGV5Km54AaABAg.9KUznvOuULt9NClmoNbJYW,@pvgirish7801,Ugz6un8fetz-QGV5Km54AaABAg,2,TYHWv1vT0Pk,0,0,2021-05-11T15:57:08Z,@@stratascratch yeah.. something like data structures and algorithms or competitive programming for interviews. When We go to Data scientist interviews .. this is the one of the round that we are facing after sql and data science related rounds (at least in India),2021-05-11T16:00:10Z
Ugz6un8fetz-QGV5Km54AaABAg.9KUznvOuULt9NCwo7mWvVS,@stratascratch,Ugz6un8fetz-QGV5Km54AaABAg,2,TYHWv1vT0Pk,0,1,2021-05-11T17:33:26Z,@@pvgirish7801 OK got it. Will do that! I&#39;m planning on adding some algo questions to the platform later in the year so I&#39;ll create some videos on that once I am able to integrate them on the StrataScratch platform.,2021-05-11T17:33:26Z
Ugxah5yWBeXlInXV48B4AaABAg,@owenbird1075,,1,TYHWv1vT0Pk,1,4,2021-02-18T21:29:35Z,"This is really great stuff, this is going to help me out massively. Really like the presentation and clear yet simple to understand explainations",2021-02-18T21:29:35Z
Ugxah5yWBeXlInXV48B4AaABAg.9JvDdyZC4Vo9JvQCZnkJrq,@stratascratch,Ugxah5yWBeXlInXV48B4AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-18T23:19:18Z,"Thanks for watching! I got many more coding videos coming out so I hope you watch them too. If there are any topics you want me to cover, feel free to call them out too.",2021-02-18T23:19:18Z
UgxBp3iudyvfEIeDN7R4AaABAg,@459B,,1,TYHWv1vT0Pk,1,0,2021-02-13T09:55:49Z,üëçüèªüëçüèªüëçüèª,2021-02-13T09:55:49Z
UgxBp3iudyvfEIeDN7R4AaABAg.9Jh6HGZbW8a9JhwoJFN0GA,@stratascratch,UgxBp3iudyvfEIeDN7R4AaABAg,2,TYHWv1vT0Pk,0,1,2021-02-13T17:43:35Z,Thanks for watching! Let me know if you have other topics you&#39;d like for me to cover.,2021-02-13T17:43:35Z
Ugys3x0sKa39U2N1eXN4AaABAg,@mysteriousbd3743,,1,TYHWv1vT0Pk,1,0,2021-02-12T23:46:44Z,Excellent work,2021-02-12T23:46:44Z
Ugys3x0sKa39U2N1eXN4AaABAg.9Jg0_E380k99Jg24ldVuRU,@stratascratch,Ugys3x0sKa39U2N1eXN4AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-12T23:59:54Z,Thanks for watching!,2021-02-12T23:59:54Z
UgyijMwFMfMJqsGi8bF4AaABAg,@IrakliChitishvili,,1,TYHWv1vT0Pk,3,0,2021-02-12T13:03:57Z,Excellent. Immediately subbed.  Any plans in the future for  CTE walkthroughs and best practices?,2021-02-12T13:03:57Z
UgyijMwFMfMJqsGi8bF4AaABAg.9Jes0K05TSu9Jf9-pQyoOX,@stratascratch,UgyijMwFMfMJqsGi8bF4AaABAg,2,TYHWv1vT0Pk,0,1,2021-02-12T15:41:10Z,"Thanks for watching my videos! Yes, there have been some requests for CTE walkthroughs. I&#39;ve added that topic to my queue of videos and hoping to film that video sometime in late-March and April. I have a few videos coming out in the next few weeks that are going to focus on technical topics that have appeared on data science interviews in 2021. It&#39;s a bit time sensitive so I want to make sure I cover those topics first. After that, we&#39;ll dive into CTEs!",2021-02-12T15:41:10Z
UgyijMwFMfMJqsGi8bF4AaABAg.9Jes0K05TSu9Jh0ctX-vCO,@IrakliChitishvili,UgyijMwFMfMJqsGi8bF4AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-13T09:06:28Z,@@stratascratch Thanks! Of course there are tons of CTE resources but I like your particular style with practical and applicable angle.,2021-02-13T09:06:28Z
UgyijMwFMfMJqsGi8bF4AaABAg.9Jes0K05TSu9JhwOge1RO6,@stratascratch,UgyijMwFMfMJqsGi8bF4AaABAg,2,TYHWv1vT0Pk,0,1,2021-02-13T17:39:56Z,@@IrakliChitishvili Thanks so much! Will try to explain CTEs from a practical angle. There&#39;s probably a lot to say about CTEs vs subqueries vs temp tables and when to use them,2021-02-13T17:39:56Z
UgwDSK0_yIVA2d_HnLt4AaABAg,@mdabulkalamazad6775,,1,TYHWv1vT0Pk,1,0,2021-02-11T21:49:41Z,Thanks Sir,2021-02-11T21:49:41Z
UgwDSK0_yIVA2d_HnLt4AaABAg.9JdEO2l9BfN9JdUztDgCvR,@stratascratch,UgwDSK0_yIVA2d_HnLt4AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-12T00:14:47Z,Thanks for always watching my videos!,2021-02-12T00:14:47Z
UgxTFzD0-y9qr3YLc-14AaABAg,@lizard_sinno,,1,TYHWv1vT0Pk,1,0,2021-02-11T21:31:04Z,Great work!,2021-02-11T21:31:04Z
UgxTFzD0-y9qr3YLc-14AaABAg.9JdCFlz6PRI9JdUydH3cn2,@stratascratch,UgxTFzD0-y9qr3YLc-14AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-12T00:14:37Z,Thanks for watching! Glad you liked it.,2021-02-12T00:14:37Z
Ugy4C-pJBx1wvwzXPtJ4AaABAg,@priyankasarkar6600,,1,TYHWv1vT0Pk,2,0,2021-02-11T14:24:48Z,"Awesome! Thank you for being the best teacher in the world..:) Sir, once you&#39;re free , can you please make a video on CTE &amp;Temp table. For performance tuning which one is good? Can you create index on both ? explain why for yes and no? <br>And difference between Temp table &amp; Hash table?  which one is more applicable?",2021-02-11T14:24:48Z
Ugy4C-pJBx1wvwzXPtJ4AaABAg.9JcRTg1QvSp9JcgJT7s9NN,@stratascratch,Ugy4C-pJBx1wvwzXPtJ4AaABAg,2,TYHWv1vT0Pk,0,1,2021-02-11T16:43:13Z,"Yes, I can definitely do this. I have a queue of other videos right now but this topic will be on the list. I&#39;m hoping I can talk about this in April (sorry long queue of videos lined up =)).",2021-02-11T16:43:13Z
Ugy4C-pJBx1wvwzXPtJ4AaABAg.9JcRTg1QvSp9JcguvxSflE,@priyankasarkar6600,Ugy4C-pJBx1wvwzXPtJ4AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-11T16:48:28Z,@@stratascratch Great ! thank you sir :),2021-02-11T16:48:28Z
UgyCJ8Z_NH3n9PMeU7t4AaABAg,@NotFound-iu8wx,,1,TYHWv1vT0Pk,1,1,2021-02-11T13:03:15Z,"Instructions unclear and I manipulated my date and I am in jail now<br><br>Just kidding, great video as always",2021-02-11T13:03:15Z
UgyCJ8Z_NH3n9PMeU7t4AaABAg.9JcI8OLglY19Jcg9wGB9nT,@stratascratch,UgyCJ8Z_NH3n9PMeU7t4AaABAg,2,TYHWv1vT0Pk,0,1,2021-02-11T16:41:55Z,hah! Thanks for watching. You&#39;ve been a long time watcher so thanks for keeping up with my vids!,2021-02-11T16:41:55Z
UgwTcPK_DmRKKKG1AkV4AaABAg,@prernakalra5281,,1,TYHWv1vT0Pk,4,1,2021-02-11T12:51:54Z,"step by step explanation ,  great framework on how to approach any SQL Query !",2021-02-11T12:51:54Z
UgwTcPK_DmRKKKG1AkV4AaABAg.9JcGqCrl4A_9JcgD6eP9Cm,@stratascratch,UgwTcPK_DmRKKKG1AkV4AaABAg,2,TYHWv1vT0Pk,0,1,2021-02-11T16:42:21Z,"Thanks! Next week, we&#39;ll jump into some advance topics.",2021-02-11T16:42:21Z
UgwTcPK_DmRKKKG1AkV4AaABAg.9JcGqCrl4A_9JclHdnat6J,@prernakalra5281,UgwTcPK_DmRKKKG1AkV4AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-11T17:26:39Z,@@stratascratch would be great help if you share anyonline blog where i can practice sql interview questions for free. Solutions mostly are not given on many platforms,2021-02-11T17:27:11Z
UgwTcPK_DmRKKKG1AkV4AaABAg.9JcGqCrl4A_9JcmKqdFnn0,@stratascratch,UgwTcPK_DmRKKKG1AkV4AaABAg,2,TYHWv1vT0Pk,0,1,2021-02-11T17:35:50Z,"@@prernakalra5281 StrataScratch (https://platform.stratascratch.com) has 50 free SQL questions you can use to practice and LeetCode has a few free SQL questions as well. LeetCode sql questions focus mainly on syntax while SS focuses on interview questions (syntax + implementing edge cases/real life scenarios). HackerRank would be the other option which has some free sql questions. If you add all the free questions on those 3 platforms, you should have about 100-150!",2021-02-11T17:35:50Z
UgwTcPK_DmRKKKG1AkV4AaABAg.9JcGqCrl4A_9JcnbJhwXAh,@prernakalra5281,UgwTcPK_DmRKKKG1AkV4AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-11T17:46:57Z,"@@stratascratch Awesomee I am gonna practice all, I have to give sql online test from some company as part of inrerview process. Thank you so much :)",2021-02-11T17:46:57Z
UgynjWPmazd2tvBKYyR4AaABAg,@anuragsingh4766,,1,TYHWv1vT0Pk,1,1,2021-02-11T08:15:01Z,üëçüëçüëå,2021-02-11T08:15:01Z
UgynjWPmazd2tvBKYyR4AaABAg.9Jbm9JmRvXG9Jcg2eyhPKy,@stratascratch,UgynjWPmazd2tvBKYyR4AaABAg,2,TYHWv1vT0Pk,0,0,2021-02-11T16:40:55Z,Thanks for watching! We&#39;ll get into some advanced topics next week!,2021-02-11T16:40:55Z
Ugw4XL8yWjDF5QpdmOV4AaABAg,@stephenmukuria3565,,1,0GpgMvyN0Fg,0,0,2023-05-12T13:24:25Z,This is exactly the video that I was looking forü§©ü§© I&#39;m really impressed.<br>Could you be able to recommend some credible videos on product management? It&#39;s hard to tell who&#39;s really sharing relevant content. I would appreciate it.,2023-05-12T13:24:25Z
UgwQir9TYG3347wRV2F4AaABAg,@Rockrypt4,,1,0GpgMvyN0Fg,0,2,2023-03-08T04:55:12Z,Don‚Äôt understand how this channel doesn‚Äôt have millions of subs. thank you so much!,2023-03-08T04:55:12Z
Ugxn2CgarLkJjVi7Vn54AaABAg,@mihamilojevic5894,,1,0GpgMvyN0Fg,0,0,2022-10-19T14:21:31Z,awesome,2022-10-19T14:21:31Z
Ugzrzm3T4XEWHQE2PFZ4AaABAg,@Lone.wolf004,,1,0GpgMvyN0Fg,1,2,2022-09-09T07:18:05Z,my new fave youtuber rn. thanks sir. hopefully more of this kind of video,2022-09-09T07:18:05Z
Ugzrzm3T4XEWHQE2PFZ4AaABAg.9fkFiPuBKno9fl0g-mfQL7,@stratascratch,Ugzrzm3T4XEWHQE2PFZ4AaABAg,2,0GpgMvyN0Fg,0,0,2022-09-09T14:25:55Z,"Thank you and you&#39;re welcome.  Sure, we will continue to produce more videos like this.",2022-09-09T14:25:55Z
Ugysv4Ob0ZLIB77MxsV4AaABAg,@dethebare3865,,1,0GpgMvyN0Fg,1,0,2022-07-31T18:47:03Z,"Can you talk about how Data Visualization, information retrieval (data mining )and Data Warehousing plays into the framework you provided?",2022-07-31T18:47:03Z
Ugysv4Ob0ZLIB77MxsV4AaABAg.9e9Ulc3Ty5f9eWkGN4i1Tz,@stratascratch,Ugysv4Ob0ZLIB77MxsV4AaABAg,2,0GpgMvyN0Fg,0,0,2022-08-09T19:33:44Z,"Thanks for these suggestions. They are great, and it&#39;s something for us to consider. We&#39;ll see how this fits in our publication plan and try to come up with something interesting.",2022-08-09T19:33:44Z
Ugw14MjjJIUJk6ddW794AaABAg,@johns.conteh491,,1,0GpgMvyN0Fg,1,0,2022-05-13T19:07:56Z,Thanks for your helpful tips,2022-05-13T19:07:56Z
Ugw14MjjJIUJk6ddW794AaABAg.9az6MSB4N7E9kUzja4lFdP,@stratascratch,Ugw14MjjJIUJk6ddW794AaABAg,2,0GpgMvyN0Fg,0,0,2023-01-05T06:18:49Z,My pleasure üòä,2023-01-05T06:18:49Z
UgyrP4InC-bYx5crsmp4AaABAg,@Abhi-qi6wm,,1,0GpgMvyN0Fg,0,0,2021-12-30T15:36:46Z,Really liked the video. Could you make another video covering how one should navigate Kaggle as a beginner and explain the code from people who score high in popular competitions. Thanks,2021-12-30T15:36:46Z
Ugwruk8EUfRKU4lGJBl4AaABAg,@kuxinno1348,,1,0GpgMvyN0Fg,0,0,2021-12-03T06:34:01Z,I have a job so I can&#39;t make a perfect schedule. Could you give me some advise?,2021-12-03T06:34:01Z
Ugxbg_B8Wh_S1-c2_Zt4AaABAg,@jasonlin6320,,1,0GpgMvyN0Fg,0,0,2021-12-01T02:03:02Z,I wish I could come across your channel much earlier. Thank you.,2021-12-01T02:03:02Z
UgxeMI5ygeFNkFmsuX14AaABAg,@harryx.3250,,1,0GpgMvyN0Fg,0,1,2021-11-20T23:57:31Z,This channel deserves more like and subscribe! Great content Nate!,2021-11-20T23:57:31Z
UgwL_N5-vtVCsxgdVgx4AaABAg,@joaopedroreissilva7075,,1,0GpgMvyN0Fg,0,1,2021-10-15T12:19:28Z,"Thank you, Nate!<br>Amazing Content, you&#39;re helping a lot of people, inclusive me.",2021-10-15T12:19:28Z
UgwrbLaIEC8mlZhtuhp4AaABAg,@brutalidze1680,,1,0GpgMvyN0Fg,0,0,2021-10-13T19:29:02Z,Thank you!,2021-10-13T19:29:02Z
UgxpCNT09AXu1g1dbIV4AaABAg,@RespectTheConglomerate,,1,0GpgMvyN0Fg,0,0,2021-10-12T00:19:22Z,Thanks for the clarity!,2021-10-12T00:19:22Z
UgwHupk5EtjzFbnJtSV4AaABAg,@user-yr6xc7gg8q,,1,0GpgMvyN0Fg,0,0,2021-10-01T01:27:18Z,Buy statcrunch,2021-10-01T01:27:18Z
UgzYNz9D65cadfwKwwF4AaABAg,@hicarlee8762,,1,0GpgMvyN0Fg,1,2,2021-07-12T21:08:08Z,tysm 4 the video! next step is building confidence as I&#39;m learning sql/python; you&#39;ve put me in a better direction &lt;3,2021-07-12T21:08:08Z
UgzYNz9D65cadfwKwwF4AaABAg.9PhyeqRNdyN9Pi241lQsvi,@stratascratch,UgzYNz9D65cadfwKwwF4AaABAg,2,0GpgMvyN0Fg,0,2,2021-07-12T21:46:39Z,Thank you! I&#39;m glad that I&#39;ve helped in some way =),2021-07-12T21:46:39Z
UgxWmqMHI4ZUnrNaTCl4AaABAg,@JustinKuanomics,,1,0GpgMvyN0Fg,1,0,2021-07-12T18:21:40Z,"Enjoy your videos, found them helpful even as a data scientist already.<br><br>Hope you don&#39;t mind me commenting on some trivial details, but I did catch a few spelling errors that I wanted to bring to you attention.  In this specific instance, it is &quot;produce management principles&quot; when you mean &quot;product management principles&quot;.",2021-07-12T18:21:51Z
UgxWmqMHI4ZUnrNaTCl4AaABAg.9PhfbaL9D3l9Pi2BsA6Uim,@stratascratch,UgxWmqMHI4ZUnrNaTCl4AaABAg,2,0GpgMvyN0Fg,0,0,2021-07-12T21:47:43Z,"Thanks so much for letting me know. I changed what I could find but yea, I sometimes rush through writing the description...",2021-07-12T21:47:43Z
UgzCSDQ6x4MogXQRGIV4AaABAg,@monagulapa3022,,1,0GpgMvyN0Fg,2,43,2021-05-13T11:16:04Z,I have high respect for people like you who are so generous in sharing knowledge in this field of Data Science. Thank you so much.,2021-05-13T11:16:04Z
UgzCSDQ6x4MogXQRGIV4AaABAg.9NHQCzO5wD69NI-37ci3H3,@stratascratch,UgzCSDQ6x4MogXQRGIV4AaABAg,2,0GpgMvyN0Fg,0,3,2021-05-13T16:38:02Z,Thank you so much for the kind words. And glad you enjoyed these videos!,2021-05-13T16:38:02Z
UgzCSDQ6x4MogXQRGIV4AaABAg.9NHQCzO5wD69Vfp5l4w-Sa,@ellosteve,UgzCSDQ6x4MogXQRGIV4AaABAg,2,0GpgMvyN0Fg,0,0,2021-12-08T04:14:26Z,"Seriously second that!!! Thank you so much, Nate!",2021-12-08T04:14:26Z
UgzFUpKoxWvm36wiIWJ4AaABAg,@richarda1630,,1,0GpgMvyN0Fg,4,0,2021-03-14T11:26:02Z,"Thanks for your guidance. May I ask, based on your experience in the industry now, how prevalent is the use of AutoML ?",2021-03-14T11:26:02Z
UgzFUpKoxWvm36wiIWJ4AaABAg.9KrweS7AcJC9Kt2VOvA9VZ,@stratascratch,UgzFUpKoxWvm36wiIWJ4AaABAg,2,0GpgMvyN0Fg,0,1,2021-03-14T21:45:04Z,"I&#39;ve heard of autoML but have never used it. Unfortunately, I don&#39;t know too many teams that use it. I know of one team that uses autoML for forecasting work. They&#39;re not data scientists but forecasters by training so the use of autoML helps them since they&#39;re not super technical folks. I hope that helps.",2021-03-14T21:45:04Z
UgzFUpKoxWvm36wiIWJ4AaABAg.9KrweS7AcJC9KtOWnVZX1B,@richarda1630,UgzFUpKoxWvm36wiIWJ4AaABAg,2,0GpgMvyN0Fg,0,0,2021-03-15T00:57:30Z,"@@stratascratch yes it does alot ! last question , what autoML do they use? super thanks!",2021-03-15T00:57:30Z
UgzFUpKoxWvm36wiIWJ4AaABAg.9KrweS7AcJC9KtYdVokj1c,@stratascratch,UgzFUpKoxWvm36wiIWJ4AaABAg,2,0GpgMvyN0Fg,0,1,2021-03-15T02:25:56Z,"@@richarda1630 I believe they use AWS Sagemaker with Autopilot (<a href=""https://aws.amazon.com/sagemaker/autopilot/)"">https://aws.amazon.com/sagemaker/autopilot/)</a>. Hope that helps!",2021-03-15T02:25:56Z
UgzFUpKoxWvm36wiIWJ4AaABAg.9KrweS7AcJC9KtbMzcQ9g_,@richarda1630,UgzFUpKoxWvm36wiIWJ4AaABAg,2,0GpgMvyN0Fg,0,0,2021-03-15T02:58:29Z,@@stratascratch Yes it does! thanks so much once again :D,2021-03-15T02:58:29Z
UgyXC9VGq_hM_NzG_H94AaABAg,@kellyt285,,1,0GpgMvyN0Fg,2,0,2021-03-07T06:50:14Z,Is udemy a good sources? I was thinking doing their class.,2021-03-07T06:50:14Z
UgyXC9VGq_hM_NzG_H94AaABAg.9K_QXKHKw1e9KbHvzCxvlR,@stratascratch,UgyXC9VGq_hM_NzG_H94AaABAg,2,0GpgMvyN0Fg,0,3,2021-03-08T00:13:34Z,I think it&#39;s a good starting point if you have no experience. But you definitely need to try platforms with real examples and exercises if you want to progress past the intermediate level.,2021-03-08T00:13:34Z
UgyXC9VGq_hM_NzG_H94AaABAg.9K_QXKHKw1e9KbHznDfOys,@kellyt285,UgyXC9VGq_hM_NzG_H94AaABAg,2,0GpgMvyN0Fg,0,1,2021-03-08T00:14:06Z,Thank you,2021-03-08T00:14:06Z
Ugzhu9_OWQyCikt_lNF4AaABAg,@BBBBBBAAAl,,1,0GpgMvyN0Fg,2,2,2021-02-09T16:11:16Z,Thanks alot. Do you think being a DBA for about 3 years can translate in to a softer landing for anything beside SQL? Been looking around for a while on how to approach Data Science and your video really helped me.,2021-02-09T16:11:16Z
Ugzhu9_OWQyCikt_lNF4AaABAg.9JYU3fS3DhA9JYZBlPRu-4,@stratascratch,Ugzhu9_OWQyCikt_lNF4AaABAg,2,0GpgMvyN0Fg,0,7,2021-02-09T16:56:03Z,"Yes, I do think a DBA can definitely make it&#39;s way to data science. There&#39;s a current trend of data scientists that specialize in data engineering/pipeline building so your skills would be greatly appreciated here. A difference is that while DBAs mainly work in SQL, building pipelines for data scientists often include knowledge in python and Airflow (or similar automation technologies). Once you get good at python, you can start developing more statistical and mathematical skills. <br><br>Another path is to become a data analyst, which mainly codes in SQL and python, but does not build models. This gives you the skills to solve business problems and work with stakeholders. The career trajectory here is to then become someone that deals with product/marketing analytics solving product related questions to drive growth for both the product and marketing teams. From here you can then build your skills in more statistics and math so that you can build ML models. <br><br>Most &quot;Data science&quot; jobs do not build ML models so even if you go down the path as a data science engineer or in product/marketing analytics, you&#39;re on the level as most data scientists.",2021-02-09T16:56:03Z
Ugzhu9_OWQyCikt_lNF4AaABAg.9JYU3fS3DhA9J_vfy9jHDA,@BBBBBBAAAl,Ugzhu9_OWQyCikt_lNF4AaABAg,2,0GpgMvyN0Fg,0,1,2021-02-10T14:59:46Z,@@stratascratch  Thank you! priceless advices. will continue following you and learn.,2021-02-10T14:59:46Z
UgzFv_6nz0JybnydrUx4AaABAg,@mdabulkalamazad6775,,1,0GpgMvyN0Fg,1,0,2021-02-08T20:34:14Z,"Excellent Video, Loved it. Thank You !!",2021-02-08T20:34:14Z
UgzFv_6nz0JybnydrUx4AaABAg.9JWNMuUkPd-9JWj6SVmd3E,@stratascratch,UgzFv_6nz0JybnydrUx4AaABAg,2,0GpgMvyN0Fg,0,0,2021-02-08T23:52:58Z,Thanks so much for watching!,2021-02-08T23:52:58Z
UgxNASm488Qt9y80IK94AaABAg,@Gamma3,,1,0GpgMvyN0Fg,1,1,2021-02-07T02:58:12Z,Thanks. Great video. I will suscribe,2021-02-07T02:58:12Z
UgxNASm488Qt9y80IK94AaABAg.9JRuiNQtx_l9JRyTkoMLDx,@stratascratch,UgxNASm488Qt9y80IK94AaABAg,2,0GpgMvyN0Fg,0,1,2021-02-07T03:31:01Z,"Thanks for watching! if you have any requests for topics, let me know and I&#39;ll see if I can cover them.",2021-02-07T03:31:01Z
UgywAhE5sKQKUgu5awp4AaABAg,@followmycrafts8811,,1,0GpgMvyN0Fg,1,0,2021-02-06T10:52:15Z,Thanks for your clear instruction and resources,2021-02-06T10:52:15Z
UgywAhE5sKQKUgu5awp4AaABAg.9JQBAY3nNfa9JQecUw_I80,@stratascratch,UgywAhE5sKQKUgu5awp4AaABAg,2,0GpgMvyN0Fg,0,0,2021-02-06T15:18:21Z,Thanks for watching! Appreciate the comments you&#39;ve made throughout the other vids as well.,2021-02-06T15:18:21Z
Ugz1XYXQ4_7wSRsPhyB4AaABAg,@rakibraihanrimon8784,,1,0GpgMvyN0Fg,1,0,2021-02-05T12:42:43Z,Thanks,2021-02-05T12:42:43Z
Ugz1XYXQ4_7wSRsPhyB4AaABAg.9JNo0jbm27h9JOI4CfVSSx,@stratascratch,Ugz1XYXQ4_7wSRsPhyB4AaABAg,2,0GpgMvyN0Fg,0,0,2021-02-05T17:14:04Z,Thanks for watching. Happy to also cover any topics you have in mind.,2021-02-05T17:14:04Z
UgzDIqfaQnXC1OIGSIp4AaABAg,@MehmetMustafaICER,,1,0GpgMvyN0Fg,1,0,2021-02-05T10:17:34Z,Thank you. I appreciate the content. Really liked it.,2021-02-05T10:17:34Z
UgzDIqfaQnXC1OIGSIp4AaABAg.9JNYPZWtgYk9JOHzmVeha9,@stratascratch,UgzDIqfaQnXC1OIGSIp4AaABAg,2,0GpgMvyN0Fg,0,0,2021-02-05T17:13:20Z,"Thanks for watching! If there&#39;s any other topic you&#39;d like me to cover, please let me know!",2021-02-05T17:13:20Z
UgztrHGA3NfXIu-TiZB4AaABAg,@priyankasarkar6600,,1,0GpgMvyN0Fg,2,0,2021-02-05T02:33:59Z,Thank you sir!,2021-02-05T02:33:59Z
UgztrHGA3NfXIu-TiZB4AaABAg.9JMiMB46oBk9JOI2dIVKR9,@stratascratch,UgztrHGA3NfXIu-TiZB4AaABAg,2,0GpgMvyN0Fg,0,1,2021-02-05T17:13:51Z,Thank you for watching the video! Let me know if you have other suggestions for topics for me to cover.,2021-02-05T17:13:51Z
UgztrHGA3NfXIu-TiZB4AaABAg.9JMiMB46oBk9JPDFSh71qs,@priyankasarkar6600,UgztrHGA3NfXIu-TiZB4AaABAg,2,0GpgMvyN0Fg,0,0,2021-02-06T01:51:09Z,@@stratascratch Yes Sir. :),2021-02-06T01:51:09Z
UgxTkROAcs6YNfMOUWB4AaABAg,@sauravkumar9454,,1,0GpgMvyN0Fg,1,1,2021-02-04T16:28:12Z,A lot of details to go through. Gives a clear picture. Loved it. Thanks.,2021-02-04T16:28:12Z
UgxTkROAcs6YNfMOUWB4AaABAg.9JLd1KnTa1J9JLejTWO0r7,@stratascratch,UgxTkROAcs6YNfMOUWB4AaABAg,2,0GpgMvyN0Fg,0,0,2021-02-04T16:43:06Z,Thanks for watching! I hope that at least you have some resources you can explore to improve your skills.,2021-02-04T16:43:06Z
UgwDdpzAVfEeXFApOAF4AaABAg,@majafuntv4538,,1,0GpgMvyN0Fg,1,0,2021-02-04T10:00:45Z,thank you,2021-02-04T10:00:45Z
UgwDdpzAVfEeXFApOAF4AaABAg.9JKwgbMtCIe9JLek8OQiap,@stratascratch,UgwDdpzAVfEeXFApOAF4AaABAg,2,0GpgMvyN0Fg,0,0,2021-02-04T16:43:11Z,Thanks for watching!,2021-02-04T16:43:11Z
UgzIoB-qVjDCY9V8BZV4AaABAg,@marktoledo6595,,1,vLjAG9eXkcU,1,0,2023-09-24T12:40:27Z,"Do you or does anyone have a recommendation, either books or courses, where I could deep dive this type of workflow when querying databases?",2023-09-24T12:40:27Z
UgzIoB-qVjDCY9V8BZV4AaABAg.9v2Ifx6WZBK9v62GBxV6vt,@stratascratch,UgzIoB-qVjDCY9V8BZV4AaABAg,2,vLjAG9eXkcU,0,0,2023-09-25T23:33:56Z,"It&#39;s all practice in my opinion. You can try on StrataScratch, Leetcode, HackerRank, etc. I would try to solve the problems yourself and look at other user solutions.",2023-09-25T23:33:56Z
UgyrcZ9XxlVYBvQd9mp4AaABAg,@SundarRaj-bm8lf,,1,vLjAG9eXkcU,0,0,2023-03-16T19:27:11Z,Need to say.. best video on SQL for knowing how to solve complex queries,2023-03-16T19:27:11Z
UgyhHhN48A5fxU8sjJp4AaABAg,@nidhigupta5977,,1,vLjAG9eXkcU,0,0,2022-10-21T10:29:25Z,"is this correct? can someone check?<br><br>with ranks as (<br>select *, dense_rank() over (order by count_of_cateogry desc) as ranks from (<br>select pe_description, count(*) as count_of_cateogry<br>from los_angeles_restaurant_health_inspections<br>where lower(facility_name) Like &#39;%cafe%&#39; <br>or lower(facility_name) Like &#39;%tea%&#39;<br>or lower(facility_name) Like &#39;%juice%&#39;<br>group by pe_description)temp )<br><br>select la.facility_name, r.pe_description from ranks r <br>join los_angeles_restaurant_health_inspections la on r.pe_description = la.pe_description<br>where ranks = 3",2022-10-21T10:29:25Z
UgzAUorw2GWTLdlukTt4AaABAg,@dwaipayansaha4443,,1,vLjAG9eXkcU,1,2,2022-08-24T19:51:02Z,"My sql solution:-<br><br>with t1 as(select facility_name,pe_description,record_id from los_angeles_restaurant_health_inspections lashi<br>where facility_name like &#39;%TEA%&#39; or<br>facility_name like &#39;%CAFE%&#39; or<br>facility_name like &#39;%JUICE%&#39;),<br>t2 as(select pe_description, count(record_id) no_rec,rank() over(order by count(record_id) desc) rnk from t1 group by pe_description)<br>select facility_name from t1<br>join t2<br>on t1.pe_description=t2.pe_description<br>where rnk=3",2022-08-24T19:51:02Z
UgzAUorw2GWTLdlukTt4AaABAg.9f7PAEu-Mf69f7lIPUBkNQ,@stratascratch,UgzAUorw2GWTLdlukTt4AaABAg,2,vLjAG9eXkcU,0,0,2022-08-24T23:13:08Z,Thank you for sharing!,2022-08-24T23:13:08Z
UgyFRvC2mOTrXTxvulx4AaABAg,@dwaipayansaha4443,,1,vLjAG9eXkcU,0,0,2022-08-24T19:32:15Z,"Hey there, anybody searching for the python version of the solution:-<br><br>df=df[[&#39;facility_name&#39;,&#39;pe_description&#39;,&#39;record_id&#39;]]<br>df[&#39;business_list&#39;]=df[&#39;facility_name&#39;].str.split()<br>df=df.explode(&#39;business_list&#39;).reset_index(drop=True)<br>df=df[(df[&#39;business_list&#39;]==&#39;CAFE&#39;) | (df[&#39;business_list&#39;]==&#39;TEA&#39;) | (df[&#39;business_list&#39;]==&#39;JUICE&#39;)]<br>df1=df.groupby(&#39;pe_description&#39;)[&#39;record_id&#39;].count().reset_index().sort_values(by=&#39;record_id&#39;,ascending=False)<br>df1[&#39;rank&#39;]=df1[&#39;record_id&#39;].rank(ascending=False,method=&#39;dense&#39;)<br>df=pd.merge(df,df1,on=&#39;pe_description&#39;,how=&#39;inner&#39;)<br>df=df[df[&#39;rank&#39;]==3][&#39;facility_name&#39;]<br><br>Happy Coding!! üòÉ<br><br>hey Nate!!<br>A request to you, please share videos with python solutions. So we can learn both python and sql.",2022-08-24T19:38:22Z
UgwaMBXNKuWw-EabhQ54AaABAg,@sandeepnaidu2157,,1,vLjAG9eXkcU,1,1,2022-08-17T03:56:08Z,"Thanks Nate! Your videos are so useful to understand how to break down and approach interview questions.<br>Also, just for reference, here is a simplified solution to the same question:<br><br>with cte_3rd_most as (<br>select<br>pe_description<br>from (<br>select<br>pe_description<br>, dense_rank() over(order by count(facility_name) desc) as rk<br>from (<br>    select <br>    facility_name<br>    , pe_description<br>    from los_angeles_restaurant_health_inspections<br>    where lower(facility_name) ~ &#39;\y(tea|cafe|juice)\y&#39; ) as t1<br>group by pe_description) as t2<br>where rk = 3)<br><br>select <br>    facility_name<br>    from los_angeles_restaurant_health_inspections<br>    where lower(facility_name) ~ &#39;\y(tea|cafe|juice)\y&#39;<br>    and pe_description in (select pe_description from cte_3rd_most) ;",2022-08-17T03:58:21Z
UgwaMBXNKuWw-EabhQ54AaABAg.9eofKDWtf8S9erqXdN6zBs,@stratascratch,UgwaMBXNKuWw-EabhQ54AaABAg,2,vLjAG9eXkcU,0,1,2022-08-18T09:31:49Z,That is wonderful.  Keep rockin!,2022-08-18T09:31:49Z
UgxbhQkiyVvy_CPbrdR4AaABAg,@supriyam6896,,1,vLjAG9eXkcU,0,0,2022-06-24T11:48:20Z,"WITH CTE AS<br>(SELECT pe_description, COUNT(record_id) AS n_issues, DENSE_RANK() OVER(ORDER BY COUNT(record_id) DESC) AS rnk<br>FROM los_angeles_restaurant_health_inspections<br>WHERE lower(facility_name) REGEXP &#39;(cafe|tea|juice)&#39;<br>GROUP BY pe_description)<br><br>SELECT facility_name<br>FROM los_angeles_restaurant_health_inspections L<br>JOIN CTE <br>ON L.pe_description = CTE.pe_description<br>WHERE  rnk = 3 AND lower(facility_name) REGEXP &#39;(cafe|tea|juice)&#39;;",2022-06-24T11:48:20Z
Ugyku75RhzyNmiWr_v94AaABAg,@kaiyao5135,,1,vLjAG9eXkcU,2,0,2022-03-13T00:34:15Z,"Thanks Nate! Love your channel! Just a question after watching a couple of your videos. I cannot find some of the cases in your video in StrataScratch, and I guess they are removed or renamed? This is a little bit inconvenient for me as I cannot preview the table and follow your step by step to test the stage output, only frequently pausing the video to see the structure of the original table, and if my syntax and structure are the same with yours. I&#39;ve noticed that each case has a case Id, and I have been trying to search by this ID in StrataScratch but does not work.",2022-03-13T00:37:05Z
Ugyku75RhzyNmiWr_v94AaABAg.9ZV2Q897RKT9ZV2sn7tAcJ,@kaiyao5135,Ugyku75RhzyNmiWr_v94AaABAg,2,vLjAG9eXkcU,0,0,2022-03-13T00:38:18Z,I am not sure if it is the case.Maybe due to my bad search approachüòÇ,2022-03-13T00:38:18Z
Ugyku75RhzyNmiWr_v94AaABAg.9ZV2Q897RKT9ZXDM0cJ1nz,@stratascratch,Ugyku75RhzyNmiWr_v94AaABAg,2,vLjAG9eXkcU,0,0,2022-03-13T20:48:17Z,"Hi, you&#39;re right that some of the questions aren&#39;t found in the platform anymore. If you click on the direct link in the description, you&#39;ll still have access to the question. But the question itself isn&#39;t searchable on the platform. That&#39;s because we&#39;ve had to take a few off the platform (I think around 4?). Most videos will be searchable on the platform though. You can search using the question&#39;s title. Thanks for following and sorry for the inconvenience. Unfortunately, those 4 that were taken off happen to be very popular on YT",2022-03-13T20:48:17Z
UgwhYMyxY67ZCMSkt8F4AaABAg,@SamairaJain11,,1,vLjAG9eXkcU,1,1,2022-03-04T18:16:15Z,I have one doubt-if we use dense-rank function to find top 3 and then just filter where rank=3 then we don‚Äôt need 3rd CTE i.e Categories,2022-03-04T18:16:15Z
UgwhYMyxY67ZCMSkt8F4AaABAg.9Z9ln_4lnet9ZAPsZIWy92,@stratascratch,UgwhYMyxY67ZCMSkt8F4AaABAg,2,vLjAG9eXkcU,0,0,2022-03-05T00:15:11Z,Probably works! Try it out on the platform and see if you get the same output.,2022-03-05T00:15:11Z
UgwbYXce4ZN2cO26cR94AaABAg,@ApurvAnshuman,,1,vLjAG9eXkcU,0,0,2022-01-13T15:47:53Z,"How to go for subqueries, any study material you would suggest",2022-01-13T15:47:53Z
Ugz_EK-CjohK7ygFcX54AaABAg,@shobhamourya8396,,1,vLjAG9eXkcU,1,0,2021-11-29T12:56:12Z,"Hi Nate. I couldn&#39;t understand how to find the number of reported issues and was beating around the bush with pe_code and then peeked at your solution ;) I think the problem description should have been more clear...<br><br>Here&#39;s my solution using the rank window function and only one join<br><br>with cte1<br>as<br>(<br>    select pe_description, record_id, facility_name<br>    from los_angeles_restaurant_health_inspections<br>    where facility_name ilike &#39;%TEA%&#39; or<br>          facility_name ilike &#39;%CAFE%&#39; or<br>          facility_name ilike &#39;%JUICE%&#39;<br>    order by facility_name<br>),<br>cte2<br>as<br>(<br>    select pe_description, count(record_id) num_of_issues<br>    from cte1<br>    group by pe_description<br>    order by num_of_issues desc<br>),<br>cte3<br>as<br>(<br>    select pe_description, num_of_issues, <br>       dense_rank() over(order by num_of_issues desc) rank_num_of_issues<br>    from cte2<br>)<br>select facility_name<br>from cte1 <br>where pe_description IN (select pe_description <br>                              from cte3<br>                              where  rank_num_of_issues = 3)",2021-11-29T12:56:12Z
Ugz_EK-CjohK7ygFcX54AaABAg.9VK_eKWIuRU9VLYDrqYR81,@stratascratch,Ugz_EK-CjohK7ygFcX54AaABAg,2,vLjAG9eXkcU,0,1,2021-11-29T21:54:13Z,"Thanks for trying it! If you have any questions and/or feedback about the question, feel free to leave it in the discussion board of the question. The link is in the description. Someone from my team will help you and revise the questionn, if needed. Thanks for trying out the questions!",2021-11-29T21:54:13Z
Ugy1sYK3WaEGU3B4JYd4AaABAg,@redleo4257,,1,vLjAG9eXkcU,0,0,2021-10-14T17:46:03Z,One word only‚Ä¶ Amazing!!! The way u explained. I was having so much confusion &amp; was looking for some internal tips n boom.. subscribed..,2021-10-14T17:46:03Z
UgzXGVxlidQrMAGetdx4AaABAg,@AnuragSingh-vv3qv,,1,vLjAG9eXkcU,1,0,2021-10-06T09:36:43Z,Wow so good,2021-10-06T09:36:43Z
UgzXGVxlidQrMAGetdx4AaABAg.9T9Au69FaQg9T9d37vbAms,@stratascratch,UgzXGVxlidQrMAGetdx4AaABAg,2,vLjAG9eXkcU,0,0,2021-10-06T13:51:30Z,Thanks for watching!,2021-10-06T13:51:30Z
Ugy482T6VGH_S15Kk514AaABAg,@dianadennis7225,,1,vLjAG9eXkcU,1,5,2021-09-10T01:28:24Z,Im new to your channel and so happy I ve found it; quite rare to find real world queries clearly explained beginning to end. I&#39;m writing similarly long queries at work(I m a BI) and it was a bit scary at the beginning. <br>I m curious what are some advanced SQL concepts in data science? <br>Thank you for the great content!,2021-09-10T01:28:24Z
Ugy482T6VGH_S15Kk514AaABAg.9S5MLidNfEm9S71VLM00pP,@stratascratch,Ugy482T6VGH_S15Kk514AaABAg,2,vLjAG9eXkcU,0,2,2021-09-10T17:04:41Z,"A lot of the advanced concepts are things like window partitions, rankings, playing around with dates/timestamps, and subqueries. But at work, what makes things advanced is the complexity of the problem and having to deal with problems with a lot of corner cases or problems where you need to manipulate/clean the data more than normal. This makes your code long even if advanced functions aren&#39;t used. On interviews, they&#39;ll test you on the latter (questions with corner cases) and test your ability to identify and solve each corner case. Whether or not you use an advanced sql concept is up to you as you can usually solve problems without them (but it might take longer)",2021-09-10T17:04:41Z
Ugx4KWUsmA-S8_XwIHh4AaABAg,@Tridentor,,1,vLjAG9eXkcU,1,0,2021-09-08T17:28:26Z,Thanks Nate! An idea for the follow-up video: how would you refactor this code into smth more &#39;production-grade&#39; ?,2021-09-08T17:28:26Z
Ugx4KWUsmA-S8_XwIHh4AaABAg.9S1vcVEGzJR9S2MxT-Zi3y,@stratascratch,Ugx4KWUsmA-S8_XwIHh4AaABAg,2,vLjAG9eXkcU,0,0,2021-09-08T21:35:58Z,Lots of refactoring for production-grade code =),2021-09-08T21:35:58Z
UgyZUNiB-Na6CD_4aQp4AaABAg,@abhinasneupane2392,,1,vLjAG9eXkcU,1,1,2021-09-04T06:37:29Z,"Thanks for making these videos, you are awesome. Like you explained it is really hard to find these tips specially in SQL and though process  on solving problems.  Usually we only find basic syntax / or anything related to what is joins are  , what is rank or sql . Which we never get asked in interview.",2021-09-04T06:37:29Z
UgyZUNiB-Na6CD_4aQp4AaABAg.9RrSxKIrDK49RsK_tP96q_,@stratascratch,UgyZUNiB-Na6CD_4aQp4AaABAg,2,vLjAG9eXkcU,0,1,2021-09-04T14:43:37Z,Thanks for watching! Totally agree with you. YT has a ton of basic SQL stuff but nothing overly complicated. Glad I can fill the gap!,2021-09-04T14:43:37Z
Ugx3kO6C80JVDUclCIN4AaABAg,@niveditakumari701,,1,vLjAG9eXkcU,1,0,2021-08-12T19:55:27Z,"Hey, really helpful explanation, Can we also do a OFFSET along with LIMIT to get 3rd highest issue?",2021-08-12T19:55:27Z
Ugx3kO6C80JVDUclCIN4AaABAg.9QxezceSor-9QxwYYYdryo,@stratascratch,Ugx3kO6C80JVDUclCIN4AaABAg,2,vLjAG9eXkcU,0,0,2021-08-12T22:28:54Z,I think so? give it a try on the platform and see if you get the same solution!,2021-08-12T22:28:54Z
Ugzk5d_pu55F4STykV14AaABAg,@majorcemp3612,,1,vLjAG9eXkcU,1,1,2021-07-26T10:49:52Z,"Hi, is it possible to replace the where Ilike by case when ilike here to use count ? if yes how would it be, and would it be more efficient or no ? :D",2021-07-26T10:49:52Z
Ugzk5d_pu55F4STykV14AaABAg.9QFv1oN0nGu9QGLGJLgJ0A,@stratascratch,Ugzk5d_pu55F4STykV14AaABAg,2,vLjAG9eXkcU,0,1,2021-07-26T14:47:46Z,"It would be just as efficient to use a case statement vs ilike. You can give it a try in the platform (link in the description). If you get stuck, just ask for help in the discussion. Someone from my team or myself will answer you!",2021-07-26T14:47:46Z
UgwNmQltNT52nwhDZ7t4AaABAg,@jimwoodward7293,,1,vLjAG9eXkcU,1,0,2021-07-25T04:14:10Z,This approach is very helpful -- thanks for documenting how you solve these types of SQL problems!,2021-07-25T04:14:10Z
UgwNmQltNT52nwhDZ7t4AaABAg.9QCcxnLglCG9QDzVN9X8DV,@stratascratch,UgwNmQltNT52nwhDZ7t4AaABAg,2,vLjAG9eXkcU,0,0,2021-07-25T16:50:22Z,"Thanks a lot, Jim! Thanks for watching my videos!",2021-07-25T16:50:22Z
UgzZConyoVx8CUv1MLh4AaABAg,@nan4061,,1,vLjAG9eXkcU,1,4,2021-05-13T14:32:42Z,"Excellent, excellent, excellent!! Thank you for sharing, this is super helpful!!",2021-05-13T14:32:42Z
UgzZConyoVx8CUv1MLh4AaABAg.9NHli9ecCCn9NI-542C2Rk,@stratascratch,UgzZConyoVx8CUv1MLh4AaABAg,2,vLjAG9eXkcU,0,0,2021-05-13T16:38:18Z,Thank you for watching!,2021-05-13T16:38:18Z
UgxRc140mgxjk9cohNl4AaABAg,@MeerVideos,,1,vLjAG9eXkcU,1,0,2021-03-19T04:44:28Z,Why didn&#39;t I find your videos earlier? -_-<br><br>I guess my WHERE clause weren&#39;t too specific :p,2021-03-19T04:44:51Z
UgxRc140mgxjk9cohNl4AaABAg.9L35fdZUXwO9L4L085wqI7,@stratascratch,UgxRc140mgxjk9cohNl4AaABAg,2,vLjAG9eXkcU,0,1,2021-03-19T16:17:42Z,=) Expert level joke! Glad you found my channel!,2021-03-19T16:17:42Z
UgyNuWJ5DIVxGJOX2Ol4AaABAg,@PATRICKCHUAD,,1,vLjAG9eXkcU,2,0,2021-02-28T04:25:53Z,"HI Nate, i have a problem that require to get the total lenght for these data  [0,30] [5,10] [15,20] [25,40] where we dont count the overlapp more than once. Can you pls try to solve this using SQL?",2021-02-28T04:25:53Z
UgyNuWJ5DIVxGJOX2Ol4AaABAg.9KI8Rvsf3qb9KMRCmqq50f,@stratascratch,UgyNuWJ5DIVxGJOX2Ol4AaABAg,2,vLjAG9eXkcU,0,1,2021-03-01T20:26:48Z,This might be better to solve on python =),2021-03-01T20:26:48Z
UgyNuWJ5DIVxGJOX2Ol4AaABAg.9KI8Rvsf3qb9KN-YSSRWfO,@PATRICKCHUAD,UgyNuWJ5DIVxGJOX2Ol4AaABAg,2,vLjAG9eXkcU,0,0,2021-03-02T01:44:20Z,"@@stratascratch i see<br><br>i Tried to solve it using SQL and using CTE which you have taught in your sample. Below is the code.  The answer seems correct when I simulate it PostgreSQL.<br><br>FIND THE TOTAL LENGHT FROM THE LIST OF SEGMENT <br><br>	-- 5<br>	WITH MAX_CTE AS (<br>	SELECT 1 x , b.l, max(b.r) max from segments b<br>					 group by x, b.l<br>					 order by x,b.l<br>	)<br>	,<br>	OVERLAP AS (<br>		SELECT 1 as x , a.l as al, c.l ,c.max ,<br>					(case when a.l &gt; c.l and a.l &lt; c.max and a.r &gt; c.max then &#39;overlap&#39; else &#39;&#39;  end ) overl,<br>					(case when a.l &gt; c.l and a.l &lt; c.max  and a.r &gt; c.max then a.l else  c.max end ) newl<br>				FROM segments a<br>				LEFT JOIN MAX_CTE c<br>				ON x = c.x <br>	)		<br>	,<br>	GET_MIN  AS (<br>	SELECT max(e.l), max(e.newl) max2, max(e.max) emax <br>			from (<br>				SELECT d.l,d.newl,d.max from <br>				OVERLAP d<br>				) e<br>			-- 3<br>			GROUP BY e.newl, e.l<br>			ORDER BY l,newl<br>	)<br>	,<br>	<br>	SAMPLE_CTE AS (<br>		-- 4<br>		select f.max, min(f.max2) from <br>		     GET_MIN f<br>		group by f.max <br>		order by f.max<br>		-- 4<br>		) <br>		,<br>		SAMPLE_CTE5 AS (<br>		SELECT 1 AS X, MIN(Min)  MIN4 , MAX(MIN) MAX4<br>					FROM SAMPLE_CTE<br>					GROUP BY X<br>		)<br>		,<br>		SAMPLE_CTE45 AS (<br>		SELECT 1 AS X,min AS MIN45, min-max as  lenght2 FROM SAMPLE_CTE order by max asc <br>			LIMIT 1<br>		)<br>		,<br>	SAMPLE_CTE55 AS (<br>		select 1 AS X,max as min from SAMPLE_CTE GROUP BY max LIMIT 1<br>	    )<br>	    ,<br>		SAMPLE_CTE3 AS (<br>   			SELECT 1 AS X,yy.MAX AS l, yy.MIN as r ,  yy.min,<br>	         ZZ.MIN45, JJ.MIN, zz.lenght2,MAX4, yy.min - yy.max  as lenght,<br>          	(case when yy.max &lt;= ZZ.MIN45<br>			      then 0 else zz.MIN45 - yy.max end ) as left,<br>			<br>			 (case when yy.MIN &gt; zz.MIN45 <br>			    then yy.min - ZZ.MIN45 else 0  end ) as right<br>			<br>			FROM SAMPLE_CTE YY<br>			LEFT JOIN SAMPLE_CTE45 ZZ<br>			ON ZZ.X=X<br>			LEFT JOIN SAMPLE_CTE5 XX<br>			ON xx.X = XX.X<br>			LEFT JOIN SAMPLE_CTE55 JJ<br>			ON XX.X = JJ.X<br>			)<br>	    ,<br>		SAMPLE_CTE4 AS (<br>		SELECT 1AS X,L,R,R-L AS LEN from SAMPLE_CTE3<br>		ORDER BY L <br>		LIMIT 1)<br><br>		,<br>		SAMPLE_CTE6 AS (<br>		SELECT * FROM SAMPLE_CTE4 A<br>		LEFT JOIN SAMPLE_CTE5 B<br>		ON A.X=B.X<br>		)<br>		<br>		--SELECT * FROM SAMPLE_CTE6<br>		<br>		select a.x,sum(A.left)+sum(A.right)+MAX(B.LEN) Total_length,MAX(B.LEN) as first_element_lenght ,<br>		sum(A.left)+sum(A.right) Others_Lenght<br>		froM SAMPLE_CTE3 A<br>	    LEFT JOIN SAMPLE_CTE4 B<br>		ON A.x=B.x <br>		GROUP BY A.x",2021-03-02T01:55:08Z
UgyiO-WnrrOZbb08Ewp4AaABAg,@PATRICKCHUAD,,1,vLjAG9eXkcU,12,0,2021-02-14T13:25:05Z,Thanks for sharing. Really find this sample very helpful.,2021-02-14T13:25:05Z
UgyiO-WnrrOZbb08Ewp4AaABAg.9Jk30vV70BM9JkXikL-5j0,@stratascratch,UgyiO-WnrrOZbb08Ewp4AaABAg,2,vLjAG9eXkcU,0,0,2021-02-14T17:53:21Z,Thanks so much for watching.,2021-02-14T17:53:21Z
UgyiO-WnrrOZbb08Ewp4AaABAg.9Jk30vV70BM9JlMrEtZngV,@PATRICKCHUAD,UgyiO-WnrrOZbb08Ewp4AaABAg,2,vLjAG9eXkcU,0,0,2021-02-15T01:37:38Z,"@@stratascratch I tried the CTE but got error  like &quot;with is not valid at this position for this server version&quot;.  I&#39;m not able to fix it. so I did subquery instead and here&#39;s my solution.<br><br><br>SELECT  facility_name <br>FROM <br> (<br><br>                   SELECT <a href=""http://qq.mini/"">qq.minI</a>,Y.pe_desc<br>                   FROM  <br>                   (<br>                   SELECT min(Q.n_issues) as minI from  <br>                           (SELECT pe_desc,sum(score) n_issues<br>                           FROM facility <br>                           GROUP BY pe_desc<br>                           ORDER BY  n_issues DESC  limit 3 ) Q <br>                   ) as qq<br>                  LEFT JOIN<br>                         (<br>                         SELECT pe_desc,sum(score) n_issues<br>                         FROM facility<br>                        GROUP BY pe_desc<br>                          ) Y<br>               ON Y.n_issues = <a href=""http://qq.mini/"">qq.minI</a> <br> ) zz<br>LEFT JOIN <br>          (<br>           SELECT  facility_name,pe_desc <br>           FROM facility<br>         ) J<br> ON J.pe_desc = zz.pe_desc<br>WHERE facility_name LIKE &#39;%TEA%&#39; OR<br>              facility_name LIKE &#39;%CAFE%&#39; OR<br>              facility_name LIKE &#39;%JUICE%&#39;",2021-02-15T01:45:20Z
UgyiO-WnrrOZbb08Ewp4AaABAg.9Jk30vV70BM9JlZKOk4n8i,@stratascratch,UgyiO-WnrrOZbb08Ewp4AaABAg,2,vLjAG9eXkcU,0,1,2021-02-15T03:26:36Z,"@@PATRICKCHUAD Can you do something like this? I&#39;m not sure what your position was with the WITH but this works:<br><br>WITH counts AS<br>  (SELECT pe_description,<br>          COUNT(record_id) cnt<br>   FROM los_angeles_restaurant_health_inspections<br>   WHERE facility_name ILIKE &#39;%tea%&#39;<br>     OR facility_name ILIKE &#39;%cafe%&#39;<br>     OR facility_name ILIKE &#39;%juice%&#39;<br>   GROUP BY 1),<br>     ranks AS<br>  (SELECT pe_description,<br>          DENSE_RANK() OVER(<br>                            ORDER BY cnt DESC) rnk<br>   FROM counts)<br>SELECT facility_name<br>FROM los_angeles_restaurant_health_inspections<br>WHERE pe_description IN<br>    (SELECT pe_description<br>     FROM ranks<br>     WHERE rnk = 3)<br>  AND ((facility_name ILIKE &#39;%CAFE%&#39;<br>        OR facility_name ILIKE &#39;%TEA%&#39;<br>        OR facility_name ILIKE &#39;%JUICE%&#39;))",2021-02-15T03:26:36Z
UgyiO-WnrrOZbb08Ewp4AaABAg.9Jk30vV70BM9JleJYcrpDu,@PATRICKCHUAD,UgyiO-WnrrOZbb08Ewp4AaABAg,2,vLjAG9eXkcU,0,0,2021-02-15T04:18:55Z,@@stratascratch let me try this. thks,2021-02-15T04:18:55Z
UgyiO-WnrrOZbb08Ewp4AaABAg.9Jk30vV70BM9JmTKzlbgsw,@PATRICKCHUAD,UgyiO-WnrrOZbb08Ewp4AaABAg,2,vLjAG9eXkcU,0,0,2021-02-15T11:53:30Z,@@stratascratch i tried it works. Thanks.,2021-02-15T11:53:30Z
UgygQDJvsZexlsxEa494AaABAg,@mdabulkalamazad6775,,1,vLjAG9eXkcU,1,0,2021-01-26T21:49:56Z,"Excellent, Thanks for your dedication Nate",2021-01-26T21:49:56Z
UgygQDJvsZexlsxEa494AaABAg.9J-1gvBKIqG9J-6mKZ79Mw,@stratascratch,UgygQDJvsZexlsxEa494AaABAg,2,vLjAG9eXkcU,0,0,2021-01-26T22:34:22Z,Thanks for always watching!,2021-01-26T22:34:22Z
Ugz2abZL7vnEUXV3Ont4AaABAg,@hariguhan8399,,1,vLjAG9eXkcU,3,0,2021-01-26T04:28:20Z,"Hello Nate,<br>The shortened link opens your YT video, please can you update the URL.<br>(Neat content &amp; presentation!)<br>Thank You!",2021-01-26T04:28:20Z
Ugz2abZL7vnEUXV3Ont4AaABAg.9IyAUykh0ij9IyBRtNPBk-,@stratascratch,Ugz2abZL7vnEUXV3Ont4AaABAg,2,vLjAG9eXkcU,0,0,2021-01-26T04:36:39Z,"Hey Hari, sorry about that. Here you go! <a href=""https://platform.stratascratch.com/coding-question?id=9701&amp;python="">https://platform.stratascratch.com/coding-question?id=9701&amp;python=</a>",2021-01-26T04:36:39Z
Ugz2abZL7vnEUXV3Ont4AaABAg.9IyAUykh0ij9IyBypIGOi2,@hariguhan8399,Ugz2abZL7vnEUXV3Ont4AaABAg,2,vLjAG9eXkcU,0,0,2021-01-26T04:41:17Z,Thanks!<br>You&#39;ve earned a sub +1<br>üí™üèΩ,2021-01-26T04:41:17Z
Ugz2abZL7vnEUXV3Ont4AaABAg.9IyAUykh0ij9IyCh2fD_4c,@stratascratch,Ugz2abZL7vnEUXV3Ont4AaABAg,2,vLjAG9eXkcU,0,0,2021-01-26T04:47:35Z,@@hariguhan8399 I appreciate it man! Feel free to let me know if you have specific topics or anything else you&#39;re interested in learning.,2021-01-26T04:47:35Z
UgxBzWMLQVcFOtvMYoB4AaABAg,@classkori5507,,1,vLjAG9eXkcU,1,0,2021-01-25T23:16:23Z,Thanks sir,2021-01-25T23:16:23Z
UgxBzWMLQVcFOtvMYoB4AaABAg.9IxbnCJmEsV9Ixe6i5cj1Q,@stratascratch,UgxBzWMLQVcFOtvMYoB4AaABAg,2,vLjAG9eXkcU,0,0,2021-01-25T23:36:39Z,Thanks for watching!,2021-01-25T23:36:39Z
Ugw5LqmfcXpFBcpVU8p4AaABAg,@khushbumehta1350,,1,vLjAG9eXkcU,4,0,2021-01-24T03:34:36Z,"Hi, May I know the shortcut to indent multiple lines of code at once for organizing.",2021-01-24T03:35:01Z
Ugw5LqmfcXpFBcpVU8p4AaABAg.9IsvkeoI8Jw9IswhfUV1pl,@stratascratch,Ugw5LqmfcXpFBcpVU8p4AaABAg,2,vLjAG9eXkcU,0,1,2021-01-24T03:42:55Z,"Hi, thanks for watching. There&#39;s no shortcut unless you program the hotkey yourself. Otherwise, it&#39;s an issue between using tab or 4 spaces. I am a 4 space type of guy and mainly just will either hotkey it in my editor (I use sublime) or just type in the 4 spaces. Hope that helps.",2021-01-24T03:42:55Z
Ugw5LqmfcXpFBcpVU8p4AaABAg.9IsvkeoI8Jw9IuWSiBUpW4,@ChandraKanth7,Ugw5LqmfcXpFBcpVU8p4AaABAg,2,vLjAG9eXkcU,0,1,2021-01-24T18:23:18Z,I guess he‚Äôs asking about indenting multiple lines at the same time‚Ä¶ It will mostly be select the lines and use Ctrl+] or CMD+],2021-01-24T18:23:18Z
Ugw5LqmfcXpFBcpVU8p4AaABAg.9IsvkeoI8Jw9Ixf7Lr271m,@khushbumehta1350,Ugw5LqmfcXpFBcpVU8p4AaABAg,2,vLjAG9eXkcU,0,1,2021-01-25T23:45:29Z,"@@ChandraKanth7 yes, you are right. Thank you sharing!",2021-01-25T23:45:29Z
Ugw5LqmfcXpFBcpVU8p4AaABAg.9IsvkeoI8Jw9J_t-z79au-,@toekneema,Ugw5LqmfcXpFBcpVU8p4AaABAg,2,vLjAG9eXkcU,0,0,2021-02-10T14:36:26Z,Just highlight the block and hit tab,2021-02-10T14:36:26Z
UgxcLAy9Ju6wBMpIpJ14AaABAg,@anuragsingh4766,,1,vLjAG9eXkcU,1,1,2021-01-20T17:45:44Z,üëçüëåüëå,2021-01-20T17:45:44Z
UgxcLAy9Ju6wBMpIpJ14AaABAg.9Ik8z4Nsz4p9IkIj41nFkG,@stratascratch,UgxcLAy9Ju6wBMpIpJ14AaABAg,2,vLjAG9eXkcU,0,0,2021-01-20T19:10:55Z,Thanks for watching! Please let me know if there&#39;s any topics you&#39;d like for me to cover,2021-01-20T19:10:55Z
UgzSkDQq6Wn0vjtPddR4AaABAg,@xeskan,,1,vLjAG9eXkcU,1,1,2021-01-20T08:58:29Z,Solving this during the interview would take time. How quick is this question expected to be solved in a typical interview?,2021-01-20T08:58:29Z
UgzSkDQq6Wn0vjtPddR4AaABAg.9IjCdM8-hNs9Ik6a4X69Zj,@stratascratch,UgzSkDQq6Wn0vjtPddR4AaABAg,2,vLjAG9eXkcU,0,2,2021-01-20T17:24:50Z,"In my experience, this type of question would appear on one of the last rounds of the interview process where the questions are often much more complex and lengthy. I would expect this to take about 15-20 minutes on the whiteboard while also talking to the interviewer.",2021-01-20T17:24:50Z
UgzQVTZvmUy2C7Yb2_54AaABAg,@MirasAdilov,,1,j8kGqAAIhxA,1,0,2022-01-20T04:18:48Z,"thank you, Nate! Luv the t-shirt!",2022-01-20T04:18:48Z
UgzQVTZvmUy2C7Yb2_54AaABAg.9XPYmFpVvpL9XPZl7KtQlw,@stratascratch,UgzQVTZvmUy2C7Yb2_54AaABAg,2,j8kGqAAIhxA,0,1,2022-01-20T04:27:23Z,haha I&#39;m a big fan of Boiler Room!,2022-01-20T04:27:23Z
UgyghkogIceRvy4cM7t4AaABAg,@huseyindansman7415,,1,j8kGqAAIhxA,0,0,2021-11-02T17:57:32Z,"select top 1 with ties (ms sql server) ,I was trying to do it exactly the way I do in ms sql server then found out it doesnt work out in postgreSQL",2021-11-02T17:57:32Z
UgysMtkjxWzv7y8Pi2B4AaABAg,@Tridentor,,1,j8kGqAAIhxA,0,1,2021-09-03T16:37:13Z,Great stuff Nate. Pls note it looks like the site question has been modified and the site solution is now different from the video solution.,2021-09-03T16:37:13Z
UgxYn4sMP5y374bqV3l4AaABAg,@hta-bi249,,1,j8kGqAAIhxA,1,0,2021-08-03T00:00:14Z,"Hello, <br>Thank you for the great video.<br>I have a question!<br>Since we typed in the subquery limit 1, why the overall output still returning 2 rows?<br>cheerz!",2021-08-03T00:00:14Z
UgxYn4sMP5y374bqV3l4AaABAg.9QZM2m5yzBl9QZeQ97Ufbb,@stratascratch,UgxYn4sMP5y374bqV3l4AaABAg,2,j8kGqAAIhxA,0,1,2021-08-03T02:49:27Z,"The limit one was just to get the max total order cost. Once I got that, I used it to get all the customers that matched the max. And that&#39;s why you see 2 rows in the output. Hope that helps and thanks for watching!",2021-08-03T02:49:27Z
UgxTDxly9jPfmjj_UyB4AaABAg,@niveditakumari701,,1,j8kGqAAIhxA,0,0,2021-08-02T03:35:38Z,"Hey, we can also do this using DENSE_RANK() ?",2021-08-02T03:35:38Z
Ugwx98kusKJPb8K9nod4AaABAg,@abhisheksinghchouhan8506,,1,j8kGqAAIhxA,1,0,2021-05-24T12:37:57Z,Your explanation very and easy to understand..but your site name is very difficult to remember,2021-05-24T12:37:57Z
Ugwx98kusKJPb8K9nod4AaABAg.9NitKK12bo39NjrDR38YFI,@stratascratch,Ugwx98kusKJPb8K9nod4AaABAg,2,j8kGqAAIhxA,0,0,2021-05-24T21:38:46Z,"haha I know right? If I had to do it all over again, I would have picked an easier name =)",2021-05-24T21:38:46Z
UgxzXSnrjda8AFuAQgB4AaABAg,@dominic2446,,1,j8kGqAAIhxA,1,2,2021-02-02T23:34:08Z,"<a href=""https://www.youtube.com/watch?v=j8kGqAAIhxA&amp;t=7m14s"">7:14</a> people make this mistake because of the phrasing of the question. &quot;customer&quot; should be changed to &quot;customer(s)&quot; and &quot;first name&quot; should be changed to &quot;first name(s)&quot;. this way, the question suggests the possibility that the solution can contain more than one person. <br><a href=""https://www.youtube.com/watch?v=j8kGqAAIhxA&amp;t=8m09s"">8:09</a> what is &quot;data at scale issues&quot;?",2021-02-02T23:41:20Z
UgxzXSnrjda8AFuAQgB4AaABAg.9JHFBGiOe9q9JHHCdef89S,@stratascratch,UgxzXSnrjda8AFuAQgB4AaABAg,2,j8kGqAAIhxA,0,0,2021-02-02T23:51:48Z,"I agree with you. It&#39;s important to talk to the interviewer to identify potential issues like this. You won&#39;t always get a perfectly phrased question in real life, especially on the job, so the purpose of this question is really to think of all possible scenarios and build a solution that solves for it. Doing it without someone prompting you to look for these scenarios is even better.<br><br>Data at scale issues are data issues that pop-up only when you&#39;re dealing with a massive amount of data (ie, the amount of data that Facebook or Google might encounter). Because of this you might have to handle the data differently.",2021-02-02T23:51:48Z
UgwKlNpnMYnpYXvrhHx4AaABAg,@mdabulkalamazad6775,,1,j8kGqAAIhxA,1,0,2021-01-09T17:24:09Z,"great video as always, Thanks",2021-01-09T17:24:09Z
UgwKlNpnMYnpYXvrhHx4AaABAg.9IImlOrXBnH9IJOCX-8qeg,@stratascratch,UgwKlNpnMYnpYXvrhHx4AaABAg,2,j8kGqAAIhxA,0,0,2021-01-09T23:00:02Z,Thanks for watching as always,2021-01-09T23:00:02Z
UgwWoFYMxtz9CygcxtF4AaABAg,@classkori5507,,1,j8kGqAAIhxA,1,0,2021-01-07T22:02:42Z,Excellent,2021-01-07T22:02:42Z
UgwWoFYMxtz9CygcxtF4AaABAg.9IE82omHURu9IE_PLA2pbV,@stratascratch,UgwWoFYMxtz9CygcxtF4AaABAg,2,j8kGqAAIhxA,0,0,2021-01-08T02:10:27Z,Thanks for watching. Let me know if you have any feedback or ideas for videos!,2021-01-08T02:10:27Z
Ugz-_HmIeDOGgyAuStp4AaABAg,@rakibraihanrimon8784,,1,j8kGqAAIhxA,1,0,2021-01-07T16:55:10Z,Thank You,2021-01-07T16:55:10Z
Ugz-_HmIeDOGgyAuStp4AaABAg.9ID_rNDU6IG9IDdbICIqyk,@stratascratch,Ugz-_HmIeDOGgyAuStp4AaABAg,2,j8kGqAAIhxA,0,0,2021-01-07T17:27:55Z,Thanks for watching!,2021-01-07T17:27:55Z
Ugy450aC8wNYKkwjn3R4AaABAg,@vikrant.9039,,1,j8kGqAAIhxA,1,0,2021-01-07T14:33:32Z,Your github is not updated since past one year Nate?,2021-01-07T14:33:32Z
Ugy450aC8wNYKkwjn3R4AaABAg.9IDKe2H1Yw09IDRyMIqIuF,@stratascratch,Ugy450aC8wNYKkwjn3R4AaABAg,2,j8kGqAAIhxA,0,0,2021-01-07T15:37:29Z,My public one is not other than the one that I linked to in the description. I have one at work and private ones for StrataScratch. But I&#39;ll be posting more projects throughout the year and putting them on youtube.,2021-01-07T15:37:29Z
UgxGSK2KokaaCtUAVJN4AaABAg,@IanWitherow,,1,j8kGqAAIhxA,2,1,2021-01-07T01:20:15Z,"Trying out the StrataScratch question - I&#39;m returning two records, Mia and Farida, both with 400 for the total and their respective order dates, but it&#39;s not accepting it. The Expected Output button is showing 430 for the order totals, but I&#39;m not seeing that order total in the data. Am I missing something?<br>Edit: Ah, multiple orders on the same date, got it. Was thrown off since you had 400 in your video there.",2021-01-07T01:26:16Z
UgxGSK2KokaaCtUAVJN4AaABAg.9IBurlrj_yO9IBy1TLtSF-,@stratascratch,UgxGSK2KokaaCtUAVJN4AaABAg,2,j8kGqAAIhxA,0,0,2021-01-07T01:47:55Z,"Hi, yes, there&#39;s actually a change in the solution because I realized that the 400 corresponded to 400 for 1 item that was purchased on a specific day, not all purchases on a specific day. When you count all purchases made by 1 customer on a specific day, the max is 430. So the code below will get you there.<br><br>The concept is the same though. In the video, I really wanted to highlight the fact that LIMIT 1s are common mistakes people make. To properly find the customers with the highest number, you have to consider that many people meet that max, and write your solution accordingly.<br><br>Thanks for trying the code out on the platform. Sorry for the confusion. <br><br>SELECT first_name,<br>       order_date,<br>       sum(total_order_cost)<br>FROM<br>  (SELECT first_name,<br>          order_cost * order_quantity AS total_order_cost,<br>          order_date<br>   FROM orders o<br>   LEFT JOIN customers c ON o.cust_id = <a href=""http://c.id/"">c.id</a><br>   WHERE order_date BETWEEN &#39;2019-02-1&#39; AND &#39;2019-05-1&#39; ) a<br>GROUP BY first_name,<br>         order_date<br>HAVING sum(total_order_cost) =<br>  (SELECT max(total_order_cost)<br>   FROM<br>     (SELECT sum(order_cost * order_quantity) AS total_order_cost<br>      FROM orders<br>      WHERE order_date BETWEEN &#39;2019-02-1&#39; AND &#39;2019-05-1&#39;<br>      GROUP BY cust_id,<br>               order_date) b )",2021-01-07T01:47:55Z
UgxGSK2KokaaCtUAVJN4AaABAg.9IBurlrj_yO9JHEuevcZm0,@dominic2446,UgxGSK2KokaaCtUAVJN4AaABAg,2,j8kGqAAIhxA,0,0,2021-02-02T23:31:44Z,"@@stratascratch the question is vague, because it could be interpreted as the total cost for the week or the month too.",2021-02-02T23:31:44Z
UgzOgbU9FYlr8WMkAix4AaABAg,@culpgrant21,,1,j8kGqAAIhxA,2,5,2021-01-06T14:32:09Z,What do you think about using the RANK function to handle the case of ties?,2021-01-06T14:32:09Z
UgzOgbU9FYlr8WMkAix4AaABAg.9IAkh-MEY3l9IBUTc2nHgh,@tripy75,UgzOgbU9FYlr8WMkAix4AaABAg,2,j8kGqAAIhxA,0,2,2021-01-06T21:20:53Z,Or even simpler: a max() on the computed total without sorting and limit.<br>This could reduce the I/O a lot depending the table size.,2021-01-06T21:20:53Z
UgzOgbU9FYlr8WMkAix4AaABAg.9IAkh-MEY3l9IBhSzetmgF,@stratascratch,UgzOgbU9FYlr8WMkAix4AaABAg,2,j8kGqAAIhxA,0,3,2021-01-06T23:23:07Z,max() as mentioned would be great to optimize the code. And then match the max value with others in the table.,2021-01-06T23:23:07Z
UgwPCMHSn1VrJ5CsAXV4AaABAg,@anuragsingh4766,,1,j8kGqAAIhxA,1,2,2021-01-06T08:14:48Z,Great video.,2021-01-06T08:14:48Z
UgwPCMHSn1VrJ5CsAXV4AaABAg.9IA4WBnwxLy9IBhI8l9Xsi,@stratascratch,UgwPCMHSn1VrJ5CsAXV4AaABAg,2,j8kGqAAIhxA,0,0,2021-01-06T23:21:39Z,Thanks for watching! Let me know if you have any feedback or topic suggestions.,2021-01-06T23:21:39Z
UgzXtaqCHBAgMfAnMix4AaABAg,@MyRevoltec,,1,j8kGqAAIhxA,1,0,2021-01-06T08:12:44Z,Awesome video as always!,2021-01-06T08:12:44Z
UgzXtaqCHBAgMfAnMix4AaABAg.9IA4H48iR9f9IBhIVjzB_n,@stratascratch,UgzXtaqCHBAgMfAnMix4AaABAg,2,j8kGqAAIhxA,0,0,2021-01-06T23:21:42Z,Thanks for watching! Let me know if you have any feedback or topic suggestions.,2021-01-06T23:21:42Z
Ugzu0JC9BXlOKOxUd_54AaABAg,@sunny2253,,1,W_IERUwElkg,0,0,2022-11-14T04:36:06Z,I gave a sql interview recently and 3 out of 4 questions were exactly based on this logic,2022-11-14T04:36:06Z
UgxSQWpS6cSbDqsYRkt4AaABAg,@NikhilKumar-ot3vi,,1,W_IERUwElkg,0,0,2022-08-05T13:50:10Z,Sir please mention you are doing this in postgres SQL. there&#39;s nothing like ilike in mysql.,2022-08-05T17:31:44Z
UgylAL8y_snrVypf3QJ4AaABAg,@pakfrd7887,,1,W_IERUwElkg,0,0,2022-05-03T03:07:27Z,Anyway I can pay for an hour over teams to check my answers?,2022-05-03T03:07:27Z
UgxlVaBhdedHTv03NcB4AaABAg,@3danim8r1,,1,W_IERUwElkg,0,0,2022-03-02T01:47:58Z,Hey man Thanks for sharing.. Just a question i have not found second and third question on your website could you please suggest how i can find these questions,2022-03-02T01:47:58Z
UgxOO8vV4jbOdYaQQjt4AaABAg,@adelabdallah3833,,1,W_IERUwElkg,0,0,2022-02-19T17:17:32Z,Can you please show examples of counting retained users by date difference between first_transaction_date and transaction date but in a dynamic way.,2022-02-19T17:17:32Z
UgwSqcilFypChFmgKpx4AaABAg,@observer698,,1,W_IERUwElkg,0,0,2022-01-19T22:09:25Z,"So especially the 3rd one, what if we use Where to select the special cases???",2022-01-19T22:09:25Z
UgwKEUAuJxSP79QYalp4AaABAg,@mosherchtman,,1,W_IERUwElkg,0,8,2022-01-11T00:07:07Z,"in my opinion, the most confusing part of the question is understanding the need for Host_id, a bit tricky.",2022-01-11T00:07:07Z
UgzLbz86YyU-qapLgkF4AaABAg,@pratiksamanta2478,,1,W_IERUwElkg,0,0,2022-01-05T08:40:01Z,https://www.youtube.com/watch?v=xB1iXa7pRic&amp;t=1s,2023-10-31T14:21:02Z
Ugxs9iNyihgfqncgaNt4AaABAg,@gauravdutta5486,,1,W_IERUwElkg,0,7,2021-12-30T05:58:52Z,These practical scenarios of case are really helpful n useful even for our own IT implementations.,2021-12-30T05:58:52Z
Ugz9DB5Jo6Yu-J8rfpJ4AaABAg,@vargaslu85,,1,W_IERUwElkg,0,0,2021-12-24T15:03:49Z,"What are the requirements to get in as a DS, questions are seem too simple.",2021-12-24T15:03:49Z
UgwRiEsyTGHSZbhLwEp4AaABAg,@zetsui0411,,1,W_IERUwElkg,1,0,2021-12-03T16:13:53Z,count vs sum with case when?,2021-12-03T16:13:53Z
UgwRiEsyTGHSZbhLwEp4AaABAg.9VVESlnzK-P9VVQs4xLPLv,@stratascratch,UgwRiEsyTGHSZbhLwEp4AaABAg,2,W_IERUwElkg,0,0,2021-12-03T18:02:20Z,"<a href=""https://stackoverflow.com/questions/46894050/difference-between-count-and-sum-within-an-aggregate-case-statement"">https://stackoverflow.com/questions/46894050/difference-between-count-and-sum-within-an-aggregate-case-statement</a>",2021-12-03T18:02:20Z
UgzyPtKvKeZ1r_iebfx4AaABAg,@miteshbagwe9765,,1,W_IERUwElkg,1,0,2021-12-01T14:22:11Z,Excellent explanation. Thanks for posting.,2021-12-01T14:22:23Z
UgzyPtKvKeZ1r_iebfx4AaABAg.9VPt4pqUAOQ9VPwAf-CNPB,@stratascratch,UgzyPtKvKeZ1r_iebfx4AaABAg,2,W_IERUwElkg,0,0,2021-12-01T14:49:11Z,Thanks for watching! Appreciate it,2021-12-01T14:49:11Z
Ugylx8K4YKRsJSAEebx4AaABAg,@elfridhasman4181,,1,W_IERUwElkg,0,0,2021-11-29T14:07:40Z,"Thank you very much for sharing sir, you help me a lot, God Bless You and Family :)",2021-11-29T14:07:40Z
Ugy6UxngFU50K16JKDh4AaABAg,@gauthamambethkar4483,,1,W_IERUwElkg,2,3,2021-11-21T17:01:45Z,"Hello Nate, thanks for this awesome video. I have one doubt. In 1st question, inside the cte why do we use group by 1,2,3 because inside the cte we don&#39;t use any aggregate function. But if I remove that the average column value doesn&#39;t come right in the final output. I&#39;m missing something here. Could you please help?",2021-11-21T17:01:45Z
Ugy6UxngFU50K16JKDh4AaABAg.9V0QOrVyGHn9V0SXkcOyyi,@stratascratch,Ugy6UxngFU50K16JKDh4AaABAg,2,W_IERUwElkg,0,0,2021-11-21T17:20:27Z,"What I&#39;m trying to do with the group by 1,2,3 is dedup any information since I want everything to be unique. Probably not required on this dataset though but give it a try on the platform",2021-11-21T17:20:27Z
Ugy6UxngFU50K16JKDh4AaABAg.9V0QOrVyGHn9V0TaOyRnX6,@gauthamambethkar4483,Ugy6UxngFU50K16JKDh4AaABAg,2,W_IERUwElkg,0,1,2021-11-21T17:29:41Z,@@stratascratch but without that group by the answer doesn&#39;t come right. It is needed. But not sure why,2021-11-21T17:29:41Z
UgxWH3Qj_lCFOVVNK6x4AaABAg,@atifkhan9560,,1,W_IERUwElkg,1,0,2021-10-20T11:30:58Z,"why we cannot use this query <br><br> select <br>case <br>    when number_of_reviews&lt;1 then &#39;New&#39;<br>    when number_of_reviews between 1 and 5 then &#39;Rising&#39;<br>    when number_of_reviews between 6 and 15 then &#39;Trending Up&#39;<br>    when number_of_reviews between 16 and 40 then &#39;Popular&#39;<br>    when number_of_reviews&gt;40 then &#39;Hot&#39;<br>    end as a,<br>    min(price),avg(price),max(price)<br>    from airbnb_host_searches<br>    group by a<br>    order by a",2021-10-20T11:30:58Z
UgxWH3Qj_lCFOVVNK6x4AaABAg.9TiR635T5oJ9Tj3PjUVk5V,@stratascratch,UgxWH3Qj_lCFOVVNK6x4AaABAg,2,W_IERUwElkg,0,0,2021-10-20T17:23:10Z,Try it in the platform! I don&#39;t see anything wrong with it but I haven&#39;t executed the query to see what the differences are between mine and yours.,2021-10-20T17:23:10Z
Ugz2crdQDNTo8ccxANZ4AaABAg,@kathyle6275,,1,W_IERUwElkg,1,0,2021-10-04T19:38:54Z,"this video is super helpful and awesome, thank you a bunch :D",2021-10-04T19:38:54Z
Ugz2crdQDNTo8ccxANZ4AaABAg.9T56Dn8Qs1x9T58NM-4gK1,@stratascratch,Ugz2crdQDNTo8ccxANZ4AaABAg,2,W_IERUwElkg,0,1,2021-10-04T19:57:40Z,Glad you found it educational and helpful!,2021-10-04T19:57:40Z
Ugy28dzLr_E2mxR7FEl4AaABAg,@fitriwidyananda9598,,1,W_IERUwElkg,2,0,2021-10-04T03:54:23Z,"Hello!<br>I was trying to solve the second problem using the query below but why the result is different from yours. Can you tell me, please? thank you.<br><br>WITH new_view AS(<br>    SELECT CONCAT(price, room_type, host_since, zipcode, number_of_reviews) AS host_id,<br>            number_of_reviews,<br>            price,<br>            CASE<br>                WHEN number_of_reviews = 0 THEN &#39;New&#39;<br>                WHEN number_of_reviews BETWEEN 1 AND 5 THEN &#39;Rising&#39;<br>                WHEN number_of_reviews BETWEEN 6 AND 15 THEN &#39;Trending Up&#39;<br>                WHEN number_of_reviews BETWEEN 16 AND 40 THEN &#39;Popular&#39;<br>                WHEN number_of_reviews &gt; 40 THEN &#39;Hot&#39;<br>            END AS host_popularity<br>    FROM airbnb_host_searches<br>    GROUP BY 1,2,3)<br>SELECT host_popularity AS host_pop_rating,<br>        MIN(price) AS min_price,<br>        MAX(price) AS max_price,<br>        AVG(price) AS avg_price<br>FROM new_view<br>GROUP BY host_pop_rating;",2021-10-04T03:54:23Z
Ugy28dzLr_E2mxR7FEl4AaABAg.9T3Q8-DFwlw9T4vd9YedE7,@stratascratch,Ugy28dzLr_E2mxR7FEl4AaABAg,2,W_IERUwElkg,0,0,2021-10-04T17:57:38Z,"Hi, is it possible for you to ask this question on the platform&#39;s discussion forum? Someone on my team will answer your question within a few days!",2021-10-04T17:57:38Z
Ugy28dzLr_E2mxR7FEl4AaABAg.9T3Q8-DFwlw9XOuL67eh_p,@observer698,Ugy28dzLr_E2mxR7FEl4AaABAg,2,W_IERUwElkg,0,0,2022-01-19T22:16:41Z,"Your host_id creation has one more value, maybe that makes the difference.",2022-01-19T22:16:41Z
Ugyy2DG1GAiO1a2Le8B4AaABAg,@henryyin2731,,1,W_IERUwElkg,1,1,2021-09-29T06:09:39Z,"I am not sure if you can still see this comment after almost a year but I am wondering for the first Airbnb question, why do you have to specify &quot;Group by 1, 2, 3&quot;. I didn&#39;t specify and my answer is so close to the right answer, and could you explain what does 1, 2, 3 mean in this situation?",2021-09-29T06:09:39Z
Ugyy2DG1GAiO1a2Le8B4AaABAg.9SrmdK8CEDs9StZkjf2cgd,@stratascratch,Ugyy2DG1GAiO1a2Le8B4AaABAg,2,W_IERUwElkg,0,0,2021-09-29T22:46:48Z,"Group by 1,2,3 will group the output by the first 3 columns - host id, num of reviews, and price. This helps to de-duplicate any rows. You kind of want to dedup because there shouldn&#39;t be any rows where there are duplicates of host_id, number of reviews, and price. These 3 combinations of columns are unique. Hope this clears it!",2021-09-29T22:46:48Z
Ugxd_kKYKZHpfISSHlx4AaABAg,@pendyalarajeshwari499,,1,W_IERUwElkg,0,0,2021-09-24T18:58:04Z,"Hi, could u help me in this query if u are having two tables flights consists of following columns I&#39;d,company name , departure name and jet _id and jet consists of I&#39;d , capacity here jet_id is foreign key reference id in jet table   I want sort the data with multiple condition like if total number of rows  are equal by each company name  then sort by sum of capacity column in jet  and if count two rows are not equal sort by count of each company  and then depature name, and still equal count  sort by departure time",2021-09-24T18:58:04Z
UgxgHE-GwfFZy7Tyygt4AaABAg,@sonyrao2615,,1,W_IERUwElkg,1,2,2021-09-19T03:11:41Z,"this content is awesome! thank you! I had a question, in the business inspection scores problem, why did you need to write &quot;distinct inspection_type&quot; as a part of the sub query. I don&#39;t understand why/how you knew we need to use distinct to solve this.",2021-09-19T03:11:41Z
UgxgHE-GwfFZy7Tyygt4AaABAg.9SSiK97TYrm9SU3pVKluD5,@stratascratch,UgxgHE-GwfFZy7Tyygt4AaABAg,2,W_IERUwElkg,0,0,2021-09-19T15:47:35Z,mainly because I didn&#39;t want duplicates in my output so I used distinct in the subquery. We probably could have used distinct at the very end but I just chose to do it in the subquery. A lot of this is about playing with the data but if you&#39;re on an interview you can ask the interviewer about whether or not you care about uniqueness or duplicates in the final output.,2021-09-19T15:47:35Z
Ugyc6dcq2vFufaHBCXd4AaABAg,@meghasyam427,,1,W_IERUwElkg,1,0,2021-08-08T11:51:52Z,"Ilike is cool one. Learnt a new function today. <br>Btw nate, what rdbms is it exactly...like hive or mysql or something else",2021-08-08T11:51:52Z
Ugyc6dcq2vFufaHBCXd4AaABAg.9QmVTKPDq8y9Qmr6qLCzP3,@stratascratch,Ugyc6dcq2vFufaHBCXd4AaABAg,2,W_IERUwElkg,0,1,2021-08-08T15:09:47Z,This is postgres. Thanks for watching,2021-08-08T15:09:47Z
UgyNheNaVSwnA5QdeHx4AaABAg,@niveditakumari701,,1,W_IERUwElkg,0,0,2021-07-31T14:57:46Z,Really Informative video,2021-07-31T14:57:46Z
UgxhtOGCzANSta5I7tl4AaABAg,@zulumane,,1,W_IERUwElkg,3,2,2021-07-19T14:00:19Z,I LOVE YOUR CONTENT! Could you maybe do a series on the most common SQL scenarios that a DS would face in a work environment?,2021-07-19T14:00:19Z
UgxhtOGCzANSta5I7tl4AaABAg.9PzEGXfbhgU9PzRuik6hg1,@stratascratch,UgxhtOGCzANSta5I7tl4AaABAg,2,W_IERUwElkg,0,0,2021-07-19T15:59:32Z,"Thanks! Many of my interview questions series can be adapted for the work environment. In the newer videos I always try to explain how you&#39;d solve the problem on an interview and how to do so on the job so I try to straddle both realms. But I see your point -- If I find a good set of questions that are perfect for the work environment, I&#39;ll definitely cover it in a video",2021-07-19T15:59:32Z
UgxhtOGCzANSta5I7tl4AaABAg.9PzEGXfbhgU9Q141Rb2uL2,@zulumane,UgxhtOGCzANSta5I7tl4AaABAg,2,W_IERUwElkg,0,0,2021-07-20T16:28:35Z,"@@stratascratch Awesome thanks! Another suggestion might be going over some of the SQL theory, specifically comparing computing power used for certain queries or certain parts of those queries. I know you mention it in some of your videos but that could still be helpful! Thanks again for all the great resources that you put on YouTube, I&#39;ll be using Strata Scratch very soon!",2021-07-20T16:28:35Z
UgxhtOGCzANSta5I7tl4AaABAg.9PzEGXfbhgU9Q1b1fecZQf,@stratascratch,UgxhtOGCzANSta5I7tl4AaABAg,2,W_IERUwElkg,0,0,2021-07-20T21:25:43Z,"@@zulumane Yea that&#39;s a great suggestion. For some of the videos, I can dive deeper into optimizations and performance.",2021-07-20T21:25:43Z
UgxvvsLCiirl1RDgBRR4AaABAg,@mimibhattacharjee9668,,1,W_IERUwElkg,1,0,2021-07-18T23:13:19Z,Do you have videos on data model questions from facebook?,2021-07-18T23:13:19Z
UgxvvsLCiirl1RDgBRR4AaABAg.9PxdkxjDFKq9Py2PS6UteV,@stratascratch,UgxvvsLCiirl1RDgBRR4AaABAg,2,W_IERUwElkg,0,0,2021-07-19T02:57:26Z,Unfortunately not really. I don&#39;t cover any data modeling questions yet...,2021-07-19T02:57:26Z
UgxFxU_10dPZZvV7uBR4AaABAg,@manugupta3062,,1,W_IERUwElkg,1,0,2021-07-13T21:05:04Z,Which coding platform it is??,2021-07-13T21:05:04Z
UgxFxU_10dPZZvV7uBR4AaABAg.9PkY6M47PN89PkZB_yYiFa,@stratascratch,UgxFxU_10dPZZvV7uBR4AaABAg,2,W_IERUwElkg,0,0,2021-07-13T21:14:31Z,"This is off of <a href=""https://platform.stratascratch.com/"">https://platform.stratascratch.com</a> which uses a postgres db engine and python3",2021-07-13T21:14:31Z
UgyZK4ACZuC26stcKRF4AaABAg,@miraskhabibulla1345,,1,W_IERUwElkg,0,0,2021-07-13T13:11:34Z,you are awesome! keep it up!,2021-07-13T13:11:34Z
Ugy70cAxEZknl0a2YgR4AaABAg,@damian2955,,1,W_IERUwElkg,0,0,2021-07-04T15:14:27Z,"nice one, thanks",2021-07-04T15:14:27Z
UgyNgAQBaG_PUEpUyo14AaABAg,@ahassignment3961,,1,W_IERUwElkg,2,1,2021-06-26T09:55:44Z,"I wonder why I get a different answer for the average price in the first question? <br><br>This is my SQL <br><br>select A.host_pop_rating, min(A.price) as min_price, max(A.price) as max_price, avg(A.price) as avg_price <br>from <br>(select *,<br>case when number_of_reviews = 0 then &#39;New&#39;<br>when number_of_reviews &lt;= 5 then &#39;Rising&#39;<br>when number_of_reviews &lt;= 15 then &#39;Trending Up&#39;<br>when number_of_reviews &lt;= 40 then &#39;Popular&#39;<br>when number_of_reviews &gt; 40 then &#39;Hot&#39;<br>end as host_pop_rating<br>from airbnb_host_searches) A<br>Group by A.host_pop_rating ;",2021-06-26T09:55:44Z
UgyNgAQBaG_PUEpUyo14AaABAg.9P2_-4aBxV59P6H_ywX5TL,@stratascratch,UgyNgAQBaG_PUEpUyo14AaABAg,2,W_IERUwElkg,0,0,2021-06-27T20:31:51Z,Can you post this solution onto the user discussion board on this question? Someone can help you within 1-2 days.,2021-06-27T20:31:51Z
UgyNgAQBaG_PUEpUyo14AaABAg.9P2_-4aBxV59P6HfETbq5e,@stratascratch,UgyNgAQBaG_PUEpUyo14AaABAg,2,W_IERUwElkg,0,0,2021-06-27T20:32:34Z,"Here&#39;s the link to the question (<a href=""https://platform.stratascratch.com/coding-question?id=9632&amp;python=)"">https://platform.stratascratch.com/coding-question?id=9632&amp;python=)</a>",2021-06-27T20:32:34Z
UgxzCtiJ_KbJFZmnwUN4AaABAg,@priyankalad7789,,1,W_IERUwElkg,1,0,2021-06-07T21:19:38Z,"count(case when position &lt;=3 and has_clicked= &#39;yes&#39; then b.searchid else null end ) from table left join table on a.result_id = b.result_id. Here as we are using aggregate function, why not group by is used ?",2021-06-07T21:19:38Z
UgxzCtiJ_KbJFZmnwUN4AaABAg.9OIs9Tg0eW69OJ5oBCY_sx,@stratascratch,UgxzCtiJ_KbJFZmnwUN4AaABAg,2,W_IERUwElkg,0,0,2021-06-07T23:27:39Z,Because there&#39;s no 2nd column being returned. It&#39;s just 1 column which is that aggregate. So what would you be grouping by? Let me know if that makes sense.,2021-06-07T23:27:39Z
UgymlJ_0daKHgv2eIC14AaABAg,@modhua4497,,1,W_IERUwElkg,4,0,2021-06-02T10:52:06Z,What is the link for this question? Thanks,2021-06-02T10:52:06Z
UgymlJ_0daKHgv2eIC14AaABAg.9O4sMzMyu0p9O5e0r51Zdu,@stratascratch,UgymlJ_0daKHgv2eIC14AaABAg,2,W_IERUwElkg,0,0,2021-06-02T18:05:59Z,"It&#39;s in the description. But here it is copy and pasted: <br>- CASE statement for numerical values: <a href=""http://bit.ly/2Wmnvad"">http://bit.ly/2Wmnvad</a><br>- CASE statement for text: <a href=""http://bit.ly/3p0eboP"">http://bit.ly/3p0eboP</a><br>- The most popular CASE statement on SQL interviews: <a href=""http://bit.ly/2ITCi9f"">http://bit.ly/2ITCi9f</a>",2021-06-02T18:05:59Z
UgymlJ_0daKHgv2eIC14AaABAg.9O4sMzMyu0p9O5gyfAcfPG,@modhua4497,UgymlJ_0daKHgv2eIC14AaABAg,2,W_IERUwElkg,0,0,2021-06-02T18:31:46Z,@@stratascratchthanks Nate. Is there ‚Äúadditive Case statement‚Äù in sql? If yes can you show an example? Thanks a lot! You have been very helpful to me in learning sql. <br>Also can you show an example of ‚Äúcross apply‚Äù vs ‚Äúinner join‚Äù?,2021-06-02T18:31:46Z
UgymlJ_0daKHgv2eIC14AaABAg.9O4sMzMyu0p9O69XgForw5,@stratascratch,UgymlJ_0daKHgv2eIC14AaABAg,2,W_IERUwElkg,0,0,2021-06-02T22:50:06Z,"@@modhua4497 What do you mean by additive case statement? I haven&#39;t used cross apply in SQL but here are some resources I found <br><br>1. <a href=""https://stackoverflow.com/questions/1139160/when-should-i-use-cross-apply-over-inner-join"">https://stackoverflow.com/questions/1139160/when-should-i-use-cross-apply-over-inner-join</a><br>2. <a href=""https://explainextended.com/2009/07/16/inner-join-vs-cross-apply/"">https://explainextended.com/2009/07/16/inner-join-vs-cross-apply/</a><br><br>Seems like you&#39;d use cross apply where there&#39;s no simple join query. I haven&#39;t had to use cross apply in my work but maybe it&#39;s because I use python for anything I can&#39;t solve with my knowledge in SQL. Hope that helps!",2021-06-02T22:50:06Z
UgymlJ_0daKHgv2eIC14AaABAg.9O4sMzMyu0p9O6Fn2L8if6,@modhua4497,UgymlJ_0daKHgv2eIC14AaABAg,2,W_IERUwElkg,0,0,2021-06-02T23:44:45Z,Thanks Nate. Have a good day.,2021-06-02T23:44:45Z
Ugxe-IjYcodT7ADr_mt4AaABAg,@iitian2012,,1,W_IERUwElkg,3,0,2021-05-09T14:46:58Z,"Your sql videos are really very helpful....It would be great if you could prepare a video on real time sql query containing multiple sub queries (basically multiple from, may be 3-4 inner queries ) covering around 100 plus line of codes......Bsically need to understand how someone approach. to code such a large and complex query and where do they start querying since that involve multiple set of inner queries....",2021-05-09T14:46:58Z
Ugxe-IjYcodT7ADr_mt4AaABAg.9N7VA8ynIf89N7v6ZwRUB7,@stratascratch,Ugxe-IjYcodT7ADr_mt4AaABAg,2,W_IERUwElkg,0,0,2021-05-09T18:42:24Z,"oh man...yea I see where you&#39;re going with this. Basically, I probably would have 3-4 inner queries and CTEs but I&#39;d probably also have a lot of tables and temp tables to help me manage the data as well. Let me think about what I can do for 100+ lines. It&#39;s not something too easy to come up with but I&#39;ll see what I can do.",2021-05-09T18:42:24Z
Ugxe-IjYcodT7ADr_mt4AaABAg.9N7VA8ynIf89N8lVt8pihf,@iitian2012,Ugxe-IjYcodT7ADr_mt4AaABAg,2,W_IERUwElkg,0,0,2021-05-10T02:37:43Z,@@stratascratch thanx a lot...we are really learning a lot üòä,2021-05-10T02:37:43Z
Ugxe-IjYcodT7ADr_mt4AaABAg.9N7VA8ynIf89N8qMP_sSye,@stratascratch,Ugxe-IjYcodT7ADr_mt4AaABAg,2,W_IERUwElkg,0,2,2021-05-10T03:20:07Z,@@iitian2012 more videos to come!,2021-05-10T03:20:07Z
Ugz5NL5p4DYg1yotvsx4AaABAg,@huanchenli4137,,1,W_IERUwElkg,2,3,2021-04-28T07:06:24Z,"Dont understand why here needs a group by 1,2,3 on in inner code",2021-04-28T07:06:24Z
Ugz5NL5p4DYg1yotvsx4AaABAg.9MfLiEnr6Bq9MgKMbWPXu7,@stratascratch,Ugz5NL5p4DYg1yotvsx4AaABAg,2,W_IERUwElkg,0,1,2021-04-28T16:13:49Z,I think it&#39;s because the case statement (column 4) was considered an aggregate so I required to group the columns in order for the code to run successfully.,2021-04-28T16:13:49Z
Ugz5NL5p4DYg1yotvsx4AaABAg.9MfLiEnr6Bq9SWGkd8_i7U,@nadavnavon2140,Ugz5NL5p4DYg1yotvsx4AaABAg,2,W_IERUwElkg,0,0,2021-09-20T12:19:00Z,"It could be that instead of the GROUP BY, just use a DISTINCT initially at the select statement, its worth checking it out..<br>Seems like the group by acts for getting unique combinations of the first 3 columns. <br>Personally I didin&#39;t know that you can set a group by when there is no aggregate. nor that a case statement is considered as an aggregate",2021-09-20T12:19:00Z
UgwPnmuOcKjZO6hw1WJ4AaABAg,@rahaapooya,,1,W_IERUwElkg,0,0,2021-04-26T17:51:35Z,"Very helpful, thanks a lot!",2021-04-26T17:51:35Z
UgxjNfZupTdVOI-_cst4AaABAg,@priyankadhanawade8631,,1,W_IERUwElkg,1,0,2021-04-16T17:31:21Z,please  create some content on TIMESTAMP  .thank you!,2021-04-16T17:31:21Z
UgxjNfZupTdVOI-_cst4AaABAg.9MCZh1Lm0Dl9MCuY1syivA,@stratascratch,UgxjNfZupTdVOI-_cst4AaABAg,2,W_IERUwElkg,0,0,2021-04-16T20:42:14Z,Will try to find some examples! Thanks for the suggestion.,2021-04-16T20:42:14Z
Ugw3lthWGzIvQsseCA94AaABAg,@songsong2334,,1,W_IERUwElkg,1,0,2021-03-30T05:00:00Z,"Thanks for the good content! Thumbs up! One question from some of ur videos, how did u come up the metrics design by looking at the SQL data?",2021-03-30T05:00:00Z
Ugw3lthWGzIvQsseCA94AaABAg.9LVSC5kh8Ci9LWeyJAGwJg,@stratascratch,Ugw3lthWGzIvQsseCA94AaABAg,2,W_IERUwElkg,0,0,2021-03-30T16:19:33Z,"It&#39;s usually through looking at the table schema first. I can usually figure out how to design the metrics. I mainly only look at the schema because on interviews, you don&#39;t usually have access to the data so you have to make all your assumptions based on the column names. The platform is nice in that you can look at the data so I then will explore the dataset to make sure my assumptions are correct.",2021-03-30T16:19:33Z
Ugzlh8pez7pb1g7QsZV4AaABAg,@abhistdixit3614,,1,W_IERUwElkg,0,0,2021-03-18T15:17:11Z,"Hi, Can you pls help let me know  why we did not use this filter as well:   fb_search_results.notes ILIKE CONCAT (&#39;%&#39;,fb_search_results.query,&#39;%&#39;)",2021-03-18T15:17:11Z
UgzY6GXWqCAXMquHCCl4AaABAg,@malvikachandhok8571,,1,W_IERUwElkg,1,0,2021-03-15T15:50:56Z,Thank you! Your videos are super helpful :),2021-03-15T15:50:56Z
UgzY6GXWqCAXMquHCCl4AaABAg.9Kuzl_Ayy119KvhwD5n7Ef,@stratascratch,UgzY6GXWqCAXMquHCCl4AaABAg,2,W_IERUwElkg,0,1,2021-03-15T22:34:21Z,Thank you for watching!,2021-03-15T22:34:21Z
UgyJ3WMggLYsHlJ7_G14AaABAg,@mikemihay,,1,W_IERUwElkg,1,0,2021-01-13T16:51:00Z,Excellent content!,2021-01-13T16:51:00Z
UgyJ3WMggLYsHlJ7_G14AaABAg.9IT191otKEc9ITtEWN7I8Y,@stratascratch,UgyJ3WMggLYsHlJ7_G14AaABAg,2,W_IERUwElkg,0,0,2021-01-14T00:52:20Z,Thanks for watching!,2021-01-14T00:52:20Z
UgzxmACBxzqbo0RPOJp4AaABAg,@portiseremacunix,,1,W_IERUwElkg,1,0,2021-01-05T21:46:58Z,Can I simply use CTE instead of the subqueries? CTE looks very clean.,2021-01-05T21:46:58Z
UgzxmACBxzqbo0RPOJp4AaABAg.9I8xep0ogWM9I96F3Xqavc,@stratascratch,UgzxmACBxzqbo0RPOJp4AaABAg,2,W_IERUwElkg,0,2,2021-01-05T23:10:42Z,"Yes, of course use a CTE. I prefer them as well. Sometimes when I&#39;m working through a problem and communicating my solution, it&#39;s more natural to use them as subqueries just based on how I&#39;m explaining the concepts.",2021-01-05T23:10:42Z
UgzU7531XmL8JKktG7t4AaABAg,@followmycrafts8811,,1,W_IERUwElkg,0,0,2021-01-04T11:58:39Z,Thank You,2021-01-04T11:58:39Z
Ugy_NkLMYbuyr6B_Mgl4AaABAg,@TinaHuang1,,1,W_IERUwElkg,1,18,2021-01-03T06:53:22Z,Your thumbnails are getting more awesome :D Also awesome content ofc üî•,2021-01-03T06:53:22Z
Ugy_NkLMYbuyr6B_Mgl4AaABAg.9I2CoC9dOEy9I3AlBfj5tJ,@stratascratch,Ugy_NkLMYbuyr6B_Mgl4AaABAg,2,W_IERUwElkg,0,3,2021-01-03T15:54:43Z,"haha, I&#39;m trying to push the ridiculousness up a level to get that CTR up.",2021-01-03T15:54:43Z
Ugxj1_Bm0cMC5AJQUjd4AaABAg,@rakibraihanrimon8784,,1,W_IERUwElkg,1,0,2020-12-31T18:10:22Z,Tnx,2020-12-31T18:10:22Z
Ugxj1_Bm0cMC5AJQUjd4AaABAg.9Hwgu28AKtu9HwkItGeTBn,@stratascratch,Ugxj1_Bm0cMC5AJQUjd4AaABAg,2,W_IERUwElkg,0,0,2020-12-31T18:40:07Z,Thanks for watching!,2020-12-31T18:40:07Z
UgxOOo4r8UGti6gClr14AaABAg,@mdabulkalamazad6775,,1,W_IERUwElkg,1,0,2020-12-30T23:00:32Z,Thanks a lot Sir,2020-12-30T23:00:32Z
UgxOOo4r8UGti6gClr14AaABAg.9HudJKKFMIL9HuhxVdxGgx,@stratascratch,UgxOOo4r8UGti6gClr14AaABAg,2,W_IERUwElkg,0,0,2020-12-30T23:41:06Z,Thanks so much! Glad you enjoyed this one.,2020-12-30T23:41:06Z
UgxkZJTyAp-UxwHWguB4AaABAg,@majafuntv4538,,1,W_IERUwElkg,1,0,2020-12-30T22:59:53Z,Best SQL Tutorial,2020-12-30T22:59:53Z
UgxkZJTyAp-UxwHWguB4AaABAg.9HudEc6a9Qz9Huhvq4Ufv1,@stratascratch,UgxkZJTyAp-UxwHWguB4AaABAg,2,W_IERUwElkg,0,2,2020-12-30T23:40:52Z,Thanks! I&#39;m glad you&#39;ve enjoyed them. More to come! And maybe I&#39;ll dive into some python if people are interested.,2020-12-30T23:40:52Z
Ugx3Ugl455rDN_DD4Hp4AaABAg,@rollinas1,,1,W_IERUwElkg,2,1,2020-12-28T21:03:31Z,"The most valuable training material on SQL ever. <br>Sampling data, probability, p-value - these things would be very interesting for me.",2020-12-28T21:03:31Z
Ugx3Ugl455rDN_DD4Hp4AaABAg.9HpHKXUSR9L9HpVogED24L,@stratascratch,Ugx3Ugl455rDN_DD4Hp4AaABAg,2,W_IERUwElkg,0,2,2020-12-28T23:10:06Z,Thanks for the kind words. I am going to be doing some python coding sometime in the near future which would include some data sampling (or more specifically how to do that). I&#39;ll be creating tutorials on how to work with python soon once I get through some &quot;interviewing&quot; material. Hope you keep watching.,2020-12-28T23:10:06Z
Ugx3Ugl455rDN_DD4Hp4AaABAg.9HpHKXUSR9L9Kvq4FUrJAD,@malvikachandhok8571,Ugx3Ugl455rDN_DD4Hp4AaABAg,2,W_IERUwElkg,0,0,2021-03-15T23:45:29Z,@@stratascratch Will look forward to that as well! Thanks again :),2021-03-15T23:45:29Z
Ugz9J5ObX3g1xxkDw1Z4AaABAg,@classkori5507,,1,W_IERUwElkg,1,0,2020-12-28T15:15:54Z,Thank you,2020-12-28T15:15:54Z
Ugz9J5ObX3g1xxkDw1Z4AaABAg.9HoeYbKRBw89HopmabaNWE,@stratascratch,Ugz9J5ObX3g1xxkDw1Z4AaABAg,2,W_IERUwElkg,0,0,2020-12-28T16:54:04Z,Thanks for watching. Let me know if you have any topics you&#39;d like me to cover. Happy to make videos to help others.,2020-12-28T16:54:04Z
Ugwe-n8EYD5_-BN56ll4AaABAg,@stratascratch,,1,1SgHHQeRuxs,0,0,2020-10-18T20:36:21Z,"Please let me know what you guys think!<br><br>Timestamps:<br>Intro: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=0m13s"">0:13</a>)<br>Format Of A DS Interview: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=0m57s"">0:57</a>)<br>4 Topics Covered During DS Interview: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=1m47s"">1:47</a>)<br>2 Things To Consider In Choosing a Learning Resource: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=2m51s"">2:51</a>)<br>Glassdoor As A Learning Platform: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=3m40s"">3:40</a>)<br><a href=""http://brilliant.org/"">Brilliant.org</a> For Statistics &amp; Probability: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=5m40s"">5:40</a>)<br>Resources To Learn Modeling Techniques: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=7m17s"">7:17</a>)<br>LeetCode To For The Coding Portion Of Interview: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=10m00s"">10:00</a>)<br>StrataScratch To Prepare For Data Science Interviews: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=11m43s"">11:43</a>)<br>Conclusion: (<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=13m42s"">13:42</a>)",2020-10-18T20:36:21Z
Ugxd1f9PR4a5jiBcBuh4AaABAg,@DataProfessor,,1,1SgHHQeRuxs,1,0,2021-07-18T15:56:03Z,"<a href=""https://www.youtube.com/watch?v=1SgHHQeRuxs&amp;t=9m24s"">9:24</a> Hey, thanks Nate for the shoutout!",2021-07-18T15:56:03Z
Ugxd1f9PR4a5jiBcBuh4AaABAg.9PwriPpc9gW9Py2pd-bSwl,@stratascratch,Ugxd1f9PR4a5jiBcBuh4AaABAg,2,1SgHHQeRuxs,0,0,2021-07-19T03:01:08Z,Thanks for interviewing me this past weekend!,2021-07-19T03:01:08Z
UgxnFHdCo5z7w9b4NG54AaABAg,@praveenkumar-mh2dt,,1,1SgHHQeRuxs,1,0,2021-06-14T18:34:17Z,Do Data Science interviews have a round of data structures and algorithms questions ?,2021-06-14T18:34:17Z
UgxnFHdCo5z7w9b4NG54AaABAg.9O_anYF8vwl9O_yjl7sC75,@stratascratch,UgxnFHdCo5z7w9b4NG54AaABAg,2,1SgHHQeRuxs,0,1,2021-06-14T22:03:29Z,Some of them do. Not all though. Much of it depends on what level of experience and what type of company you&#39;re interviewing for. It&#39;s good to ask the recruiter and practice for them.,2021-06-14T22:03:29Z
UgySGmqkVd65P2-mDyh4AaABAg,@Tridentor,,1,1SgHHQeRuxs,1,1,2021-05-25T19:06:21Z,Statquest youtube channel covers a lot of math and DS topics,2021-05-25T19:06:21Z
UgySGmqkVd65P2-mDyh4AaABAg.9Nm9Zt23Qtj9NmIhrNIhwF,@stratascratch,UgySGmqkVd65P2-mDyh4AaABAg,2,1SgHHQeRuxs,0,1,2021-05-25T20:26:13Z,You&#39;re totally right about that channel. I used to use it to prep for interviews too,2021-05-25T20:26:13Z
UgwEhoZMdkBg-Rb-9s54AaABAg,@Mj_0509,,1,1SgHHQeRuxs,1,0,2021-01-05T00:47:54Z,Extremely helpful video..,2021-01-05T00:47:54Z
UgwEhoZMdkBg-Rb-9s54AaABAg.9I6h_6v-D2d9I75ss9SHwz,@stratascratch,UgwEhoZMdkBg-Rb-9s54AaABAg,2,1SgHHQeRuxs,0,0,2021-01-05T04:29:03Z,Feel free to let me know if you have any further questions or topic ideas.,2021-01-05T04:29:03Z
UgzmamvhsbVGxbFM6KV4AaABAg,@russ_fx,,1,1SgHHQeRuxs,1,2,2020-10-29T07:18:08Z,The platform looks pretty good. Please add content for Data Engineering Interviews. A lot probably overlaps with Data Science anyway.,2020-10-29T07:18:08Z
UgzmamvhsbVGxbFM6KV4AaABAg.9FOJAktVRCi9FPRwAAMkf1,@stratascratch,UgzmamvhsbVGxbFM6KV4AaABAg,2,1SgHHQeRuxs,0,1,2020-10-29T17:53:53Z,"Thanks Paul. I wouldn&#39;t mind adding some DE interviews but I don&#39;t know much about it. My impression of a data engineer is a lot of pipeline building? Is that true? If so, that&#39;s what we do in data science day in and day out. I wouldn&#39;t mind creating a few videos on how I&#39;ve built and productionized data pipelines at my job.",2020-10-29T17:53:53Z
Ugy3pl1lPMtQ3OAqe_54AaABAg,@andyn6053,,1,1SgHHQeRuxs,1,1,2020-10-21T13:03:34Z,They usually dont ask ask about technical things. What is more important is chemistry and that u connect with the recruiter. They hire the one they like. Skills can be taught later.,2020-10-21T13:03:34Z
Ugy3pl1lPMtQ3OAqe_54AaABAg.9F4KLqLo4x09F4QZ4EEf1m,@stratascratch,Ugy3pl1lPMtQ3OAqe_54AaABAg,2,1SgHHQeRuxs,0,1,2020-10-21T13:57:48Z,"Apologizes if I was unclear. This is after the recruiter screen call and is about how to prepare for (and what to expect on) the first few rounds of your data science interview. You&#39;re right in that the recruiter screening will not be technical. They&#39;ll usually ask about things on your resume and make sure you&#39;re interested in the role. I&#39;ve had recruiters ask me simple probability questions and simple product sense questions, but it was more along the vein of giving me an example of what I can expect on the real interview.",2020-10-21T13:57:48Z
Ugzx5or_yU9gsvQYL2h4AaABAg,@siddeshsakhalkar6117,,1,1SgHHQeRuxs,1,2,2020-10-18T18:10:50Z,Domain knowledge,2020-10-18T18:10:50Z
Ugzx5or_yU9gsvQYL2h4AaABAg.9Ey97iYp-mE9EyPqhYBqu0,@stratascratch,Ugzx5or_yU9gsvQYL2h4AaABAg,2,1SgHHQeRuxs,0,1,2020-10-18T20:36:55Z,Thank you!,2020-10-18T20:36:55Z
UgwJi8pIOiVzeBmQtfJ4AaABAg,@anthonyl9722,,1,1SgHHQeRuxs,1,2,2020-10-18T18:01:09Z,"Thank you. I am an aspiring computer scientist and therefore value the coding and mathematics exercises most, and some of these certainly help.",2020-10-18T18:01:09Z
UgwJi8pIOiVzeBmQtfJ4AaABAg.9Ey80iZauKn9EyPpb0GORK,@stratascratch,UgwJi8pIOiVzeBmQtfJ4AaABAg,2,1SgHHQeRuxs,0,0,2020-10-18T20:36:46Z,Good luck! Feel free to drop me a message if you have any questions or comments.,2020-10-18T20:36:46Z
Ugy25ynlGsVy2GGt4Y94AaABAg,@Prashanth-yj6qx,,1,rMoUoYxfBNk,1,0,2021-10-05T17:32:34Z,I have opened the question which you share the link. for date 2020-01-04 there is no &#39;accepted&#39; record,2021-10-05T17:32:34Z
Ugy25ynlGsVy2GGt4Y94AaABAg.9T7SZSlAM7A9T7lkpOxB9f,@stratascratch,Ugy25ynlGsVy2GGt4Y94AaABAg,2,rMoUoYxfBNk,0,0,2021-10-05T20:29:01Z,I think there was a change in dataset. The schema is the same but the values might be slightly off to compensate for some corner cases. You can still watch the video for the theory and how to handle the question but there will be some things off since the data has changed slightly,2021-10-05T20:29:01Z
UgwXqQcxdgN9nGpK1wl4AaABAg,@snoopysweepy,,1,rMoUoYxfBNk,0,0,2021-10-03T03:32:51Z,"Hi Nate, I have a question here, why is is necessary to split the table here? Is this not similar to the popularity rate video you have (where you can write a case when statement in select statement to identify sent vs accepted? )<br>Thanks for sharing this video",2021-10-03T03:32:51Z
UgzAfCm8HL1hYdsuI2F4AaABAg,@siddeshsakhalkar6117,,1,rMoUoYxfBNk,2,0,2020-09-21T05:23:01Z,üôèüôèüôèüôèüëçüëçüëçüëç,2020-09-21T05:23:01Z
UgzAfCm8HL1hYdsuI2F4AaABAg.9DrFnNpfAbZ9DsRjqy4t0Y,@stratascratch,UgzAfCm8HL1hYdsuI2F4AaABAg,2,rMoUoYxfBNk,0,0,2020-09-21T16:26:38Z,Thanks for watching! Let me know if you have any feedback or questions for me!,2020-09-21T16:26:38Z
UgzAfCm8HL1hYdsuI2F4AaABAg.9DrFnNpfAbZ9DxpF4v_Ei5,@siddeshsakhalkar6117,UgzAfCm8HL1hYdsuI2F4AaABAg,2,rMoUoYxfBNk,0,0,2020-09-23T18:36:57Z,"@@stratascratch I&#39;m a new data science aspirant , so I don&#39;t know what to give feedback , I&#39;m a just a second year cs student . But you make very good video with clear vision. May you get more subscribers , views and likes ‚ò∫Ô∏è‚ò∫Ô∏è‚ò∫Ô∏è‚ò∫Ô∏è",2020-09-23T18:36:57Z
Ugz55SZ1MJfRQ2HCE9F4AaABAg,@urvashijoshi3829,,1,5-1ClCuQCIw,4,0,2021-03-30T08:14:06Z,"solution is:----   with cte_a as (select business_name,review_text dense_rank() over (order by cool desc) as r from yelp_review) select cte_a.business_name,cte_a.review_text from cte_a where r=1<br><br><br> is this correct or not??",2021-03-30T08:14:06Z
Ugz55SZ1MJfRQ2HCE9F4AaABAg.9LVnPk7nsFQ9LWenCjrk_A,@stratascratch,Ugz55SZ1MJfRQ2HCE9F4AaABAg,2,5-1ClCuQCIw,0,0,2021-03-30T16:18:02Z,Did you get the same output as in the video?,2021-03-30T16:18:02Z
Ugz55SZ1MJfRQ2HCE9F4AaABAg.9LVnPk7nsFQ9QYsmK6gnss,@niveditakumari701,Ugz55SZ1MJfRQ2HCE9F4AaABAg,2,5-1ClCuQCIw,0,0,2021-08-02T19:35:42Z,"@@stratascratch Same question, it is not possible to achieve the same result by DENSE_RANK()?",2021-08-02T19:35:42Z
Ugz55SZ1MJfRQ2HCE9F4AaABAg.9LVnPk7nsFQ9QYt4ff50Q9,@niveditakumari701,Ugz55SZ1MJfRQ2HCE9F4AaABAg,2,5-1ClCuQCIw,0,0,2021-08-02T19:38:21Z,We can also put the subquery in the where clause to match the max(cool) to the cool votes?,2021-08-02T19:38:21Z
Ugz55SZ1MJfRQ2HCE9F4AaABAg.9LVnPk7nsFQ9QZFgHCt0tJ,@stratascratch,Ugz55SZ1MJfRQ2HCE9F4AaABAg,2,5-1ClCuQCIw,0,1,2021-08-02T23:04:36Z,@@niveditakumari701 This one might be okay to use DENSE RANK because you just want to get the #1 and a dense_rank will perserve all ties.,2021-08-02T23:04:36Z
Ugxi9_NcXMSYiC-5gVZ4AaABAg,@urvashijoshi3829,,1,5-1ClCuQCIw,1,0,2021-03-30T08:10:12Z,hey where is the question link. that link in description is blank. can you share again.,2021-03-30T08:10:12Z
Ugxi9_NcXMSYiC-5gVZ4AaABAg.9LVmyBnpHr39LWell0Wqhs,@stratascratch,Ugxi9_NcXMSYiC-5gVZ4AaABAg,2,5-1ClCuQCIw,0,0,2021-03-30T16:17:51Z,"thanks for pointing this out: <a href=""https://platform.stratascratch.com/coding-question?id=10060&amp;python="">https://platform.stratascratch.com/coding-question?id=10060&amp;python=</a>",2021-03-30T16:17:51Z
UgwrnecaPcE1Nf3GIVZ4AaABAg,@trucker4844,,1,5-1ClCuQCIw,0,0,2020-09-02T13:55:39Z,Good,2020-09-02T13:55:39Z
UgytfovlwB-MUq1IsPN4AaABAg,@mikemihay,,1,5-1ClCuQCIw,1,1,2020-08-31T04:16:40Z,Great video! Thank you!,2020-08-31T04:16:40Z
UgytfovlwB-MUq1IsPN4AaABAg.9D03W-X88Dd9E2r3yuaCRO,@stratascratch,UgytfovlwB-MUq1IsPN4AaABAg,2,5-1ClCuQCIw,0,1,2020-09-26T02:48:21Z,You are welcome!,2020-09-26T02:48:21Z
Ugz9j4ruWxgTsj6Q_Vl4AaABAg,@deepfakevasmoy3477,,1,rf2udE9q-SM,1,1,2020-08-22T15:06:41Z,Great! Thanks,2020-08-22T15:06:41Z
Ugz9j4ruWxgTsj6Q_Vl4AaABAg.9Cf2k6Bn3en9E2r4dDyyWj,@stratascratch,Ugz9j4ruWxgTsj6Q_Vl4AaABAg,2,rf2udE9q-SM,0,0,2020-09-26T02:48:26Z,You&#39;re welcome!,2020-09-26T02:48:26Z
UgwPM-_Px89q4LGZUbF4AaABAg,@butoyighyslain171,,1,rf2udE9q-SM,1,1,2020-08-21T18:15:17Z,short and sweet..cheers!!!,2020-08-21T18:15:17Z
UgwPM-_Px89q4LGZUbF4AaABAg.9CcoXYX8KOy9E2r5SAqdRk,@stratascratch,UgwPM-_Px89q4LGZUbF4AaABAg,2,rf2udE9q-SM,0,0,2020-09-26T02:48:33Z,Glad you liked it!,2020-09-26T02:48:33Z
Ugz8PTgtgtNkdjZ1uHh4AaABAg,@borisgiba4510,,1,rf2udE9q-SM,0,1,2020-08-21T12:07:09Z,"Hello Nate,<br>thank you for the tips!<br>I am going to look for an internship next year as well and I think these tips are also very applicable in that manner.<br>I just found your channel and I am going to follow it in the future. I look forward to your next videos! :)",2020-08-21T12:07:09Z
Ugz-ziqkeqRDYsQ6iJ94AaABAg,@rushikeshgandhmal7304,,1,rf2udE9q-SM,2,0,2020-08-20T17:29:02Z,"Never opened such quick after getting notification. Thank you for great info.<br><br>There are plenty of  resource of Data Science, but we need this kinda great direction, guidance.<br><br>Please come up with such videos that could help the beginner who wants to start their career in this field.<br><br>Thank you once again ‚ù§Ô∏è",2020-08-20T17:30:57Z
Ugz-ziqkeqRDYsQ6iJ94AaABAg.9Ca9RwOzp2R9CaA0C1EDfn,@stratascratch,Ugz-ziqkeqRDYsQ6iJ94AaABAg,2,rf2udE9q-SM,0,1,2020-08-20T17:33:59Z,Thanks so much for the kind words! I plan to keep on uploading these types of videos and will do my best to balance out career advice with more technical content for beginners.,2020-08-20T17:33:59Z
Ugz-ziqkeqRDYsQ6iJ94AaABAg.9Ca9RwOzp2R9CaE_U8a-Pz,@rushikeshgandhmal7304,Ugz-ziqkeqRDYsQ6iJ94AaABAg,2,rf2udE9q-SM,0,0,2020-08-20T18:13:53Z,"@@stratascratch Thank you for your response, it will help.<br><br>Well, maybe within a year I will get good knowledge and skills, and I&#39;m also preparing from your website that helping a lot.<br><br>After learning all surely will apply for interview to you.",2020-08-20T18:13:53Z
Ugwd6gGcIxgxUG9nnRV4AaABAg,@mikemihay,,1,SrqJuUNN62I,1,0,2020-08-20T22:56:50Z,Great video! Thank you! New subscriber here :),2020-08-20T22:56:50Z
Ugwd6gGcIxgxUG9nnRV4AaABAg.9CajxlLWEFi9CatSHnERoG,@stratascratch,Ugwd6gGcIxgxUG9nnRV4AaABAg,2,SrqJuUNN62I,0,0,2020-08-21T00:19:46Z,Thank you! Feel free to comment on the type of content you&#39;d like to see here. I&#39;ll try to make vids to answer any questions you all have.,2020-08-21T00:19:46Z
Ugy35jKmkpYzl37yt2x4AaABAg,@alphaalpha5392,,1,SrqJuUNN62I,0,0,2020-08-10T15:00:33Z,Great video <br>If I was asked why I have chosed data science for my master‚Äôs degree hw should I answer?,2020-08-10T15:00:33Z
UgymMMaOQEE1tcHdsjN4AaABAg,@yaswanth2159,,1,SrqJuUNN62I,0,1,2020-08-10T14:01:44Z,"Nice information sir,I&#39;m from India üáÆüá≥",2020-08-10T14:01:44Z
Ugwtlnv5YmZ0xz6QLTF4AaABAg,@mossab1115,,1,xbc2GpGUXwc,0,1,2020-06-25T10:04:40Z,"helpful video , thank u Strata",2020-06-25T10:04:40Z
UgzXEgBNSlV_DTs0lj54AaABAg,@sbeg-wv7fz,,1,bM7LSQIKYs8,0,0,2023-04-08T18:44:16Z,Not able to increase video quality from 240p.,2023-04-08T18:44:16Z
UgwsZyMLXhJeMmbYpEB4AaABAg,@johnnywalker7894,,1,bM7LSQIKYs8,1,0,2022-04-16T05:14:38Z,"Dude seriously, out of all the vids on pandas i&#39;ve watched and there where a few, this is by far the best!",2022-04-16T05:14:52Z
UgwsZyMLXhJeMmbYpEB4AaABAg.9_s5XRWuuUe9_teMuWWFmq,@stratascratch,UgwsZyMLXhJeMmbYpEB4AaABAg,2,bM7LSQIKYs8,0,0,2022-04-16T19:47:00Z,Thank you!,2022-04-16T19:47:00Z
Ugymc9rIJXsDBeqS8f54AaABAg,@michelchaghoury870,,1,bM7LSQIKYs8,2,0,2022-01-14T18:38:02Z,"man was searching for silver, i ended up by founding gold, every single vid on this channel hold important information, I am a full stack developer I want to learn Data Science, ML, AI, this channel really helps a lot, can you please in the future make content more related to ML and AI, please? and keep going",2022-01-14T18:38:02Z
Ugymc9rIJXsDBeqS8f54AaABAg.9XBdLF0UcLN9XCqAkr_MFN,@stratascratch,Ugymc9rIJXsDBeqS8f54AaABAg,2,bM7LSQIKYs8,0,1,2022-01-15T05:49:26Z,"Thanks for the kind words! Yup, will keep creating content. ML and AI will be coming out sometime this year!",2022-01-15T05:49:26Z
Ugymc9rIJXsDBeqS8f54AaABAg.9XBdLF0UcLN9XDBCc5er-d,@michelchaghoury870,Ugymc9rIJXsDBeqS8f54AaABAg,2,bM7LSQIKYs8,0,0,2022-01-15T09:01:55Z,"@@stratascratch ahh great really exited, kep going man this channel is a treasure",2022-01-15T09:01:55Z
Ugx-at8wxPz7KtjzFtx4AaABAg,@rohitsarma4226,,1,bM7LSQIKYs8,5,2,2021-09-24T05:29:21Z,"dude thanks a lot for posting these series , it helped me clear my interview, pls do post such amazing series in the future",2021-09-24T05:29:21Z
Ugx-at8wxPz7KtjzFtx4AaABAg.9Seq2m3HFZx9Sg6WhLlwcd,@stratascratch,Ugx-at8wxPz7KtjzFtx4AaABAg,2,bM7LSQIKYs8,0,1,2021-09-24T17:21:13Z,Nice! Congrats on your interview. Good luck on everything!,2021-09-24T17:21:13Z
Ugx-at8wxPz7KtjzFtx4AaABAg.9Seq2m3HFZx9TNS1_LZB6u,@SagarKumar-db2xy,Ugx-at8wxPz7KtjzFtx4AaABAg,2,bM7LSQIKYs8,0,0,2021-10-11T22:35:47Z,Yaar konsi company m? Reference lagwa do yaar,2021-10-11T22:35:47Z
Ugx-at8wxPz7KtjzFtx4AaABAg.9Seq2m3HFZx9TNbh8l7UG3,@rohitsarma4226,Ugx-at8wxPz7KtjzFtx4AaABAg,2,bM7LSQIKYs8,0,1,2021-10-12T00:08:59Z,"@@SagarKumar-db2xy internship hain bhai college mein aayi thi fidelity hain, usne saare wahi qs pucche jo isne iss video main samjhaya hain",2021-10-12T00:08:59Z
Ugx-at8wxPz7KtjzFtx4AaABAg.9Seq2m3HFZx9Tk5Q1Ks6S3,@SagarKumar-xo5qj,Ugx-at8wxPz7KtjzFtx4AaABAg,2,bM7LSQIKYs8,0,0,2021-10-21T02:59:56Z,@@rohitsarma4226 yaar agar aur internship oppurtunities hui tho batana. chahe tho first month ki salary lelena.,2021-10-21T02:59:56Z
Ugx-at8wxPz7KtjzFtx4AaABAg.9Seq2m3HFZx9Tk82Cj27zy,@rohitsarma4226,Ugx-at8wxPz7KtjzFtx4AaABAg,2,bM7LSQIKYs8,0,1,2021-10-21T03:22:53Z,@@SagarKumar-xo5qj sure bhai salary ki koi zarurat nhi hain,2021-10-21T03:22:53Z
UgxV_1Ba5n0V-4xngDB4AaABAg,@fadwa2413,,1,bM7LSQIKYs8,0,0,2021-08-23T13:17:54Z,THANK YOU SO MUCH,2021-08-23T13:17:54Z
UgxDsCQokXjZCkTeBNl4AaABAg,@byoutekinaeiyuu,,1,bM7LSQIKYs8,1,0,2021-02-12T14:15:28Z,You cover stuff about pandas most other tutorials I watched usually miss. Will certainly watch the other videos from the pandas&#39; series and the numpy series too! Thank you! :),2021-02-12T14:15:28Z
UgxDsCQokXjZCkTeBNl4AaABAg.9Jf-CAdumHL9Jf8ieIkHAV,@stratascratch,UgxDsCQokXjZCkTeBNl4AaABAg,2,bM7LSQIKYs8,0,0,2021-02-12T15:38:41Z,Thanks so much for watching! The video series is focused on data analytics/science so some topics are missed by other videos if the focus isn&#39;t on analytics. I tried to make my videos interactive too so download the notebooks to follow along!,2021-02-12T15:38:41Z
UgxKFWF9XkrKP9Mz52N4AaABAg,@DarshitParmar,,1,JHxojhrCsRs,0,0,2022-05-30T15:39:31Z,Love this channel! Totally hooked!,2022-05-30T15:39:31Z
Ugz0JWJIlo03aDutQq94AaABAg,@yusufbas035,,1,JHxojhrCsRs,0,0,2022-04-05T18:10:56Z,Great work,2022-04-05T18:10:56Z
UgwdLu-z6Dz4oYmkdHN4AaABAg,@fadwa2413,,1,JHxojhrCsRs,0,0,2021-08-23T18:50:27Z,Great Work,2021-08-23T18:50:27Z
UgyhVqX9-TRd4TpZTdR4AaABAg,@sudiptodas4168,,1,JHxojhrCsRs,0,0,2021-07-19T14:41:57Z,this has really helped me nate. thanks for the upload. keep up the great work.,2021-07-19T14:41:57Z
UgyJ3xBtZq2YWGKTMjh4AaABAg,@ericmaus6689,,1,JHxojhrCsRs,1,0,2021-04-08T23:36:17Z,These videos are great. Thanks Nate! Keep up the good work,2021-04-08T23:36:17Z
UgyJ3xBtZq2YWGKTMjh4AaABAg.9Ltc5oTSCeW9LtoJGTTr6B,@stratascratch,UgyJ3xBtZq2YWGKTMjh4AaABAg,2,JHxojhrCsRs,0,0,2021-04-09T01:22:58Z,Thanks for watching these videos! Appreciate it.,2021-04-09T01:22:58Z
UgwGhoeGmwLCeb-ZAUV4AaABAg,@wackyprofessor,,1,mCXZXe-Na3s,1,0,2022-12-10T17:11:39Z,Wow.  Amazing Colab notenook to follow along.  There are even exercises and solutions!  Thank you.,2022-12-10T19:06:58Z
UgwGhoeGmwLCeb-ZAUV4AaABAg.9jTCmMjgcRt9jmwcZ0P6Zc,@stratascratch,UgwGhoeGmwLCeb-ZAUV4AaABAg,2,mCXZXe-Na3s,0,0,2022-12-18T18:25:49Z,Glad it was helpful!,2022-12-18T18:25:49Z
UgwvCJLnrTDeX5QZWeh4AaABAg,@yusufbas035,,1,mCXZXe-Na3s,0,0,2022-04-05T21:59:41Z,I&#39;ve learned so much from this course Thank you,2022-04-05T21:59:41Z
UgyasTQLme73DVvOPEJ4AaABAg,@juanrojo4766,,1,mCXZXe-Na3s,0,1,2021-12-22T17:02:25Z,"Dude, what a great course. Thank you for doing this !",2021-12-22T17:02:25Z
UgyTal5sI9RZnV8F17p4AaABAg,@bittusharma9401,,1,mCXZXe-Na3s,1,0,2021-11-16T17:17:05Z,Hi how can I copy the dataframe data into postgres database without using row by row iterations,2021-11-16T17:17:05Z
UgyTal5sI9RZnV8F17p4AaABAg.9Uo_AlhzJqA9UosiUBD9sB,@stratascratch,UgyTal5sI9RZnV8F17p4AaABAg,2,mCXZXe-Na3s,0,1,2021-11-16T20:07:51Z,"I created a video on how to do it (<a href=""https://www.youtube.com/watch?v=wqBFgaMgFQA)"">https://www.youtube.com/watch?v=wqBFgaMgFQA)</a>. The code is also on github (see description). This technique copies the data straight into the db table without row by row iterations",2021-11-16T20:07:51Z
UgxVqlcCkZl9gYJjPKl4AaABAg,@fadwa2413,,1,mCXZXe-Na3s,2,0,2021-08-23T19:25:23Z,"Thanks a lot for your effort and time, YOU ARE THE BEST,<br>I am just wondering if you can help me to suggest a good tutorial to learn about data Visualization (seaborn&amp;matplotlib).",2021-08-23T19:25:23Z
UgxVqlcCkZl9gYJjPKl4AaABAg.9ROwI4hldlf9RPmIAy1hD7,@stratascratch,UgxVqlcCkZl9gYJjPKl4AaABAg,2,mCXZXe-Na3s,0,1,2021-08-24T03:17:15Z,Thanks for watching! I learned data viz from a book called Python Data Science Handbook by Jake VanderPlas. He has a lot of jupyter notebooks that are free too so you might want to try his resources first. I&#39;m not sure of any other resources as it&#39;s been a while since I&#39;ve had to research this type of material. I would also try the Python Programmer YT channel. He has a lot of resources he regularly recommends.,2021-08-24T03:17:15Z
UgxVqlcCkZl9gYJjPKl4AaABAg.9ROwI4hldlf9RQYwcdfO5t,@fadwa2413,UgxVqlcCkZl9gYJjPKl4AaABAg,2,mCXZXe-Na3s,0,0,2021-08-24T10:31:05Z,"@@stratascratch Thanks a lot, i will do that",2021-08-24T10:31:05Z
Ugw3abnyTgxilUYCfgN4AaABAg,@leleemagnu6831,,1,mCXZXe-Na3s,1,1,2020-12-23T00:52:13Z,"This is a great course. Probably the best i have seen so far.<br>It is simple to follow, cover all the necessary and it is mostly to the point.<br>Thank you very much.<br>I think i spotted a couple of issues in the notebook attached to this session:<br>1) It seems that the necessary libraries are not imported:<br>import numpy as np         # Perhaps this is not really needed here    <br>import pandas as pd<br>2) The first line where the csv file is downloaded shows:<br>data = pd.read(&#39;titanic.csv&#39;)           # This will not work<br>while it should read:<br>data = pd.read_csv(&#39;titanic.csv&#39;)<br>Thank you again<br>e",2020-12-23T00:52:13Z
Ugw3abnyTgxilUYCfgN4AaABAg.9HaEjKHiW6g9HaYIb6x0Ts,@stratascratch,Ugw3abnyTgxilUYCfgN4AaABAg,2,mCXZXe-Na3s,0,0,2020-12-23T03:43:11Z,Thanks so much for those kind words. I&#39;m not sure why those lines went missing but I did add the imports of those libraries and rewrote the read_csv() line. So you should see it working in the notebook if you re-download it. Thanks and I&#39;m glad you enjoyed the lecture/notebook.,2020-12-23T03:43:11Z
Ugy_aE2q43swugHcJdF4AaABAg,@yusufbas035,,1,iwIPnlCHlVQ,0,0,2022-04-05T22:21:29Z,good work thank you.,2022-04-05T22:21:29Z
UgztWRC1qYcJ0LuWOeR4AaABAg,@nandiniguntur4509,,1,iwIPnlCHlVQ,0,0,2022-01-24T22:54:36Z,&#39;&#39;Succinct&quot; I would say. Thanks guys!,2022-01-24T22:54:36Z
UgxDDy1fE7hNLVjyvw14AaABAg,@judeleon8485,,1,iwIPnlCHlVQ,1,1,2020-09-26T01:09:57Z,"Nice tutorial but it&#39;s of little use, simply because you did not zoom in for the print to be visible enough",2020-09-26T01:10:23Z
UgxDDy1fE7hNLVjyvw14AaABAg.9E2foCe4bfI9E2rGOr1Do7,@stratascratch,UgxDDy1fE7hNLVjyvw14AaABAg,2,iwIPnlCHlVQ,0,0,2020-09-26T02:50:03Z,Sorry for that. I didn&#39;t realize it was that small. I&#39;ll make sure the text is much larger in the future!,2020-09-26T02:50:03Z
Ugx1jMK7C6C7Lts9IPp4AaABAg,@user-es1ec7lp5t,,1,DUgd48QYmfI,0,0,2023-09-19T08:32:09Z,"Content is interesting, it‚Äôs a carbon copy of material being described in the ‚ÄúData Science Handbook‚Äù from Jake VanderPlas, now you have the option to read or watch the video.",2023-09-19T08:32:09Z
UgzqXttPYz1w_UT8GXd4AaABAg,@user-gl5tj9jv2c,,1,DUgd48QYmfI,1,0,2023-08-11T17:34:15Z,can you please drop a link to download the dataset &#39;planets&#39;? That would really help me. Thank you :),2023-08-11T17:34:15Z
UgzqXttPYz1w_UT8GXd4AaABAg.9tHXKJv9Qv89tHzpf8gUdZ,@stratascratch,UgzqXttPYz1w_UT8GXd4AaABAg,2,DUgd48QYmfI,0,0,2023-08-11T21:52:04Z,"Here you go! <a href=""https://github.com/mwaskom/seaborn-data/blob/master/planets.csv"">https://github.com/mwaskom/seaborn-data/blob/master/planets.csv</a>",2023-08-11T21:52:04Z
Ugyrs-fdIaJgdFHY_554AaABAg,@Sam-tg4ii,,1,DUgd48QYmfI,0,0,2023-08-09T13:18:57Z,Hard to read the screen. Plz zoom in when recording. <br>Clear explanations. Thanks,2023-08-09T13:35:50Z
UgxaG6kJvWpaI2HIylx4AaABAg,@axhoang,,1,DUgd48QYmfI,1,0,2023-06-15T00:45:02Z,"its 2023, did you guys record this with a potato?",2023-06-15T00:45:02Z
UgxaG6kJvWpaI2HIylx4AaABAg.9qxxWes9ZwP9qzQOjEiIol,@vylon1075,UgxaG6kJvWpaI2HIylx4AaABAg,2,DUgd48QYmfI,0,0,2023-06-15T14:25:19Z,"How rude, do you think this video was made just for you? Also, you are quite dumb. This was uploaded more than 3 years ago.",2023-06-15T14:25:19Z
UgzdJaE5Vs2gAVKjP5l4AaABAg,@StefanoVerugi,,1,DUgd48QYmfI,0,0,2023-03-14T04:40:40Z,"you are truly talented, the superb teaching quality of your method is by far better (and more effective with entry level people like myself) than most of what can be found in the YT sphere these days<br>most grateful , immediately subscribed",2023-03-14T04:40:40Z
UgwfmIQjllWe3iWVbIN4AaABAg,@yadali4833,,1,DUgd48QYmfI,1,1,2023-01-27T16:32:31Z,What is x in x[&#39;data2&#39;] ? is it df? if so why when x is the cell value in the transform sectiom,2023-01-27T16:32:31Z
UgwfmIQjllWe3iWVbIN4AaABAg.9lOjSlo3VO99uiOaoHP_ZB,@osoriomatucurane9511,UgwfmIQjllWe3iWVbIN4AaABAg,2,DUgd48QYmfI,0,0,2023-09-16T09:48:07Z,"I have the same issue, I struggle a lot to get my head arround functions parameter and iterations.<br>It seems to me x is element row, each row is a distinct category. <br>Know, looking at [ ] operator, <br>X[&#39;data&#39;] I guess  gets access to data series corresponding to x category, from which the aggregation measure is calculated/performed over,  in this case the stdv",2023-09-16T09:48:07Z
Ugww19bz3Uv5IIk1i_94AaABAg,@utkalmaheshwari,,1,DUgd48QYmfI,1,0,2023-01-13T09:36:41Z,"In filter function, filter function is applied on groupby object. How it returned rows from original dataset ?",2023-01-13T09:36:41Z
Ugww19bz3Uv5IIk1i_94AaABAg.9kowjqQnlxZ9kxikSR7DsU,@stratascratch,Ugww19bz3Uv5IIk1i_94AaABAg,2,DUgd48QYmfI,0,0,2023-01-16T19:27:36Z,I&#39;m not sure if I understand your question but the filter function still has access to the dataset so the output can still have values from the original dataset if filtered in the correct way. I would play around with the filters and see what you get in the output as you experiment.,2023-01-16T19:27:36Z
UgyAil-QxHORCDAV__x4AaABAg,@manuel9345,,1,DUgd48QYmfI,0,1,2022-11-29T06:59:17Z,"Thank you, very useful",2022-11-29T06:59:17Z
Ugz6xIfFCsSGhuCNXC94AaABAg,@soojinkim6450,,1,DUgd48QYmfI,1,0,2022-10-26T02:14:58Z,Thank you for the explanation. What&#39;s the difference between transform() and apply()?,2022-10-26T02:14:58Z
Ugz6xIfFCsSGhuCNXC94AaABAg.9hcjOTn63789wmN2Ge9xPN,@adityaaware9844,Ugz6xIfFCsSGhuCNXC94AaABAg,2,DUgd48QYmfI,0,0,2023-11-06T15:54:16Z,Apply can use multiple columns in groupby but it&#39;s slower...<br><br><br>Transform can use single colm bt its faster,2023-11-06T15:54:16Z
UgzLWgcTbXevH6PzXW14AaABAg,@jongcheulkim7284,,1,DUgd48QYmfI,0,0,2022-10-05T01:27:59Z,Thank you^^,2022-10-05T01:27:59Z
UgzdNhBkMpuWV4Mon_R4AaABAg,@joseleonardosanchezvasquez1514,,1,DUgd48QYmfI,1,1,2022-09-14T16:52:55Z,Great thanks,2022-09-14T16:52:55Z
UgzdNhBkMpuWV4Mon_R4AaABAg.9fy9U2XFJEz9fzNrhrmHg3,@stratascratch,UgzdNhBkMpuWV4Mon_R4AaABAg,2,DUgd48QYmfI,0,0,2022-09-15T04:17:52Z,You&#39;re welcome.,2022-09-15T04:17:52Z
Ugyi-Ek45xDWYJLMcrV4AaABAg,@henrygraterol,,1,DUgd48QYmfI,0,1,2022-06-12T16:34:38Z,"The way you explain and manually breakdown the methods is amazing. I do not have a software background, only basic experience with for, while, and if-else loops in C.  I am able to easily understand each method due to the structure of your presentation. I subscribed to your channel after this video. Hope to see more of you and see your channel grow.",2022-06-12T16:34:38Z
Ugy78KVH1DriFXwsV4J4AaABAg,@jaysonjaylen,,1,DUgd48QYmfI,0,1,2022-04-07T03:22:44Z,"Great video, if you could increase the volume somehow that&#39;d be great though.",2022-04-07T03:22:44Z
Ugz45qKJ7iqlrfiQNNp4AaABAg,@yusufbas035,,1,DUgd48QYmfI,0,0,2022-04-06T11:46:54Z,great works keep going dude,2022-04-06T11:46:54Z
Ugy82yL8K99VFuXIVHJ4AaABAg,@surge8307,,1,DUgd48QYmfI,0,0,2022-03-30T10:13:35Z,I came from the pandas data science handbook to youtube to learn more but this is the same thing lol,2022-03-30T10:13:35Z
Ugzwoo7gk9jKQRRPdg94AaABAg,@davida99,,1,DUgd48QYmfI,2,0,2022-01-26T04:51:43Z,Love the videos. I became a premium member on SS and subbed to the channel. I&#39;ve seen a huge improvement in my sql AND python skills !<br><br>It would be nice to add more questions like leetcode does where maybe you restrict some questions to only using UPDATE or DELETE FROM or even some practice questions where we create tables,2022-01-26T04:51:43Z
Ugzwoo7gk9jKQRRPdg94AaABAg.9Xe3JbRRoj19XfKMQVHGP_,@stratascratch,Ugzwoo7gk9jKQRRPdg94AaABAg,2,DUgd48QYmfI,0,1,2022-01-26T16:39:53Z,"That&#39;s great to hear! We&#39;re definitely going to be releasing UPDATE/DELETE/CREATE questions this year. On our roadmap are data structure &amp; algorithm questions, take home assignments using python notebooks, and UPDATE/DELETE/CREATE questions. Stay tuned!",2022-01-26T16:39:53Z
Ugzwoo7gk9jKQRRPdg94AaABAg.9Xe3JbRRoj19XfsyzbBCko,@davida99,Ugzwoo7gk9jKQRRPdg94AaABAg,2,DUgd48QYmfI,0,1,2022-01-26T21:51:07Z,"@@stratascratch Wow, cant wait!",2022-01-26T21:51:07Z
Ugwnu73pX8_vFiWyqaR4AaABAg,@mohammadyahya78,,1,DUgd48QYmfI,0,0,2021-12-28T00:02:03Z,"Thank you very much. Hopefully you can also do more advanced pandas videos. This is very helpful. Not sure what <a href=""https://www.youtube.com/watch?v=DUgd48QYmfI&amp;t=36m22s"">36:22</a> `str.lower` means please and how it knows that this refer to `key` column?",2021-12-28T00:51:49Z
UgzcE5_hkBSAMBdiYPZ4AaABAg,@Sheshagiriksrao,,1,DUgd48QYmfI,0,0,2021-12-22T16:13:30Z,"Nice one, I was not able to understand groupby section from Jake Vandreplas&#39;s python data science handbook but your video helped me out, could you please take the planet data set example and use two keys to groupby, it is a bit tricky to understand, Thank you",2021-12-22T16:13:30Z
UgzPTVOkBonDG3yUueF4AaABAg,@royalchamp,,1,DUgd48QYmfI,0,0,2021-12-19T20:23:33Z,thanks,2021-12-19T20:23:33Z
Ugx4LbcykE4qvLbZ0Z14AaABAg,@Konzor,,1,DUgd48QYmfI,1,0,2021-11-29T09:22:25Z,"Hi Nate,<br><br>I have a question @<a href=""https://www.youtube.com/watch?v=DUgd48QYmfI&amp;t=25m56s"">25:56</a> when you do groupby apply: How do you use groupby().apply(function) if you have multiple input parameters of the function? E.g. if in your example &quot;norm_by_data2&quot; would have 2 inputs (x,y).",2021-11-29T09:22:25Z
Ugx4LbcykE4qvLbZ0Z14AaABAg.9VKCBYd4m0e9VLYReQP87F,@stratascratch,Ugx4LbcykE4qvLbZ0Z14AaABAg,2,DUgd48QYmfI,0,0,2021-11-29T21:56:05Z,"Are you talking about something like this? <a href=""https://stackoverflow.com/questions/43483365/use-pandas-groupby-apply-with-arguments"">https://stackoverflow.com/questions/43483365/use-pandas-groupby-apply-with-arguments</a>",2021-11-29T21:56:05Z
UgzWiRAyoZEx1EQCEvd4AaABAg,@Konzor,,1,DUgd48QYmfI,0,0,2021-11-26T17:30:56Z,Thanks a lot. Really clear.,2021-11-26T17:30:56Z
Ugy5DhpEIkA6VkEgi9F4AaABAg,@wenlizhang5283,,1,DUgd48QYmfI,1,0,2021-06-27T18:59:39Z,Nice video,2021-06-27T18:59:39Z
Ugy5DhpEIkA6VkEgi9F4AaABAg.9P671lIVtsA9P6HSVpE9fe,@stratascratch,Ugy5DhpEIkA6VkEgi9F4AaABAg,2,DUgd48QYmfI,0,0,2021-06-27T20:30:41Z,Thank you. I created this lecture and notebooks for an university course and released the contents for free. So I hope you like it.,2021-06-27T20:30:41Z
UgxUVDviiayi0y3HYnJ4AaABAg,@Bakhiet89,,1,DUgd48QYmfI,1,0,2021-05-13T14:44:59Z,Thank you so much!! I was fighting with groupby and apply!,2021-05-13T14:44:59Z
UgxUVDviiayi0y3HYnJ4AaABAg.9NHn76jqpx39NI-7T8RoJM,@stratascratch,UgxUVDviiayi0y3HYnJ4AaABAg,2,DUgd48QYmfI,0,0,2021-05-13T16:38:37Z,I&#39;m glad you found it useful! Good luck with python.,2021-05-13T16:38:37Z
UgyJIYtvd0wqFhJgN2R4AaABAg,@BHARATHEEYUDU.,,1,DUgd48QYmfI,1,0,2021-04-03T09:15:20Z,How to create html pdf reports after data cleaning to send client please make vedio,2021-04-03T09:15:20Z
UgyJIYtvd0wqFhJgN2R4AaABAg.9LfCadpV8YC9LfzfYrc5ZP,@stratascratch,UgyJIYtvd0wqFhJgN2R4AaABAg,2,DUgd48QYmfI,0,0,2021-04-03T16:32:54Z,"Try this man! <a href=""https://www.youtube.com/watch?v=UmN2_R4KEg8"">https://www.youtube.com/watch?v=UmN2_R4KEg8</a>",2021-04-03T16:32:54Z
UgybsH6tpO_nwLyRTVB4AaABAg,@craftykidsclub7039,,1,DUgd48QYmfI,1,1,2021-01-27T16:38:09Z,the way it is explaning everything is really awesome. Thanks you for nice vedio!!!,2021-01-27T16:40:08Z
UgybsH6tpO_nwLyRTVB4AaABAg.9J12oFYq2Si9J17_xcDs2C,@stratascratch,UgybsH6tpO_nwLyRTVB4AaABAg,2,DUgd48QYmfI,0,0,2021-01-27T17:19:53Z,Thank you! I&#39;m glad you enjoy the pandas tutorial. Definitely a must know if you&#39;re working with data and with python. Take a look at the notebook as well!,2021-01-27T17:19:53Z
UgzR0LatXscrVuKodsV4AaABAg,@manavsaxena5579,,1,DUgd48QYmfI,1,1,2021-01-24T11:36:08Z,"Hi Nate,<br><br>I was practicing &#39;3 Bed Minimum Problem&#39; on the website and although I managed to solve the question in SQL, I am really struggling with the Pandas Solution. Could you please make a video on it or provide the solution? Also, I would really appreciate if in your future videos you could solve the same problem in both SQL and Pandas.",2021-01-24T11:36:08Z
UgzR0LatXscrVuKodsV4AaABAg.9ItmrakiqLm9IvTB3TFBx3,@stratascratch,UgzR0LatXscrVuKodsV4AaABAg,2,DUgd48QYmfI,0,1,2021-01-25T03:13:55Z,"Here&#39;s the python solution to the 3 bed min problem:<br><br>min_beds = airbnb_search_details.groupby([&#39;neighbourhood&#39;]).filter(lambda g: min(g[&#39;beds&#39;]) &gt;= 3).groupby(&#39;neighbourhood&#39;).mean().reset_index()[[&#39;neighbourhood&#39;,&#39;beds&#39;]]<br>result = min_beds.rename(index=str, columns={&quot;beds&quot;: &quot;n_beds_avg&quot;}).sort_values(&#39;n_beds_avg&#39;,ascending=False)<br><br>Hope that helps. I&#39;ll be doing some python videos in the future but not all of the questions will have a python solution, unfortunately. I will try though!",2021-01-25T03:13:55Z
UgwP28ngjuWiKetmuNp4AaABAg,@kirubababu7127,,1,DUgd48QYmfI,0,1,2021-01-12T08:33:09Z,"HI Bro,<br>My requirement is, I have to group by key and key column with column name &#39;key&#39; and data2 column with name data2 and I need sum value of data1.<br>Kindly share your ideas",2021-01-12T08:33:09Z
UgzsVl6DMtb3K9PSAxZ4AaABAg,@SudhirKumar-ry4gk,,1,DUgd48QYmfI,4,0,2020-12-27T22:01:36Z,"Please help as I have data of employees in which they did multiple sale, I want if any employee did sale more the 50000 againt it each emp I&#39;d of that person print excellent rest low.<br>Like<br>Emp I&#39;d.   Sale       status<br>Emp1001 5000.   Excellent<br>Emp1001 45000. Excellent<br>Emp1001 2000.    Excellent<br>Emp1002 5000.    Low<br>Emp1003 2500.    Low",2020-12-27T22:01:36Z
UgzsVl6DMtb3K9PSAxZ4AaABAg.9HmoBA19udg9HmyuzboIAg,@stratascratch,UgzsVl6DMtb3K9PSAxZ4AaABAg,2,DUgd48QYmfI,0,1,2020-12-27T23:35:23Z,I think you&#39;d probably want to do a groupby() employee ID first. Then create a new column (the status column) and add the value (&#39;excellent&#39; or &#39;low) for the status column based on the employee total sale that you were able to calculate from the groupby(). This can be done using an if/else statement. Hope that helps!,2020-12-27T23:35:23Z
UgzsVl6DMtb3K9PSAxZ4AaABAg.9HmoBA19udg9HmzXs8ADw7,@SudhirKumar-ry4gk,UgzsVl6DMtb3K9PSAxZ4AaABAg,2,DUgd48QYmfI,0,0,2020-12-27T23:40:49Z,@@stratascratch can please share the code it will help a lot for me.,2020-12-27T23:40:49Z
UgzsVl6DMtb3K9PSAxZ4AaABAg.9HmoBA19udg9Hn1KyWQVeL,@stratascratch,UgzsVl6DMtb3K9PSAxZ4AaABAg,2,DUgd48QYmfI,0,1,2020-12-28T00:05:17Z,"@@SudhirKumar-ry4gk Something like this might work. Hard to test without the dataset. Refer to this resource (<a href=""https://stackoverflow.com/questions/40603264/pandas-add-a-new-column-in-a-data-frame-based-on-a-value-in-another-data-frame)"">https://stackoverflow.com/questions/40603264/pandas-add-a-new-column-in-a-data-frame-based-on-a-value-in-another-data-frame)</a> for help. Also, you can post on stackoverflow since it&#39;s a website of people helping out others. <br><br>df = employee_table.groupby(&#39;id&#39;).sum().reset_index()<br>df[&#39;status&#39;] = [&#39;excellent&#39; if x &gt; 50000 else &#39;low&#39; for x in df[&#39;sale&#39;]]<br>final_df = pdf.merge(employee_table, df, how = &#39;left&#39;)<br><a href=""http://www.youtube.com/results?search_query=%23then"">#then</a> remove all the rows you don&#39;t need.",2020-12-28T00:05:17Z
UgzsVl6DMtb3K9PSAxZ4AaABAg.9HmoBA19udg9Hn2ScweygY,@SudhirKumar-ry4gk,UgzsVl6DMtb3K9PSAxZ4AaABAg,2,DUgd48QYmfI,0,1,2020-12-28T00:15:04Z,@@stratascratch thanks for your support,2020-12-28T00:15:04Z
Ugw9VYFatU3zRDrnRWp4AaABAg,@immanuelsuleiman7550,,1,DUgd48QYmfI,2,1,2020-10-16T16:03:53Z,That display 3 dataframes function is incredibly useful<br>thanks,2020-10-16T16:03:53Z
Ugw9VYFatU3zRDrnRWp4AaABAg.9Esm04oz0Ma9EvTBwDqW9J,@stratascratch,Ugw9VYFatU3zRDrnRWp4AaABAg,2,DUgd48QYmfI,0,0,2020-10-17T17:08:27Z,Glad you found it useful!,2020-10-17T17:08:27Z
Ugw9VYFatU3zRDrnRWp4AaABAg.9Esm04oz0Ma9EvTg6ACuGf,@immanuelsuleiman7550,Ugw9VYFatU3zRDrnRWp4AaABAg,2,DUgd48QYmfI,0,0,2020-10-17T17:12:43Z,@@stratascratch keep up the good work,2020-10-17T17:12:43Z
Ugzf_czLLOL86qvwiV94AaABAg,@tarast4456,,1,DUgd48QYmfI,1,1,2020-09-17T01:41:34Z,Thank you for this information. The apply method example has helped me with my project,2020-09-17T01:41:34Z
Ugzf_czLLOL86qvwiV94AaABAg.9DgZGtP6gUK9E2r1jwE__V,@stratascratch,Ugzf_czLLOL86qvwiV94AaABAg,2,DUgd48QYmfI,0,0,2020-09-26T02:48:03Z,Wonderful!,2020-09-26T02:48:03Z
UgxThviLVkdpU65snfx4AaABAg,@juliannavas9561,,1,DUgd48QYmfI,1,1,2020-08-29T12:48:11Z,"Very good video, many many thanks!",2020-08-29T12:48:11Z
UgxThviLVkdpU65snfx4AaABAg.9CwpSiUL-tP9E2r3jl1sCO,@stratascratch,UgxThviLVkdpU65snfx4AaABAg,2,DUgd48QYmfI,0,0,2020-09-26T02:48:19Z,Glad you liked it!,2020-09-26T02:48:19Z
UgxodhH3BJGGSLQohBF4AaABAg,@shahidkarim7352,,1,DUgd48QYmfI,0,2,2020-07-29T13:23:38Z,"looking forward sql and other python vid, thanks for the content",2020-07-29T13:23:38Z
UgxX5zZdvdpRkW7_axV4AaABAg,@maximilianodelatorre2513,,1,-i9chjBSOqU,0,0,2022-09-22T13:22:08Z,Thank you so much for this!! very clrearly explained!!!,2022-09-22T13:22:08Z
Ugx720AUePBNgIfld9N4AaABAg,@migi7787,,1,-i9chjBSOqU,1,0,2022-05-18T21:12:31Z,"Excellent stuff, thank you!",2022-05-18T21:12:31Z
Ugx720AUePBNgIfld9N4AaABAg.9bBCaCZVu859kUzeumShQO,@stratascratch,Ugx720AUePBNgIfld9N4AaABAg,2,-i9chjBSOqU,0,0,2023-01-05T06:18:10Z,Glad it was helpful!,2023-01-05T06:18:10Z
UgzUxZ9d00WkMFm8Eg54AaABAg,@yusufbas035,,1,-i9chjBSOqU,0,0,2022-04-06T16:39:00Z,great video,2022-04-06T16:39:00Z
Ugx4lz_AKkVTV3EiFbx4AaABAg,@_PraveenSingh_,,1,-i9chjBSOqU,0,0,2022-02-14T13:27:03Z,"I wish I had found this playlist earlier, didn&#39;t know merge function in my previous interview. They asked me a question which was a combination of merge and groupby.<br>Thanks! for creating an amazing playlist.",2022-02-14T13:27:03Z
UgwiJHrAn6EsV7_tmB94AaABAg,@zahedinima732,,1,-i9chjBSOqU,0,0,2022-01-20T18:54:29Z,This video is a real gem! It gives me an intuitive understanding of the merge method! I hope you&#39;ll create more videos!! üíå,2022-01-20T18:54:29Z
UgwFHDPiOU2uRCHflmN4AaABAg,@skyful9,,1,-i9chjBSOqU,0,0,2022-01-16T14:01:12Z,Perfect.  Thank you,2022-01-16T14:01:12Z
UgwmJg5bc69PtRUO6wR4AaABAg,@fadwa2413,,1,-i9chjBSOqU,0,0,2021-08-24T10:39:48Z,love this channel<br>it is great<br>Thank you so much,2021-08-24T10:39:48Z
UgyB35EURu0qyIug2Dp4AaABAg,@saitejadidigam2971,,1,-i9chjBSOqU,15,2,2021-03-29T21:50:42Z,"Man!!! Your tutorials are so unique, btw your SQL playlist is awesome, totally loved it.",2021-03-29T21:50:42Z
UgyB35EURu0qyIug2Dp4AaABAg.9LUg3kulRTM9LUj9NJhCXJ,@stratascratch,UgyB35EURu0qyIug2Dp4AaABAg,2,-i9chjBSOqU,0,2,2021-03-29T22:17:40Z,I&#39;m glad you like them! I have many more videos coming out related to SQL and python.,2021-03-29T22:17:40Z
UgyB35EURu0qyIug2Dp4AaABAg.9LUg3kulRTM9ZS6XrBwwg0,@calberk1737,UgyB35EURu0qyIug2Dp4AaABAg,2,-i9chjBSOqU,0,0,2022-03-11T21:12:32Z,Ioiiooiiiiiiiiiioii,2022-03-11T21:12:32Z
UgyB35EURu0qyIug2Dp4AaABAg.9LUg3kulRTM9ZS6YyDQ4TE,@calberk1737,UgyB35EURu0qyIug2Dp4AaABAg,2,-i9chjBSOqU,0,0,2022-03-11T21:12:41Z,Iiiiiiiii8ifiiviiiiiii,2022-03-11T21:12:41Z
UgyB35EURu0qyIug2Dp4AaABAg.9LUg3kulRTM9ZS6_Y1HnKE,@calberk1737,UgyB35EURu0qyIug2Dp4AaABAg,2,-i9chjBSOqU,0,0,2022-03-11T21:12:54Z,I8fiiif,2022-03-11T21:12:54Z
UgyB35EURu0qyIug2Dp4AaABAg.9LUg3kulRTM9ZS6cdErVR0,@calberk1737,UgyB35EURu0qyIug2Dp4AaABAg,2,-i9chjBSOqU,0,0,2022-03-11T21:13:19Z,@@stratascratch iiiiiii8iiiiiiiiiiiiiiii,2022-03-11T21:13:19Z
Ugyjz3SSWWnPcC_SCo94AaABAg,@silvioaliatis9252,,1,-i9chjBSOqU,1,0,2021-03-28T16:49:28Z,Nice tutorial!,2021-03-28T16:49:28Z
Ugyjz3SSWWnPcC_SCo94AaABAg.9LRZnbD0jGa9LScfHvK-_b,@stratascratch,Ugyjz3SSWWnPcC_SCo94AaABAg,2,-i9chjBSOqU,0,0,2021-03-29T02:42:31Z,Thanks for checking it out. Hope you check out the entire series.,2021-03-29T02:42:31Z
UgxQYlauFm_opYP-8ax4AaABAg,@ahomosapienforlife5902,,1,HRzthi_FqAA,1,0,2022-07-15T19:03:17Z,clear explanation on merge. Hugely underrated playlist on pandas,2022-07-15T19:03:17Z
UgxQYlauFm_opYP-8ax4AaABAg.9dWJuVONnZg9dWZOZrRKz6,@stratascratch,UgxQYlauFm_opYP-8ax4AaABAg,2,HRzthi_FqAA,0,1,2022-07-15T21:18:36Z,I appreciate it. Glad it helped.,2022-07-15T21:18:36Z
UgxTtmjDT4m7R8W8PJJ4AaABAg,@yusufbas035,,1,HRzthi_FqAA,0,0,2022-04-06T22:54:51Z,Great series. Thank you.,2022-04-06T22:54:51Z
Ugx3xhsNjmHTa-XmTvt4AaABAg,@yuhanyao2857,,1,HRzthi_FqAA,1,0,2021-06-27T07:39:46Z,Great playlist. Went through both the NumPy and the Pandas series. Will you have a matplotlib series coming up?,2021-06-27T07:39:46Z
Ugx3xhsNjmHTa-XmTvt4AaABAg.9P4uEA96Ki69P6HWbVQXV-,@stratascratch,Ugx3xhsNjmHTa-XmTvt4AaABAg,2,HRzthi_FqAA,0,1,2021-06-27T20:31:15Z,Unfortunately no. I switched over to data science interview questions. And then will be diving into other topics like probability and statistics in the near future.,2021-06-27T20:31:15Z
Ugw1VEdkj5IzmIx0_4d4AaABAg,@byoutekinaeiyuu,,1,HRzthi_FqAA,1,0,2021-02-13T14:44:27Z,Watched&amp;liked all the videos in the pandas playlist. Very good stuff. Now I&#39;ll be watching your numpy playlist and when preparing for my job interview probably the other playlists too!,2021-02-13T14:44:27Z
Ugw1VEdkj5IzmIx0_4d4AaABAg.9JhcJLybuSV9JhwjqWOFeO,@stratascratch,Ugw1VEdkj5IzmIx0_4d4AaABAg,2,HRzthi_FqAA,0,0,2021-02-13T17:42:58Z,That&#39;s great. Thanks for watching those videos! Each one of those videos also has a link to a python notebook where you can practice with me while watching. I found working interactively helps learn the material faster.,2021-02-13T17:42:58Z
UgzFj4cLyHe7wRqzn2l4AaABAg,@dania8639,,1,mkbgEvUkSaM,1,0,2022-06-15T01:51:10Z,"Thank you, Great tutorial.",2022-06-15T01:51:10Z
UgzFj4cLyHe7wRqzn2l4AaABAg.9cHDwkBMoAn9dmmFdJmDkp,@stratascratch,UgzFj4cLyHe7wRqzn2l4AaABAg,2,mkbgEvUkSaM,0,0,2022-07-22T13:46:48Z,Glad you enjoyed it!,2022-07-22T13:46:48Z
UgyBtrqwwU3ixQ_OwVV4AaABAg,@serageibraheem2386,,1,mkbgEvUkSaM,2,1,2020-08-18T18:43:31Z,I am here from Python programmer channel,2020-08-18T18:43:31Z
UgyBtrqwwU3ixQ_OwVV4AaABAg.9CW8NjPmRSr9CYGbU8eFlu,@stratascratch,UgyBtrqwwU3ixQ_OwVV4AaABAg,2,mkbgEvUkSaM,0,1,2020-08-19T14:33:55Z,Thanks for visiting!,2020-08-19T14:33:55Z
UgyBtrqwwU3ixQ_OwVV4AaABAg.9CW8NjPmRSr9CYJ2aa5J70,@serageibraheem2386,UgyBtrqwwU3ixQ_OwVV4AaABAg,2,mkbgEvUkSaM,0,0,2020-08-19T14:55:14Z,@@stratascratch I am glad finding ur channel,2020-08-19T14:55:14Z
Ugwo9eBIdfM-2KfUQpt4AaABAg,@raashas.chauhan5295,,1,STJA-D-K9Cs,0,0,2021-12-30T00:53:17Z,Great!,2021-12-30T00:53:17Z
Ugyhk_d0GphBL4pCCzp4AaABAg,@adnanabdullah4351,,1,STJA-D-K9Cs,1,0,2021-06-03T10:55:30Z,very nice,2021-06-03T10:55:30Z
Ugyhk_d0GphBL4pCCzp4AaABAg.9O7SYgoiMuR9O7ueVMCR6P,@stratascratch,Ugyhk_d0GphBL4pCCzp4AaABAg,2,STJA-D-K9Cs,0,1,2021-06-03T15:09:50Z,Thanks for watching!,2021-06-03T15:09:50Z
UgxyGTTwVa0XcGqdUi54AaABAg,@samreenmallick2913,,1,0hjTJw9Jpmc,0,0,2022-02-16T23:49:45Z,"When the video is about to end, recommendation of new videos covers up the screen which blocks the current video.",2022-02-16T23:49:45Z
Ugx5v1H-5eA1AAcA8YB4AaABAg,@lokeshvarshney3921,,1,ceJ4lVXN0T8,1,0,2021-12-10T01:36:06Z,"Hi Nate, Excellent videos! Thank You so much.  I saw you were calculating the mean by two ways &amp; I am confused with one of the thing, np.mean(x) or x.mean() &quot;x is a 1 dimensional array&quot; both give me the same output but when it comes to median, np.median(x) works but for x.median(), it throws an error saying, &quot;&#39;numpy.ndarray&#39; object has no attribute &#39;median&#39;&quot;. Could you please make it clear?",2021-12-10T01:36:06Z
Ugx5v1H-5eA1AAcA8YB4AaABAg.9VkgZmk5mp89VkyN4DlHl-,@stratascratch,Ugx5v1H-5eA1AAcA8YB4AaABAg,2,ceJ4lVXN0T8,0,0,2021-12-10T04:11:39Z,It just means median doesn&#39;t exist as a function.,2021-12-10T04:11:39Z
UgxO5YbrUeRxRTj7yVp4AaABAg,@avinashramesh2736,,1,ceJ4lVXN0T8,1,3,2021-09-03T07:06:30Z,"These are some of the best YouTube videos. Both the Numpy and Pandas series are outstanding. Next month, I intend to subscribe to stratascratch premium. Hopefully, it will assist me in landing a dream job at any FAANG or top-tier firm.",2021-09-03T07:06:30Z
UgxO5YbrUeRxRTj7yVp4AaABAg.9RowU-msdT99Rpug_EqvCR,@stratascratch,UgxO5YbrUeRxRTj7yVp4AaABAg,2,ceJ4lVXN0T8,0,1,2021-09-03T16:10:07Z,Thank you! And good luck on preparing for your interviews. Good luck to you!,2021-09-03T16:10:07Z
UgwPcBoVOsSeF6fv8wR4AaABAg,@rakheegupta9788,,1,ceJ4lVXN0T8,1,3,2021-05-25T11:43:21Z,Why there are so less views!!!<br>Amazing video for explaining a wonderful book: &quot;Data Science HandBook&quot;<br>Please make more videos,2021-05-25T11:43:21Z
UgwPcBoVOsSeF6fv8wR4AaABAg.9NlMsJiUP-l9NlnzfiqJDf,@stratascratch,UgwPcBoVOsSeF6fv8wR4AaABAg,2,ceJ4lVXN0T8,0,3,2021-05-25T15:49:01Z,I know! I wish Youtube would promote this more. I took a long time to make all the notebooks in this series! =),2021-05-25T15:49:01Z
UgyhR1oRQaMcjzJhBfF4AaABAg,@gurkanyesilyurt4461,,1,e6C1Z-o1fyQ,1,1,2021-11-18T10:35:14Z,Thank you for this very usefull explanation.,2021-11-18T10:35:14Z
UgyhR1oRQaMcjzJhBfF4AaABAg.9Ut-mJfVLxl9UtPgzPX3B7,@stratascratch,UgyhR1oRQaMcjzJhBfF4AaABAg,2,e6C1Z-o1fyQ,0,1,2021-11-18T14:21:42Z,Thanks for watching! Appreciate it,2021-11-18T14:21:42Z
UgxvSCMdCE8LVLY4d014AaABAg,@Lex-ep1cp,,1,i80pkW8YeKI,1,0,2022-11-17T09:25:56Z,Really glad I found this channel. Great source so far üéâ,2022-11-17T09:25:56Z
UgxvSCMdCE8LVLY4d014AaABAg.9iX9CBENWmV9kUyshKzdan,@stratascratch,UgxvSCMdCE8LVLY4d014AaABAg,2,i80pkW8YeKI,0,0,2023-01-05T06:11:19Z,Welcome aboard!,2023-01-05T06:11:19Z
Ugyely3kTB9LFQZdo9Z4AaABAg,@ramu000009,,1,Y0hCQsCMIZM,1,0,2023-08-12T19:24:10Z,"thanks for sharing these, Are you using any of these plugins in your Data science projects?",2023-08-12T19:24:10Z
Ugyely3kTB9LFQZdo9Z4AaABAg.9tKIhBJVBRa9tNEzdhhVP5,@stratascratch,Ugyely3kTB9LFQZdo9Z4AaABAg,2,Y0hCQsCMIZM,0,1,2023-08-13T22:49:26Z,"I think it&#39;s good to try a few of the plugins out and see which ones work for you. I would use them to get projects started, like if I need to explore and clean data. Most of these plugins do a good job of doing the painstaking work so that you can focus on building models and tweaking them.",2023-08-13T22:49:26Z
UgxtCZRXIOs9m9Pyg0t4AaABAg,@AnuragSingh-vv3qv,,1,Y0hCQsCMIZM,1,0,2023-07-19T08:30:01Z,Greatü§©,2023-07-19T08:30:01Z
UgxtCZRXIOs9m9Pyg0t4AaABAg.9sLKl73wFAy9s_rRXVgmRW,@stratascratch,UgxtCZRXIOs9m9Pyg0t4AaABAg,2,Y0hCQsCMIZM,0,0,2023-07-25T09:12:54Z,üôå,2023-07-25T09:12:54Z
UgxFltcChmU5EywjVKF4AaABAg,@rewanthnayak2972,,1,Y0hCQsCMIZM,1,0,2023-07-19T07:09:47Z,first,2023-07-19T07:09:47Z
UgxFltcChmU5EywjVKF4AaABAg.9sLB_SWDvMC9sY1IoQe8zZ,@Sandeepkumar-dm2bp,UgxFltcChmU5EywjVKF4AaABAg,2,Y0hCQsCMIZM,0,1,2023-07-24T06:50:07Z,"congratulation, you are a pro, fastest man alive, you are great, once again congratualation",2023-07-24T06:50:07Z
Ugwgjfob31gs8k0hqz54AaABAg,@MiroKrotky,,1,qbN_t6sh4_E,1,1,2022-11-20T14:21:34Z,Very visual video. Memes on point,2022-11-20T14:21:34Z
Ugwgjfob31gs8k0hqz54AaABAg.9iePQ394u0Y9ih6-ca0T53,@stratascratch,Ugwgjfob31gs8k0hqz54AaABAg,2,qbN_t6sh4_E,0,0,2022-11-21T15:29:39Z,Thank you.  We appreciate your feedback.,2022-11-21T15:29:39Z
Ugy9vKaqf4Z9Zx6_lax4AaABAg,@danjuri3600,,1,qbN_t6sh4_E,0,1,2022-11-11T03:17:40Z,Business Analysts punching the air right now,2022-11-11T03:17:40Z
Ugw7aYTYHUdEjHvWBGJ4AaABAg,@welcometomathy6944,,1,4uVxxj07cRE,1,0,2023-08-30T14:02:31Z,"As a Data analyst, I baraly use SQL I main work with Power BI all day, everything my pears doing on tableu and python Im doing on power bi and excel easy, about the NoSQL I think it just exists as a movement to go against SQL monopoly, if you company only start up use it because is cool is different and we dont wanna spend the money for SQL servers and infrastructure + management same for companies running on python and R for data visualisation and some what automation or things that could be done better with mix of MS Power and now Frabric tools, it just looks cools and is pushed by only courcers and ifluencers, I didn&#39;t learn python in uni. I did c++ visual basic for desktop apps, java, java script for mobile apps PHP and SQL like crazy. python was something introduced later on the 3/5th year is u do masters 10 years ago, and is not changing because is not  cool anymore.",2023-08-30T14:02:31Z
Ugw7aYTYHUdEjHvWBGJ4AaABAg.9u24C9xYnfx9w19J-q7EaM,@TheElementFive,Ugw7aYTYHUdEjHvWBGJ4AaABAg,2,4uVxxj07cRE,0,0,2023-10-18T22:30:41Z,Turns out you can put words into any order you want,2023-10-18T22:30:41Z
Ugx6WupwA6NbC7AMaQx4AaABAg,@user-fn5rx3gp2t,,1,4uVxxj07cRE,1,0,2023-08-30T09:36:58Z,on scale from  to 100 <br>can i land a data analysis job with zero domain knowledge and only using pandas and the other visualization tools,2023-08-30T09:36:58Z
Ugx6WupwA6NbC7AMaQx4AaABAg.9u1aoH6fgKj9u2H2QPUnmK,@stratascratch,Ugx6WupwA6NbC7AMaQx4AaABAg,2,4uVxxj07cRE,0,1,2023-08-30T15:54:46Z,Yes you can! so 100? The primarily skill set for an analyst job is to be able to explore and manipulate data. You can do that with using only pandas if you want.,2023-08-30T15:54:46Z
UgyanOjaqAxEfd2ViKV4AaABAg,@dantherman4202,,1,4uVxxj07cRE,1,0,2023-08-30T06:47:12Z,"Thanks for the video! I have been a paid member of your platform for some months now and have worked through all of the SQL and Python questions, I think I am quite fluent with querying at this point. What would you say is the next step for me if I am trying to become a data scientist? I should mention I am already employed as a mid-tier data analyst (responsibilities include wrangling, insights, presentations; Excel/MySQL/PowerBI; relatively small databases). I am just looking to level up my career, but I am having a difficult time getting a grip on what to learn next. Currently I&#39;m just grinding through visualizations (powerbi, tableau, matplotlib) which is getting boring. But I&#39;m guessing I should be focusing on Python if I want to do Data Science, and probably the more DBA side of SQL then, I&#39;m assuming? Any good online courses you would recommend ?",2023-08-30T06:47:12Z
UgyanOjaqAxEfd2ViKV4AaABAg.9u1INonkevM9u2GvUNEn3N,@stratascratch,UgyanOjaqAxEfd2ViKV4AaABAg,2,4uVxxj07cRE,0,0,2023-08-30T15:53:41Z,I think the next step should be to either do some data projects or build automations into your workflow with python. It depends on what type of data scientists you want to be. Some primarily build ML models while others perform more advanced data analyst type of work. Ask yourself what you want to do and choose projects and learn technologies in that direction.,2023-08-30T15:53:41Z
UgyDeSrf5r6OwBE6pEx4AaABAg,@wartortle55,,1,4uVxxj07cRE,0,1,2023-08-29T17:21:48Z,"Short, sweet, and encouraging.",2023-08-29T17:21:48Z
Ugx4VIq7vTjcx1b47Rl4AaABAg,@fouzias188,,1,4uVxxj07cRE,0,0,2023-08-29T16:42:16Z,Nice ! üëå,2023-08-29T16:42:16Z
UgwLap8NCvJRnaKCaqp4AaABAg,@buak809,,1,0rSuMJB6IE4,1,0,2023-08-06T17:35:07Z,"What should I have in portfolio as future data scientist or advanced data analyst? With use of Python, SQL, etc?",2023-08-06T17:35:07Z
UgwLap8NCvJRnaKCaqp4AaABAg.9t4eSFT7Z0D9t7P7ldfdER,@stratascratch,UgwLap8NCvJRnaKCaqp4AaABAg,2,0rSuMJB6IE4,0,0,2023-08-07T19:10:13Z,Definitely more python than SQL. And some modeling and ML types of projects if you&#39;re trying to be a data scientist.,2023-08-07T19:10:13Z
UgwPH9DREjuC_cBR9dB4AaABAg,@minimin-wj8vp,,1,0rSuMJB6IE4,0,0,2023-08-03T16:38:23Z,Thank youüåπThe video was simple and very useful. Keep up the great work!,2023-08-03T16:38:23Z
Ugzl_Syx9RZDMQ9A16h4AaABAg,@andybecker5001,,1,0rSuMJB6IE4,0,0,2023-07-27T10:54:07Z,I really like using plotly with seaborn style. Sending nice looking interactive graph in html file is helpful for end users,2023-07-27T10:54:07Z
Ugyk_NLUXjTc_tDGGWZ4AaABAg,@matattz,,1,0rSuMJB6IE4,1,0,2023-07-27T10:37:59Z,"Any recommendations to get better at Story Telling as a Data Scientist? Books, Coursera, Youtube channels... Thank you guys",2023-07-27T10:37:59Z
Ugyk_NLUXjTc_tDGGWZ4AaABAg.9sf9lNaxux-9sgPS_Ws-wi,@stratascratch,Ugyk_NLUXjTc_tDGGWZ4AaABAg,2,0rSuMJB6IE4,0,1,2023-07-27T22:14:20Z,"I&#39;ve never read any books on story telling. But I used to and still do read a lot of articles on tech/data science publications like hacker news, TDS, KDnuggets. They always have a few articles on the softer skills. Also business articles on how to give effective presentations will teach you how to present in front of an audience.",2023-07-27T22:14:20Z
UgzYlYsS92NAwTjnuox4AaABAg,@yogenderyadav4550,,1,0rSuMJB6IE4,1,0,2023-07-26T17:09:27Z,love your content bro !! &lt;&lt;3,2023-07-26T17:09:27Z
UgzYlYsS92NAwTjnuox4AaABAg.9sdHlaqOFUZ9stvp3zzc-s,@stratascratch,UgzYlYsS92NAwTjnuox4AaABAg,2,0rSuMJB6IE4,0,0,2023-08-02T04:16:01Z,Appreciate it!,2023-08-02T04:16:01Z
Ugziflw2ZbE84QtJmrB4AaABAg,@adityasahoo2949,,1,0rSuMJB6IE4,1,2,2023-07-26T16:05:25Z,I must say your videos are very useful for many.Can you suggest any books for studying data science from the beginning. It would be very useful.,2023-07-26T16:05:25Z
Ugziflw2ZbE84QtJmrB4AaABAg.9sdARiF91JD9sgPknmvRt6,@stratascratch,Ugziflw2ZbE84QtJmrB4AaABAg,2,0rSuMJB6IE4,0,1,2023-07-27T22:16:57Z,"I don&#39;t think there is a single book on data science from start to finish. But I like to check out sites that specialize in a particular topic of data science. Like Mode Analytics for python and sql, <a href=""http://brilliant.org/"">brilliant.org</a> for stats, kaggle for ML, and of course stratascratch for a lot of practice exercises. All these sites keep content updated so I think they&#39;re better than books =)",2023-07-27T22:16:57Z
Ugx4-CG_Vescyh2AGlZ4AaABAg,@Jules-69lol,,1,pqeUTfNFBCU,0,0,2023-12-01T15:54:13Z,Thanks!,2023-12-01T15:54:13Z
UgxLkCJa-y0hCBdNtal4AaABAg,@sowmya6471,,1,pqeUTfNFBCU,1,0,2023-10-12T05:51:25Z,Hi Can I please get this shorts full video link for data cleaning in python,2023-10-12T05:52:19Z
UgxLkCJa-y0hCBdNtal4AaABAg.9vkvAtAxPci9vlmHe6celv,@stratascratch,UgxLkCJa-y0hCBdNtal4AaABAg,2,pqeUTfNFBCU,0,0,2023-10-12T13:52:56Z,"Hi! There you go <a href=""https://www.youtube.com/watch?v=xlppLbv7nb0"">https://www.youtube.com/watch?v=xlppLbv7nb0</a>",2023-10-12T13:52:56Z
UgwI1TTHROzSXhgLYi14AaABAg,@jianx7807,,1,pqeUTfNFBCU,0,3,2023-04-18T14:26:35Z,Numpy?,2023-04-18T14:26:35Z
UgynJxq8ONHM20qjCmd4AaABAg,@metinunlu_,,1,cEannyn5exA,0,3,2023-11-21T20:40:40Z,"Thank you for the video, it was an informative watch ^^. Why have you excluded Power bi at enterprise tho lol.",2023-11-21T20:40:40Z
UgzIe1HU9B5EuKoRMzp4AaABAg,@chanceelwell2845,,1,gtHI672Tlbw,1,1,2022-09-08T17:02:48Z,Happy to see you started making shorts! As I begin ramping up my job search these will be invaluable!,2022-09-08T17:02:48Z
UgzIe1HU9B5EuKoRMzp4AaABAg.9fiiqBY2knI9fk-AoMEYfR,@stratascratch,UgzIe1HU9B5EuKoRMzp4AaABAg,2,gtHI672Tlbw,0,0,2022-09-09T04:53:33Z,Thank you.  We have been receiving good feedback on our shorts.  We&#39;ll keep making them.  Glad you like it!,2022-09-09T04:53:33Z
Ugz6G2NgSqltWQQiyjt4AaABAg,@kingking5865,,1,mqHQwF6_r1E,1,0,2023-10-11T20:51:25Z,Such a comprehensive explanation. Thank you so much.,2023-10-11T20:51:25Z
Ugz6G2NgSqltWQQiyjt4AaABAg.9vjxNoGXbz89vyN_chjzt2,@stratascratch,Ugz6G2NgSqltWQQiyjt4AaABAg,2,mqHQwF6_r1E,0,0,2023-10-17T11:18:27Z,Glad it was helpful!,2023-10-17T11:18:27Z
UgyjJGsj5SGSi69W0eN4AaABAg,@sahil5124,,1,mqHQwF6_r1E,0,1,2023-09-19T05:39:22Z,"For me, a data science project consists of <br>Data Collection --&gt; Data Cleaning --&gt; EDA --&gt; Feature Engineering --&gt; Model Training and Evaluation --&gt; Deployment",2023-09-19T05:39:22Z
UgwBjL0IJhlg3p_VK5F4AaABAg,@arpitakar3384,,1,mqHQwF6_r1E,0,0,2023-09-13T16:08:43Z,"<a href=""about:invalid#zCSafez""></a> genuine  effort prasings",2023-09-13T16:08:43Z
UgwwAp1wSbw9jJPCIvh4AaABAg,@kamesejordan9550,,1,pBC6DJRBzSw,1,0,2023-09-25T06:27:19Z,this has been helpfully informative... although  need a little help with specifics on implementation ... other videos sound like gibberish ... I&#39;m working with a dataset that has 1k features üò¢ how can I do this without having to list them?,2023-09-25T06:27:19Z
UgwwAp1wSbw9jJPCIvh4AaABAg.9v4ClsXJN8I9v62NNau2sN,@stratascratch,UgwwAp1wSbw9jJPCIvh4AaABAg,2,pBC6DJRBzSw,0,0,2023-09-25T23:34:55Z,Figure out an approach to reduce your feature set. And don&#39;t list them but handle them programmatically. That would be my advice.,2023-09-25T23:34:55Z
UgxpoZsVXWusnWQBV3h4AaABAg,@alphar85,,1,pBC6DJRBzSw,0,0,2023-07-13T06:48:48Z,You are a star.,2023-07-13T06:48:48Z
UgzCTuHNhIna3Vo8D5V4AaABAg,@rizdiramadhan1641,,1,rDteKsifPow,0,0,2023-07-25T21:22:04Z,Tried to find the source that provides details about this topic but could not be able to. May you share the link? Thank you,2023-07-25T21:22:04Z
UgzcTFsEWEwtHDAZfIB4AaABAg,@rokketmerah,,1,rDteKsifPow,1,0,2023-07-01T08:10:05Z,"hi nate, I am considering subscribing to your course. could you please let me know if it includes portfolio projects that I can showcase to recruiters?",2023-07-01T08:10:05Z
UgzcTFsEWEwtHDAZfIB4AaABAg.9rbxAOsDpYY9rd-oWlNs16,@stratascratch,UgzcTFsEWEwtHDAZfIB4AaABAg,2,rDteKsifPow,0,0,2023-07-01T18:01:09Z,"Hey, that&#39;s great! Yes we have 50+ data projects and many of them have solutions. <a href=""https://platform.stratascratch.com/data-projects"">https://platform.stratascratch.com/data-projects</a>",2023-07-01T18:01:09Z
UgxSS6DH7WO-B7BPlEB4AaABAg,@saidjomaa9617,,1,rDteKsifPow,0,0,2023-06-30T14:30:27Z,"Thanks for this, very insightful",2023-06-30T14:30:27Z
Ugxp169AoJEDprcxGLt4AaABAg,@jasper7411,,1,EDArFuFl0tU,0,0,2023-08-20T20:22:25Z,&quot;promo sm&quot; üòî,2023-08-20T20:22:25Z
UgwjQmJbMpDWxdSOC7d4AaABAg,@mitchconnor7066,,1,EDArFuFl0tU,1,1,2023-08-10T21:57:35Z,"kNN is classification algorithm, not clustering.",2023-08-10T21:58:29Z
UgwjQmJbMpDWxdSOC7d4AaABAg.9tFQf6LY0Z39tFRqWyoTIw,@stratascratch,UgwjQmJbMpDWxdSOC7d4AaABAg,2,EDArFuFl0tU,0,0,2023-08-10T22:07:52Z,"Yes, thanks for catching. It should have been labeled as classification. I should have mentioned that K-means is clustering....",2023-08-10T22:07:52Z
UgzKIZWJaj6YWMBW09V4AaABAg,@hackwithharsha,,1,0EoaJE3ePcE,1,0,2022-09-12T10:54:25Z,Does this remove duplicates while aggregating nominees ?,2022-09-12T10:54:25Z
UgzKIZWJaj6YWMBW09V4AaABAg.9fsMr_9dIaW9pRPiUk2Q-E,@DarkOceanShark,UgzKIZWJaj6YWMBW09V4AaABAg,2,0EoaJE3ePcE,0,2,2023-05-08T03:34:36Z,No it doesn&#39;t and it shouldn&#39;t. You can get that by selecting the distinct rows and then aggregating,2023-05-08T03:34:36Z
Ugwf2b5ZhiWfZB5XZDR4AaABAg,@guptariya43,,1,0EoaJE3ePcE,1,1,2022-09-03T06:17:29Z,Whats equivalent in Postgres ?,2022-09-03T06:17:29Z
Ugwf2b5ZhiWfZB5XZDR4AaABAg.9fVh0HS9Mmq9ugTrd1JmBo,@muhammadtalhaumair9916,Ugwf2b5ZhiWfZB5XZDR4AaABAg,2,0EoaJE3ePcE,0,0,2023-09-15T15:55:37Z,Cdjvcjnvcim cj.,2023-09-15T15:55:37Z
UgxNUiwJTrj0YkBdAxJ4AaABAg,@genghis360,,1,0EoaJE3ePcE,1,1,2022-08-28T08:22:52Z,Great way to learn in shorts. Please keep them coming. Thank you üòä,2022-08-28T08:22:52Z
UgxNUiwJTrj0YkBdAxJ4AaABAg.9fGTaMgUh_o9fHeUDTzKha,@stratascratch,UgxNUiwJTrj0YkBdAxJ4AaABAg,2,0EoaJE3ePcE,0,0,2022-08-28T19:25:59Z,You&#39;re welcome.  Sure thing.  Subscribe to our channel to get updates on our videos.,2022-08-28T19:25:59Z
UgzL6YERzVRIZCMDqJN4AaABAg,@caiyu538,,1,vuJi4lF52YU,2,0,2023-07-06T02:54:02Z,Is SHAP algorithm an exhaustive search feature selection method?,2023-07-06T02:54:02Z
UgzL6YERzVRIZCMDqJN4AaABAg.9roFywum8iF9rqGqfLjS54,@stratascratch,UgzL6YERzVRIZCMDqJN4AaABAg,2,vuJi4lF52YU,0,0,2023-07-06T21:40:07Z,Yeah it can be considered as an exhaustive search method.,2023-07-06T21:40:07Z
UgzL6YERzVRIZCMDqJN4AaABAg.9roFywum8iF9rqUjODGIU4,@caiyu538,UgzL6YERzVRIZCMDqJN4AaABAg,2,vuJi4lF52YU,0,1,2023-07-06T23:41:28Z,@@stratascratch thank you so much.,2023-07-06T23:41:28Z
Ugw40NF3A6vLYPkof9F4AaABAg,@khizarstudy2095,,1,o3Yau5MJDZ4,0,0,2023-12-06T17:30:06Z,Scrum meetings ü§£üòÇ,2023-12-06T17:30:06Z
Ugy3lA0_YWB01GHXCmF4AaABAg,@klvncj,,1,3FRoEyC0hdI,0,0,2023-08-18T09:14:45Z,Ngl your thumbnail looked like that of fireship üòÇ,2023-08-18T09:14:45Z
UgypN-Hr9CkAhcO9Ysp4AaABAg,@neerajkhurrana283,,1,tT7STcGCvUY,0,0,2023-11-08T14:45:51Z,"Nice, thanks for the video üòä",2023-11-08T14:45:51Z
UgydbiAY8D__pZtOP9t4AaABAg,@iklintsov,,1,tT7STcGCvUY,0,7,2023-11-08T09:58:20Z,Interesting but lacks examples.,2023-11-08T09:58:20Z
UgzgjCwHzuEGug5ruSN4AaABAg,@austinocampo2410,,1,GxR66KdqBFU,0,0,2023-08-20T20:55:33Z,Incomplete,2023-08-20T20:55:33Z
UgwkwFdl7G5cll7FWXF4AaABAg,@DataSet,,1,GxR66KdqBFU,0,0,2023-07-16T06:36:07Z,Wow,2023-07-16T06:36:07Z
UgyxAj4OmGOTyFnj-HJ4AaABAg,@azrflourish9032,,1,LL7dnUymOvo,0,0,2021-07-12T13:01:15Z,quite informative! easy explanation! thanks!!,2021-07-12T13:01:15Z
UgxwzYAXWoAJMdz0f-N4AaABAg,@rjpdev2431,,1,LL7dnUymOvo,1,0,2021-02-15T22:54:31Z,"Very helpful, thank you",2021-02-15T22:54:31Z
UgxwzYAXWoAJMdz0f-N4AaABAg.9JndzQAvyFt9JnhCG5aZI-,@stratascratch,UgxwzYAXWoAJMdz0f-N4AaABAg,2,LL7dnUymOvo,0,0,2021-02-15T23:22:37Z,Someone actually watched my Numpy videos! haha thanks so much for watching. They don&#39;t get enough love.,2021-02-15T23:22:37Z
UgzyGmuGbrQKPii-h5Z4AaABAg,@JacquesHenriqueDias,,1,wqxDfVdZ8wM,0,0,2023-03-11T01:15:27Z,"Thx man, great video, it helped me a lot!",2023-03-11T01:15:27Z
Ugz1WfXrER-qARzz3PV4AaABAg,@pujabehani3008,,1,wqxDfVdZ8wM,0,0,2022-02-04T06:38:56Z,"Hi ,<br>Can you please  recommend good books for practicing and understanding the concepts related to SQL qnd Python?",2022-02-04T06:38:56Z
Ugzk2QZfFeP6aeK5RDp4AaABAg,@michelchaghoury870,,1,wqxDfVdZ8wM,2,0,2022-01-03T14:37:44Z,"please keep going this channel is amazing, we need more and more vids",2022-01-03T14:37:44Z
Ugzk2QZfFeP6aeK5RDp4AaABAg.9Wjt5b_CSf49WkDnNXgt2U,@stratascratch,Ugzk2QZfFeP6aeK5RDp4AaABAg,2,wqxDfVdZ8wM,0,1,2022-01-03T17:47:20Z,More vids to come! I&#39;m going to be leveraging my team to help pump out more videos each month. So you&#39;ll see less of me but more content in general!,2022-01-03T17:47:20Z
Ugzk2QZfFeP6aeK5RDp4AaABAg.9Wjt5b_CSf49WkEt65u6IH,@michelchaghoury870,Ugzk2QZfFeP6aeK5RDp4AaABAg,2,wqxDfVdZ8wM,0,0,2022-01-03T17:56:52Z,"@@stratascratch pleasee keep going, I just finished your portfolio project series and I learned a lotttttt, thank you, very exited for more and more content",2022-01-03T17:56:52Z
Ugwl2YfEU1MD11V5zIl4AaABAg,@YUVRAJSINGH-iz9gt,,1,wqxDfVdZ8wM,1,0,2021-06-19T13:06:34Z,Your platform is really superb... I would really want to buy premium but price is little high if it would be less... It would be smooth for me to buy and continue learning.,2021-06-19T13:06:34Z
Ugwl2YfEU1MD11V5zIl4AaABAg.9OltGibO4ss9Omb3aZ3OnK,@stratascratch,Ugwl2YfEU1MD11V5zIl4AaABAg,2,wqxDfVdZ8wM,0,2,2021-06-19T19:46:44Z,Probably the steepest discounts I give is a 30% off one for parity pricing for those outside of the US and in other countries where the currency is not as strong as the US dollar. The discount code is PPP30.,2021-06-19T19:46:44Z
UgzGwaawgWY2_ELUtTh4AaABAg,@byoutekinaeiyuu,,1,wqxDfVdZ8wM,2,1,2021-03-12T23:08:16Z,"You are awesome. :) Next month I should be able to finally watch your SQL playlist, looking forward to that :D",2021-03-12T23:08:16Z
UgzGwaawgWY2_ELUtTh4AaABAg.9Ko2QzR9h5E9Ko3THdYKcO,@stratascratch,UgzGwaawgWY2_ELUtTh4AaABAg,2,wqxDfVdZ8wM,0,1,2021-03-12T23:17:19Z,Great to hear! It&#39;ll be a binge watching type of day =),2021-03-12T23:17:19Z
UgzGwaawgWY2_ELUtTh4AaABAg.9Ko2QzR9h5E9Ko51OTguLE,@byoutekinaeiyuu,UgzGwaawgWY2_ELUtTh4AaABAg,2,wqxDfVdZ8wM,0,1,2021-03-12T23:30:59Z,@@stratascratch Probably more then a day as I will be doing those problems myself first and then watching your solutions.,2021-03-12T23:30:59Z
UgxK5bwi0NRJHK4yMSl4AaABAg,@privateaccount5917,,1,wqxDfVdZ8wM,4,0,2021-02-04T00:18:58Z,"Wow, great help for my journey üòä",2021-02-04T00:18:58Z
UgxK5bwi0NRJHK4yMSl4AaABAg.9JJu6X6wmY49JKPVimBl1m,@stratascratch,UgxK5bwi0NRJHK4yMSl4AaABAg,2,wqxDfVdZ8wM,0,1,2021-02-04T05:02:02Z,"Thanks for watching. If there are any topics you&#39;d for me to cover, please let me know!",2021-02-04T05:02:02Z
UgxK5bwi0NRJHK4yMSl4AaABAg.9JJu6X6wmY49JMCxjz8P4T,@privateaccount5917,UgxK5bwi0NRJHK4yMSl4AaABAg,2,wqxDfVdZ8wM,0,0,2021-02-04T21:50:53Z,‚Äã@@stratascratch It is really great i am thinking to upgrade my membership in website. I am master in computer science but i really don&#39;t have codding and working  experience in USA. I am just following all your advice.:D,2021-02-04T21:50:53Z
UgxK5bwi0NRJHK4yMSl4AaABAg.9JJu6X6wmY49JMGX2CjOpt,@stratascratch,UgxK5bwi0NRJHK4yMSl4AaABAg,2,wqxDfVdZ8wM,0,1,2021-02-04T22:22:03Z,"@@privateaccount5917 That&#39;s great! The platform contains questions you&#39;d find on interviews at various companies. I try to add about 20+ new questions every 2-3 months depending how many new interview questions I can collect. New ones will come out early March. If you just did those questions, you&#39;ll have relevant coding experience for industry. I hope you do upgrade to practice more. But I&#39;m also glad my videos are helpful to you.",2021-02-04T22:22:03Z
UgxK5bwi0NRJHK4yMSl4AaABAg.9JJu6X6wmY49JMJA84Q2pt,@privateaccount5917,UgxK5bwi0NRJHK4yMSl4AaABAg,2,wqxDfVdZ8wM,0,0,2021-02-04T22:45:08Z,"@@stratascratch Thank you so much. You are great, keep spreading your knowledge and giving opportunity to people to get their dream job or skill. All the best I may hang around a lot :D",2021-02-04T22:45:08Z
UgzDKvAwhzR0132kUjB4AaABAg,@anuragsingh4766,,1,wqxDfVdZ8wM,1,2,2021-01-07T18:11:00Z,Your blogs are incredible.,2021-01-07T18:11:00Z
UgzDKvAwhzR0132kUjB4AaABAg.9IDiXlh8WIK9IDon8Ar8kC,@stratascratch,UgzDKvAwhzR0132kUjB4AaABAg,2,wqxDfVdZ8wM,0,0,2021-01-07T19:05:40Z,Thanks so much! I try to post at least 1-2x articles a week so stay tuned for more updates.,2021-01-07T19:05:40Z
UgxT0RoksbgTQSBAla94AaABAg,@dominic2446,,1,wqxDfVdZ8wM,1,0,2020-11-30T03:38:56Z,"i dont think freecodecamp should be lecture-based, it should be more interactive, self-guided.",2020-11-30T03:38:56Z
UgxT0RoksbgTQSBAla94AaABAg.9GfJXHFJKi89GfRevAhGyT,@stratascratch,UgxT0RoksbgTQSBAla94AaABAg,2,wqxDfVdZ8wM,0,0,2020-11-30T04:50:01Z,Thanks for your input. I guess I never really thought it was as interactive as the other platforms. But I do see your point.,2020-11-30T04:50:01Z
UgyOFsKKyge-Mq7aneB4AaABAg,@rushikeshgandhmal7304,,1,wqxDfVdZ8wM,2,0,2020-08-18T18:24:39Z,"All videos are great, thank you for great information.<br><br>Help me out here, <br>I&#39;m gonna complete my Degree (CSE dept.)<br>I want to do my carrier in Data Science field.<br><br>I have pretty much of knowledge of data science but not so exactly,<br><br>What steps should I take now and after completing my degree ? <br><br>Should I do Post Graduation in Data Science or learn by myself ? What is best for me ? Please guide me",2020-08-18T18:24:39Z
UgyOFsKKyge-Mq7aneB4AaABAg.9CW6DVaKAnx9CXFBAKLl_S,@rushikeshgandhmal7304,UgyOFsKKyge-Mq7aneB4AaABAg,2,wqxDfVdZ8wM,0,0,2020-08-19T05:02:12Z,Thank you for your valuable time.<br>This will help me :),2020-08-19T05:02:12Z
UgyOFsKKyge-Mq7aneB4AaABAg.9CW6DVaKAnx9U2fTDcSxe4,@jayasankarmouryan3508,UgyOFsKKyge-Mq7aneB4AaABAg,2,wqxDfVdZ8wM,0,0,2021-10-28T17:29:15Z,@@rushikeshgandhmal7304 can you guide me too ..i am also in the same position now ...about to complete my degree,2021-10-28T17:29:15Z
UgwR71CTlR7kaM9yyrl4AaABAg,@algyngom,,1,wqxDfVdZ8wM,0,1,2020-08-17T19:58:48Z,Very instructive video. Thank you !,2020-08-17T19:58:48Z
UgxCsFeQt0NG3OXrKw14AaABAg,@adnanezertiti7725,,1,60NaEc7NP_U,2,0,2021-11-09T21:42:06Z,Can you please estimate how long does it take (following this road map) for someone with an engineer Background?,2021-11-09T21:42:06Z
UgxCsFeQt0NG3OXrKw14AaABAg.9UY0weLFweS9UYF73bEoXV,@stratascratch,UgxCsFeQt0NG3OXrKw14AaABAg,2,60NaEc7NP_U,0,1,2021-11-09T23:45:59Z,"Probably just a few years. Usually people will go through a master program in DS or get their first job out of BS as a data analyst. Just a few years of experience and you&#39;d be able to get a job as a DS. My definition of DS is one that creates ML models, btw. There are a lot of definitions of DS out there so I hope you don&#39;t get confused!",2021-11-09T23:45:59Z
UgxCsFeQt0NG3OXrKw14AaABAg.9UY0weLFweS9UbT_PTiVd5,@adnanezertiti7725,UgxCsFeQt0NG3OXrKw14AaABAg,2,60NaEc7NP_U,0,0,2021-11-11T15:09:17Z,Got it ! thank you for your quick feedback,2021-11-11T15:09:17Z
UgxHnmBnh4JL19ivZo94AaABAg,@adnanezertiti7725,,1,60NaEc7NP_U,0,0,2021-11-09T21:28:16Z,Thank you man ! Nice Road Map ! You helped a lot. Now let&#39;s go into it ...,2021-11-09T21:28:16Z
UgzT8kzZnqRgm8-6_IV4AaABAg,@ankit7918,,1,60NaEc7NP_U,0,2,2020-09-13T19:39:10Z,You are great üôè,2020-09-13T19:39:10Z
Ugz46rwaiO7G9_uTw3V4AaABAg,@diaconescutiberiu7535,,1,sF6gngs9myY,0,2,2023-04-08T12:14:26Z,This is wonderful content. Please consider doing a similar video for data engineers,2023-04-08T12:14:26Z
UgyWhfoCQSICAl3lQVh4AaABAg,@suryachellam2098,,1,sF6gngs9myY,1,0,2023-04-07T07:55:30Z,can we use anaconda for data modelling?,2023-04-07T07:55:30Z
UgyWhfoCQSICAl3lQVh4AaABAg.9oC2wAle2ne9oKcUSDmV4C,@stratascratch,UgyWhfoCQSICAl3lQVh4AaABAg,2,sF6gngs9myY,0,1,2023-04-10T15:48:48Z,Yes of course!,2023-04-10T15:48:48Z
UgxI-skZ-o0w4IL1J3h4AaABAg,@MrKeyur077,,1,sF6gngs9myY,0,1,2023-04-07T04:30:01Z,"A nice, compact &amp; to the point video with correct memes üëåüèª",2023-04-07T04:30:01Z
UgzWnO9aX2-bhy_6pHJ4AaABAg,@mitchconnor7066,,1,sF6gngs9myY,1,0,2023-04-05T23:15:08Z,You said pythonanywhere is IaaS and after that you said that it is PaaS? I am confused. Can it be both?,2023-04-05T23:15:08Z
UgzWnO9aX2-bhy_6pHJ4AaABAg.9o8Ya-MMC6B9oKfPgACXL1,@stratascratch,UgzWnO9aX2-bhy_6pHJ4AaABAg,2,sF6gngs9myY,0,0,2023-04-10T16:14:22Z,Sorry for the confusion. It can be both a place to host python applications and a part of the python application itself. It&#39;s mainly a IaaS though.,2023-04-10T16:14:22Z
Ugzmm80u-WLfjqrBGil4AaABAg,@BillyT83,,1,sF6gngs9myY,1,0,2023-04-05T18:38:23Z,Maybe the best video I have seen so far on YouTube for people who aspire to work in Data Analytics!,2023-04-05T18:38:23Z
Ugzmm80u-WLfjqrBGil4AaABAg.9o82v0--RSC9o8uYU2bBEE,@stratascratch,Ugzmm80u-WLfjqrBGil4AaABAg,2,sF6gngs9myY,0,0,2023-04-06T02:35:45Z,Thanks! Hope to make more of these types of videos in the future!,2023-04-06T02:35:45Z
UgxBPl1rsw-tRMQBXJF4AaABAg,@arpitakar3384,,1,sF6gngs9myY,1,1,2023-04-05T17:12:01Z,make your course with course  parity as per indian sub-continent it&#39;s similar to yearly earning of 50% indians,2023-04-05T17:12:01Z
UgxBPl1rsw-tRMQBXJF4AaABAg.9o7u1XYTYg09oAeJlfeNW5,@arjun6070,UgxBPl1rsw-tRMQBXJF4AaABAg,2,sF6gngs9myY,0,0,2023-04-06T18:52:25Z,"Yes, that&#39;s true ..",2023-04-06T18:52:25Z
UgyJCSkv9BqGqaKr5d54AaABAg,@joel-tw7hk,,1,sF6gngs9myY,0,0,2023-04-05T17:07:40Z,Want a similar video for R too..,2023-04-05T17:07:40Z
UgzO7Dy7eMVFxbkjB8F4AaABAg,@joel-tw7hk,,1,sF6gngs9myY,0,0,2023-04-05T17:04:48Z,Amazing content..,2023-04-05T17:04:48Z
UgziC03wBXwu39gcGAh4AaABAg,@Vintagetube310,,1,0cYfw5mWJ-E,0,1,2022-10-02T06:51:04Z,Do you have the analysis video?,2022-10-02T06:51:04Z
Ugwmg1o1W_qJkPBtxYJ4AaABAg,@Sunny_Tang,,1,0cYfw5mWJ-E,1,0,2022-09-29T14:50:59Z,can you please stop uploading in mobile format?,2022-09-29T14:50:59Z
Ugwmg1o1W_qJkPBtxYJ4AaABAg.9gZZS3p6y1A9g_07A7Hk7K,@stratascratch,Ugwmg1o1W_qJkPBtxYJ4AaABAg,2,0cYfw5mWJ-E,0,0,2022-09-29T19:01:32Z,These are shorts. They are only supposed to be &lt;1min long and they only show up in mobile format. They&#39;re different than the regular YT videos.,2022-09-29T19:01:32Z
UgzdaD71Cr3WJuJYnEx4AaABAg,@monkeydluffy1156,,1,0cYfw5mWJ-E,1,1,2022-09-29T03:20:40Z,Can you please provide the dataset.,2022-09-29T03:20:40Z
UgzdaD71Cr3WJuJYnEx4AaABAg.9gYKS5-6q7L9g_07Yl0sjw,@stratascratch,UgzdaD71Cr3WJuJYnEx4AaABAg,2,0cYfw5mWJ-E,0,0,2022-09-29T19:01:35Z,"<a href=""https://platform.stratascratch.com/data-projects/marketing-campaign-results"">https://platform.stratascratch.com/data-projects/marketing-campaign-results</a>",2022-09-29T19:01:35Z
Ugw6vVrmwbgEcTFrqsh4AaABAg,@ikejimenez3836,,1,UX4_IgagL9I,1,1,2020-10-19T00:54:40Z,This method did not work for me.  I followed the steps but instead of my csv file showing up directly underneath the sample_data folder icon it appeared near the bottom where the disk space indicator is.  And when I attempted to run the cell with pd.read_csv(‚Äòdata.csv‚Äô) I got a FileNotFoundError.  Any advice would be appreciated.,2020-10-19T00:54:40Z
Ugw6vVrmwbgEcTFrqsh4AaABAg.9EysLUIuQ-c9EzBO1z5z1w,@stratascratch,Ugw6vVrmwbgEcTFrqsh4AaABAg,2,UX4_IgagL9I,0,0,2020-10-19T03:49:47Z,Hey Ike. I think that means that it&#39;s still uploading. How big is your file? Can you try uploading a small file as a test? Try also refreshing your browser if your file is small already.,2020-10-19T03:49:47Z
UgxLZiMo15Io4QPJGf94AaABAg,@rizalrifai6010,,1,Xzr7PMeqXyU,0,0,2023-02-09T19:38:52Z,Can u give the title of strata question so we can explore more,2023-02-09T19:38:52Z
UgwgKxhNpbdCfzG5rsh4AaABAg,@austinocampo2410,,1,ltzJhIs8voQ,1,0,2023-08-20T20:52:51Z,This is interesting I never knew this. Thanks,2023-08-20T20:52:51Z
UgwgKxhNpbdCfzG5rsh4AaABAg.9te3CmVLVH19thUOcj8cOx,@stratascratch,UgwgKxhNpbdCfzG5rsh4AaABAg,2,ltzJhIs8voQ,0,0,2023-08-22T04:48:07Z,Happy to help,2023-08-22T04:48:07Z
UgwYzXhFknRgI7KYJlx4AaABAg,@hasanmougharbel8030,,1,M4shTrIgXs0,0,0,2022-09-26T14:30:18Z,"Hey there,<br>God bless your efforts.<br>I have come through the term concurrence in sql, what does it actually means?<br>Thanks for taking time to clarifying this to me.",2022-09-26T14:30:18Z
UgynBqJ19wENJd7sx8F4AaABAg,@prathyyyyy,,1,OLG6_EHMhFk,3,1,2022-09-03T09:26:25Z,Hey there can you enable indian cards for buying stratascratch subscription would be useful from both team and learners side !!,2022-09-03T09:26:25Z
UgynBqJ19wENJd7sx8F4AaABAg.9fW1d96erA99fdRBH4LpBC,@stratascratch,UgynBqJ19wENJd7sx8F4AaABAg,2,OLG6_EHMhFk,0,0,2022-09-06T15:43:35Z,Hi! We accept all major cards.  Make sure that your card is enabled for international transactions.,2022-09-06T15:43:35Z
UgynBqJ19wENJd7sx8F4AaABAg.9fW1d96erA99fdS00_zp9a,@prathyyyyy,UgynBqJ19wENJd7sx8F4AaABAg,2,OLG6_EHMhFk,0,0,2022-09-06T15:50:48Z,@@stratascratch uhm is there any options for local indian cards!,2022-09-06T15:50:48Z
UgynBqJ19wENJd7sx8F4AaABAg.9fW1d96erA99fdSNzubwBH,@stratascratch,UgynBqJ19wENJd7sx8F4AaABAg,2,OLG6_EHMhFk,0,0,2022-09-06T15:54:04Z,"@@prathyyyyy I am not sure, but we accept Visa and Mastercard.  So check if your card has that.",2022-09-06T15:54:04Z
Ugxxz3B0tPHOeYF4JR14AaABAg,@asaanchaudhury1721,,1,OVD26YMkT_c,1,1,2021-01-26T12:24:12Z,"Hey! Amzing Lesson! Unfortunately, I can&#39;t access the notebook :(",2021-01-26T12:24:12Z
Ugxxz3B0tPHOeYF4JR14AaABAg.9Iz0xNW-PYb9J-Ddmpg0MG,@stratascratch,Ugxxz3B0tPHOeYF4JR14AaABAg,2,OVD26YMkT_c,0,0,2021-01-26T23:34:22Z,"Try these instructions: <a href=""https://bit.ly/2YXKR8o"">https://bit.ly/2YXKR8o</a><br><br>The notebook is read-only so you&#39;ll need to open it up in Google Colabs on your browser and then go File --&gt; Save a Copy In Drive. This will save a copy for the notebook in your own google drive and allow you to edit the notebook. I hope that helps.",2021-01-26T23:34:22Z
UgxxfP7v4oYLgY_9Osh4AaABAg,@mikemihay,,1,OVD26YMkT_c,0,1,2020-08-20T23:02:29Z,Great video! Thank you for making this content !,2020-08-20T23:02:42Z
