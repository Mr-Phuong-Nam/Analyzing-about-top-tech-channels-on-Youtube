{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: #5A96E3; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 20px; font-size: 40px; font-weight: bold; border-radius: 0 0 0 0; box-shadow: 0px 6px 8px rgba(0, 0, 0, 0.2);\">\n",
    "  Stage 01 - Data collecting ðŸ“Œ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import threading\n",
    "warnings.simplefilter('ignore')\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.data_module import make_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data using Nam's Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codebasics': 'UCh9nVJoWXmFb7sLApWGcLPQ',\n",
       " 'TwoMinutePapers': 'UCbfYPyITQ-7l4upoX8nvctg',\n",
       " 'TheAiGrid': 'UCbY9xX3_jW5c2fjlZVBI4cg',\n",
       " 'AINewsOfficial': 'UCDZfn935vwaahDabsfylfIQ',\n",
       " 'abhishekkrthakur': 'UCBPRJjIWfyNG4X-CRbnv78A',\n",
       " 'IBMTechnology': 'UCKWaEZ-_VweaEx1j62do_vQ',\n",
       " 'PROROBOTS': 'UCu8luTDe_Xxd2ahAXsCWX5g',\n",
       " 'StrataScratch': 'UCW8Ews7tdKKkBT6GdtQaXvQ'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_list = json.load(open('../data/external/channel_id_nam.json'))\n",
    "channel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting all the videos from each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No playlists found from channel id: UCbY9xX3_jW5c2fjlZVBI4cg\n",
      "Error when get videos from channel id: UCbfYPyITQ-7l4upoX8nvctg\n"
     ]
    }
   ],
   "source": [
    "# Crawling data for each channel using multi-threading\n",
    "thread = []\n",
    "\n",
    "for channel, id in channel_list.items():\n",
    "    t = threading.Thread(target=make_dataset.make_dataset_playlist_video, args=(channel, id))\n",
    "    thread.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in thread:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting some comments from each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# thread = []\n",
    "\n",
    "# for channel, id in channel_list.items():\n",
    "#     t = threading.Thread(target=make_dataset.make_dataset_comment, args=(channel, ))\n",
    "#     thread.append(t)\n",
    "#     t.start()\n",
    "\n",
    "# for t in thread:\n",
    "#     t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API phuc1\n",
    "make_dataset.API_KEY = 'AIzaSyAPwntW9-9PN7PwBhHaWwTIGIzFz8NWuv8'\n",
    "thread = []\n",
    "\n",
    "t = threading.Thread(target=make_dataset.make_dataset_comment, args=('abhishekkrthakur', ))\n",
    "thread.append(t)\n",
    "t.start()\n",
    "\n",
    "# Split StrataScratch channel into 3 parts\n",
    "StrataScratch = pd.read_csv('../data/raw/StrataScratch_videos.csv')\n",
    "\n",
    "StrataScratch1 = StrataScratch.iloc[:60]\n",
    "StrataScratch2 = StrataScratch.iloc[60:120]\n",
    "StrataScratch3 = StrataScratch.iloc[120:]\n",
    "\n",
    "StrataScratch1.to_csv('../data/raw/StrataScratch1_videos.csv', index=False)\n",
    "StrataScratch2.to_csv('../data/raw/StrataScratch2_videos.csv', index=False)\n",
    "StrataScratch3.to_csv('../data/raw/StrataScratch3_videos.csv', index=False)\n",
    "\n",
    "for i in range(1, 4):\n",
    "    t = threading.Thread(target=make_dataset.make_dataset_comment, args=(f'StrataScratch{i}', ))\n",
    "    thread.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in thread:\n",
    "    t.join()\n",
    "\n",
    "# Merge all comment data into one file\n",
    "StrataScratch1_comment = pd.read_csv('../data/raw/StrataScratch1_comments.csv')\n",
    "StrataScratch2_comment = pd.read_csv('../data/raw/StrataScratch2_comments.csv')\n",
    "StrataScratch3_comment = pd.read_csv('../data/raw/StrataScratch3_comments.csv')\n",
    "\n",
    "StrataScratch_comment = pd.concat([StrataScratch1_comment, StrataScratch2_comment, StrataScratch3_comment], ignore_index=True)\n",
    "StrataScratch_comment.to_csv('../data/raw/StrataScratch_comments.csv', index=False)\n",
    "\n",
    "# Remove the StrataScratch1, StrataScratch2, StrataScratch3 files\n",
    "os.remove('../data/raw/StrataScratch1_comments.csv')\n",
    "os.remove('../data/raw/StrataScratch2_comments.csv')\n",
    "os.remove('../data/raw/StrataScratch3_comments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API phuc2\n",
    "make_dataset.API_KEY = 'AIzaSyCSynzQaJQ5QliJkVCbyz2Wy4oK5Ef4Qu0'\n",
    "thread = []\n",
    "\n",
    "# Split AINewsOfficial channel into 4 parts\n",
    "AINewsOfficial = pd.read_csv('../data/raw/AINewsOfficial_videos.csv')\n",
    "\n",
    "AINewsOfficial1 = AINewsOfficial.iloc[:100]\n",
    "AINewsOfficial2 = AINewsOfficial.iloc[100:200]\n",
    "AINewsOfficial3 = AINewsOfficial.iloc[200:300]\n",
    "AINewsOfficial4 = AINewsOfficial.iloc[300:]\n",
    "\n",
    "AINewsOfficial1.to_csv('../data/raw/AINewsOfficial1_videos.csv', index=False)\n",
    "AINewsOfficial2.to_csv('../data/raw/AINewsOfficial2_videos.csv', index=False)\n",
    "AINewsOfficial3.to_csv('../data/raw/AINewsOfficial3_videos.csv', index=False)\n",
    "AINewsOfficial4.to_csv('../data/raw/AINewsOfficial4_videos.csv', index=False)\n",
    "\n",
    "for i in range(1, 5):\n",
    "    t = threading.Thread(target=make_dataset.make_dataset_comment, args=(f'AINewsOfficial{i}', ))\n",
    "    thread.append(t)\n",
    "    t.start()\n",
    "\n",
    "\n",
    "\n",
    "# Split PROROBOTS channel into 2 parts\n",
    "PROROBOTS = pd.read_csv('../data/raw/PROROBOTS_videos.csv')\n",
    "\n",
    "PROROBOTS1 = PROROBOTS.iloc[:100]\n",
    "PROROBOTS2 = PROROBOTS.iloc[100:]\n",
    "\n",
    "PROROBOTS1.to_csv('../data/raw/PROROBOTS1_videos.csv', index=False)\n",
    "PROROBOTS2.to_csv('../data/raw/PROROBOTS2_videos.csv', index=False)\n",
    "\n",
    "for i in range(1, 3):\n",
    "    t = threading.Thread(target=make_dataset.make_dataset_comment, args=(f'PROROBOTS{i}', ))\n",
    "    thread.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in thread:\n",
    "    t.join()\n",
    "\n",
    "# Merge all comment data into one file\n",
    "AINewsOfficial1_comments = pd.read_csv('../data/raw/AINewsOfficial1_comments.csv')\n",
    "AINewsOfficial2_comments = pd.read_csv('../data/raw/AINewsOfficial2_comments.csv')\n",
    "AINewsOfficial3_comments = pd.read_csv('../data/raw/AINewsOfficial3_comments.csv')\n",
    "AINewsOfficial4_comments = pd.read_csv('../data/raw/AINewsOfficial4_comments.csv')\n",
    "\n",
    "AINewsOfficial_comments = pd.concat([AINewsOfficial1_comments, AINewsOfficial2_comments, AINewsOfficial3_comments, AINewsOfficial4_comments], ignore_index=True)\n",
    "AINewsOfficial_comments.to_csv('../data/raw/AINewsOfficial_comments.csv', index=False)\n",
    "\n",
    "# Remove the AINewsOfficial1, AINewsOfficial2, AINewsOfficial3, AINewsOfficial4 files\n",
    "os.remove('../data/raw/AINewsOfficial1_comments.csv')\n",
    "os.remove('../data/raw/AINewsOfficial2_comments.csv')\n",
    "os.remove('../data/raw/AINewsOfficial3_comments.csv')\n",
    "os.remove('../data/raw/AINewsOfficial4_comments.csv')\n",
    "\n",
    "# Merge all comment data into one file\n",
    "PROROBOTS1_comments = pd.read_csv('../data/raw/PROROBOTS1_comments.csv')\n",
    "PROROBOTS2_comments = pd.read_csv('../data/raw/PROROBOTS2_comments.csv')\n",
    "\n",
    "PROROBOTS_comments = pd.concat([PROROBOTS1_comments, PROROBOTS2_comments], ignore_index=True)\n",
    "PROROBOTS_comments.to_csv('../data/raw/PROROBOTS_comments.csv', index=False)\n",
    "\n",
    "# Remove the PROROBOTS1, PROROBOTS2 files\n",
    "os.remove('../data/raw/PROROBOTS1_comments.csv')\n",
    "os.remove('../data/raw/PROROBOTS2_comments.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't have comment in video id: -dbkE4FFPrI\n"
     ]
    }
   ],
   "source": [
    "# API Nam2\n",
    "make_dataset.API_KEY = 'AIzaSyC8N3gUvqjDt92SzTHPHEv9nSNxC_pGLEc'\n",
    "\n",
    "# Split the data in TwoMinutePapers channel into two parts because it has too many videos, causing the server to go down.\n",
    "TwoMinPapers = pd.read_csv('../data/raw/TwoMinutePapers_videos.csv')\n",
    "TwoMinPapers1 = TwoMinPapers.iloc[:300, :]\n",
    "TwoMinPapers2 = TwoMinPapers.iloc[300:600, :]\n",
    "TwoMinPapers3 = TwoMinPapers.iloc[600:900, :]\n",
    "TwoMinPapers4 = TwoMinPapers.iloc[900:1200, :]\n",
    "TwoMinPapers5 = TwoMinPapers.iloc[1200:1500, :]\n",
    "TwoMinPapers6 = TwoMinPapers.iloc[1500:, :]\n",
    "TwoMinPapers1.to_csv('../data/raw/TwoMinutePapers1_videos.csv', index=False)\n",
    "TwoMinPapers2.to_csv('../data/raw/TwoMinutePapers2_videos.csv', index=False)\n",
    "TwoMinPapers3.to_csv('../data/raw/TwoMinutePapers3_videos.csv', index=False)\n",
    "TwoMinPapers4.to_csv('../data/raw/TwoMinutePapers4_videos.csv', index=False)\n",
    "TwoMinPapers5.to_csv('../data/raw/TwoMinutePapers5_videos.csv', index=False)\n",
    "TwoMinPapers6.to_csv('../data/raw/TwoMinutePapers6_videos.csv', index=False)\n",
    "# Get comment data for TwoMinutePapers channel\n",
    "thread = []\n",
    "for i in range(1, 7):\n",
    "    t = threading.Thread(target=make_dataset.make_dataset_comment, args=('TwoMinutePapers'+str(i), ))\n",
    "    thread.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in thread:\n",
    "    t.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Nam1\n",
    "make_dataset.API_KEY = 'AIzaSyABnDxhMpbDGw2TymHI5a-jLf7c0QPy3pw'\n",
    "\n",
    "# Split codebasics channel into 6 parts\n",
    "codebasics = pd.read_csv('../data/raw/codebasics_videos.csv')\n",
    "codebasics1 = codebasics.iloc[:150, :]\n",
    "codebasics2 = codebasics.iloc[150:300, :]\n",
    "codebasics3 = codebasics.iloc[300:450, :]\n",
    "codebasics4 = codebasics.iloc[450:600, :]\n",
    "codebasics5 = codebasics.iloc[600:750, :]\n",
    "codebasics6 = codebasics.iloc[750:, :]\n",
    "codebasics1.to_csv('../data/raw/codebasics1_videos.csv', index=False)\n",
    "codebasics2.to_csv('../data/raw/codebasics2_videos.csv', index=False)\n",
    "codebasics3.to_csv('../data/raw/codebasics3_videos.csv', index=False)\n",
    "codebasics4.to_csv('../data/raw/codebasics4_videos.csv', index=False)\n",
    "codebasics5.to_csv('../data/raw/codebasics5_videos.csv', index=False)\n",
    "codebasics6.to_csv('../data/raw/codebasics6_videos.csv', index=False)\n",
    "# Get comment data for codebasics channel\n",
    "thread = []\n",
    "for i in range(1, 7):\n",
    "    t = threading.Thread(target=make_dataset.make_dataset_comment, args=('codebasics'+str(i), ))\n",
    "    thread.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in thread:\n",
    "    t.join()\n",
    "\n",
    "# Merge all comment data into one file\n",
    "codebasics1_comments = pd.read_csv('../data/raw/codebasics1_comments.csv')\n",
    "codebasics2_comments = pd.read_csv('../data/raw/codebasics2_comments.csv')\n",
    "codebasics3_comments = pd.read_csv('../data/raw/codebasics3_comments.csv')\n",
    "codebasics4_comments = pd.read_csv('../data/raw/codebasics4_comments.csv')\n",
    "codebasics5_comments = pd.read_csv('../data/raw/codebasics5_comments.csv')\n",
    "codebasics6_comments = pd.read_csv('../data/raw/codebasics6_comments.csv')\n",
    "\n",
    "codebasics_comments = pd.concat([codebasics1_comments, codebasics2_comments, codebasics3_comments, codebasics4_comments, codebasics5_comments, codebasics6_comments], ignore_index=True)\n",
    "codebasics_comments.to_csv('../data/raw/codebasics_comments.csv', index=False)\n",
    "\n",
    "# Remove the codebasics1, codebasics2, codebasics3, codebasics4, codebasics5, codebasics6 files\n",
    "os.remove('../data/raw/codebasics1_comments.csv')\n",
    "os.remove('../data/raw/codebasics2_comments.csv')\n",
    "os.remove('../data/raw/codebasics3_comments.csv')\n",
    "os.remove('../data/raw/codebasics4_comments.csv')\n",
    "os.remove('../data/raw/codebasics5_comments.csv')\n",
    "os.remove('../data/raw/codebasics6_comments.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Nhat1\n",
    "make_dataset.API_KEY = 'AIzaSyC3oTR48ir-0aTayllGVA8dA_zYc8B70KI'\n",
    "\n",
    "# Split TheAiGrid channel into 3 parts\n",
    "TheAiGrid = pd.read_csv('../data/raw/TheAiGrid_videos.csv')\n",
    "TheAiGrid1 = TheAiGrid.iloc[:50, :]\n",
    "TheAiGrid2 = TheAiGrid.iloc[50:100, :]\n",
    "TheAiGrid3 = TheAiGrid.iloc[100:, :]\n",
    "\n",
    "TheAiGrid1.to_csv('../data/raw/TheAiGrid1_videos.csv', index=False)\n",
    "TheAiGrid2.to_csv('../data/raw/TheAiGrid2_videos.csv', index=False)\n",
    "TheAiGrid3.to_csv('../data/raw/TheAiGrid3_videos.csv', index=False)\n",
    "\n",
    "# Get comment data for TheAiGrid channel\n",
    "thread = []\n",
    "for i in range(1, 4):\n",
    "    t = threading.Thread(target=make_dataset.make_dataset_comment, args=('TheAiGrid'+str(i), ))\n",
    "    thread.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in thread:\n",
    "    t.join()\n",
    "\n",
    "# Merge all comment data into one file\n",
    "TheAiGrid1_comments = pd.read_csv('../data/raw/TheAiGrid1_comments.csv')\n",
    "TheAiGrid2_comments = pd.read_csv('../data/raw/TheAiGrid2_comments.csv')\n",
    "TheAiGrid3_comments = pd.read_csv('../data/raw/TheAiGrid3_comments.csv')\n",
    "\n",
    "TheAiGrid_comments = pd.concat([TheAiGrid1_comments, TheAiGrid2_comments, TheAiGrid3_comments], ignore_index=True)\n",
    "\n",
    "TheAiGrid_comments.to_csv('../data/raw/TheAiGrid_comments.csv', index=False)\n",
    "\n",
    "# Remove the TheAiGrid1, TheAiGrid2, TheAiGrid3 files\n",
    "os.remove('../data/raw/TheAiGrid1_comments.csv')\n",
    "os.remove('../data/raw/TheAiGrid2_comments.csv')\n",
    "os.remove('../data/raw/TheAiGrid3_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Nhat2\n",
    "make_dataset.API_KEY = 'AIzaSyCcH4FsKmBAIsQ6pqHVVATsrw5_FW68AA8'\n",
    "\n",
    "# Split IBMTechnology channel into 4 parts\n",
    "IBMTechnology = pd.read_csv('../data/raw/IBMTechnology_videos.csv')\n",
    "IBMTechnology1 = IBMTechnology.iloc[:200, :]\n",
    "IBMTechnology2 = IBMTechnology.iloc[200:400, :]\n",
    "IBMTechnology3 = IBMTechnology.iloc[400:600, :]\n",
    "IBMTechnology4 = IBMTechnology.iloc[600:, :]\n",
    "IBMTechnology1.to_csv('../data/raw/IBMTechnology1_videos.csv', index=False)\n",
    "IBMTechnology2.to_csv('../data/raw/IBMTechnology2_videos.csv', index=False)\n",
    "IBMTechnology3.to_csv('../data/raw/IBMTechnology3_videos.csv', index=False)\n",
    "IBMTechnology4.to_csv('../data/raw/IBMTechnology4_videos.csv', index=False)\n",
    "\n",
    "# Get comment data for IBMTechnology channel\n",
    "thread = []\n",
    "for i in range(1, 5):\n",
    "    t = threading.Thread(target=make_dataset.make_dataset_comment, args=('IBMTechnology'+str(i), ))\n",
    "    thread.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in thread:\n",
    "    t.join()\n",
    "\n",
    "# Merge all comment data into one file\n",
    "IBMTechnology1_comments = pd.read_csv('../data/raw/IBMTechnology1_comments.csv')\n",
    "IBMTechnology2_comments = pd.read_csv('../data/raw/IBMTechnology2_comments.csv')\n",
    "IBMTechnology3_comments = pd.read_csv('../data/raw/IBMTechnology3_comments.csv')\n",
    "IBMTechnology4_comments = pd.read_csv('../data/raw/IBMTechnology4_comments.csv')\n",
    "\n",
    "IBMTechnology_comments = pd.concat([IBMTechnology1_comments, IBMTechnology2_comments, IBMTechnology3_comments, IBMTechnology4_comments], ignore_index=True)\n",
    "IBMTechnology_comments.to_csv('../data/raw/IBMTechnology_comments.csv', index=False)\n",
    "\n",
    "# Remove the IBMTechnology1, IBMTechnology2, IBMTechnology3, IBMTechnology4 files\n",
    "os.remove('../data/raw/IBMTechnology1_comments.csv')\n",
    "os.remove('../data/raw/IBMTechnology2_comments.csv')\n",
    "os.remove('../data/raw/IBMTechnology3_comments.csv')\n",
    "os.remove('../data/raw/IBMTechnology4_comments.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
